{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59741880-564d-4f9a-970b-d0d44980d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp39-cp39-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |███████████████████▋            | 461.2 MB 103.4 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 750.6 MB 13 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "!{sys.executable} -m pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f44c8b-8642-42dd-b894-461819d8bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: requests in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: torch==1.11.0 in ./.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from requests->torchvision) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.12.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "!{sys.executable} -m pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8b32c8-43b9-4c26-99ec-e93cd593857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x1554c836f3a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "cuda_enabled = torch.cuda.is_available()\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#cuda_enabled = False\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "cudnn.benchmark = True\n",
    "\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from __future__ import print_function, division\n",
    "import datetime\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1440638-1ece-4c7e-9fd5-3065b80b4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.is_training = False\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.linear = nn.Linear(self.hidden_size*2, self.num_classes)\n",
    "        if cuda_enabled:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.fc = self.fc.cuda()\n",
    "            self.linear = self.linear.cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)) # 2 for bidirection \n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size))\n",
    "        if cuda_enabled:\n",
    "            h0 = h0.cuda()  # 2 for bidirection\n",
    "            c0 = c0.cuda()\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        if self.is_training:\n",
    "            out = self.fc(out[:, -1, :])\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        # out = F.log_softmax(self.linear(out), dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31d8558-9bb1-4a28-a072-42f0158de099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):#root_dir = img_dir\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)#same as image label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
    "        name = self.landmarks_frame.iloc[idx, 0]\n",
    "        labels = np.zeros(8)\n",
    "        for i in range(8):\n",
    "            labels[i]=name\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        marks = []\n",
    "        for mark in range(len(landmarks)):\n",
    "            arr = literal_eval(landmarks[mark])[0]\n",
    "            arr1 = []\n",
    "            for i in range(len(arr)):\n",
    "                for j in range(len(arr[i])):\n",
    "                    # print(arr[i][j])\n",
    "                    arr1.append(arr[i][j])\n",
    "            marks.append(np.array(arr1))  \n",
    "            # return\n",
    "                          \n",
    "        marks = torch.Tensor(marks)\n",
    "        \n",
    "        sample = {'label': labels, 'marks': marks}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "475097c1-1302-4ea2-84f5-0f606627020b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, torch.Size([8]), torch.Size([125, 8]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyedataset = EyeLandmarksDataset(csv_file='CSVs/EyeTracking.csv')\n",
    "eyedataset.__len__(), eyedataset.__getitem__(0)['label'].shape, eyedataset.__getitem__(0)['marks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f84eadfc-6b34-4dbf-b9f0-6e75d248cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Eye NN dataset -------------------------------------\n",
      "224 <__main__.EyeLandmarksDataset object at 0x1554c1056fd0>\n",
      "156 <torch.utils.data.dataset.Subset object at 0x1554c1e7f5b0>\n",
      "68 <torch.utils.data.dataset.Subset object at 0x1554c1e7f400>\n",
      "Starting Eye NN loader -------------------------------------\n",
      "3 <torch.utils.data.dataloader.DataLoader object at 0x1554c1055be0>\n",
      "2 <torch.utils.data.dataloader.DataLoader object at 0x1554c1055b80>\n",
      "Finish  -------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train_total_dataset(dataset, total_split=0.30): #Training set be 70% of the dataset\n",
    "    train_idx, total_idx = train_test_split(list(range(len(dataset))), test_size=total_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['total_split'] = Subset(dataset, total_idx)\n",
    "    return datasets\n",
    "\n",
    "# hyperparams for the network\n",
    "batch_size = 64\n",
    "\n",
    "print('Starting Eye NN dataset -------------------------------------')\n",
    "eye_datasets = train_total_dataset(eyedataset)\n",
    "eye_train = eye_datasets['train']\n",
    "eye_test = eye_datasets['total_split']\n",
    "# The original dataset is available in the Subset class\n",
    "print(len(eye_datasets['train'].dataset), eye_datasets['train'].dataset)\n",
    "print(len(eye_train), eye_train)\n",
    "print(len(eye_test), eye_test)\n",
    "\n",
    "print('Starting Eye NN loader -------------------------------------')\n",
    "eye_train_loader = DataLoader(dataset=eye_train, batch_size=batch_size, shuffle=True)\n",
    "eye_test_loader = DataLoader(dataset=eye_test, batch_size=batch_size, shuffle=False)\n",
    "print(len(eye_train_loader), eye_train_loader)\n",
    "print(len(eye_test_loader), eye_test_loader)\n",
    "print('Finish  -------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e5d704e-5c73-4363-9f04-82b1308dec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init NN\n",
    "input_size = 1\n",
    "sequence_length = 125\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2  # TODO: Determine this from the data\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 300\n",
    "\n",
    "# The network\n",
    "eye_model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "eye_model.is_training = True\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(eye_model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_loss = 5000000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04086016-1979-4ea4-838d-0a441e2e2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing = dict()\n",
    "timing['training'] = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d632214-eac1-4d21-a6cb-17a1a2e3e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [10/2], Loss: 0.6735\n",
      "Epoch [1/300], Step [20/2], Loss: 0.7418\n",
      "Epoch [1/300], Step [30/2], Loss: 0.7058\n",
      "Epoch [1/300], Step [40/2], Loss: 0.6305\n",
      "Epoch [1/300], Step [50/2], Loss: 0.6780\n",
      "Epoch [1/300], Step [60/2], Loss: 0.7269\n",
      "Epoch [1/300], Step [70/2], Loss: 0.7840\n",
      "Epoch [1/300], Step [80/2], Loss: 0.8624\n",
      "Epoch [1/300], Step [90/2], Loss: 0.7261\n",
      "Epoch [1/300], Step [100/2], Loss: 0.5959\n",
      "Epoch [1/300], Step [110/2], Loss: 0.5799\n",
      "Epoch [1/300], Step [120/2], Loss: 0.6393\n",
      "Epoch [1/300], Step [130/2], Loss: 0.5594\n",
      "Epoch [1/300], Step [140/2], Loss: 0.4643\n",
      "Epoch [1/300], Step [150/2], Loss: 0.6321\n",
      "Epoch 0; loss = 0.6587\n",
      "Epoch [2/300], Step [10/2], Loss: 0.2611\n",
      "Epoch [2/300], Step [20/2], Loss: 1.1266\n",
      "Epoch [2/300], Step [30/2], Loss: 0.7711\n",
      "Epoch [2/300], Step [40/2], Loss: 0.4734\n",
      "Epoch [2/300], Step [50/2], Loss: 0.6314\n",
      "Epoch [2/300], Step [60/2], Loss: 0.8018\n",
      "Epoch [2/300], Step [70/2], Loss: 0.8779\n",
      "Epoch [2/300], Step [80/2], Loss: 1.0136\n",
      "Epoch [2/300], Step [90/2], Loss: 0.7916\n",
      "Epoch [2/300], Step [100/2], Loss: 0.5741\n",
      "Epoch [2/300], Step [110/2], Loss: 0.5612\n",
      "Epoch [2/300], Step [120/2], Loss: 0.5809\n",
      "Epoch [2/300], Step [130/2], Loss: 0.5569\n",
      "Epoch [2/300], Step [140/2], Loss: 0.4465\n",
      "Epoch [2/300], Step [150/2], Loss: 0.5779\n",
      "Epoch 1; loss = 0.6524\n",
      "Epoch [3/300], Step [10/2], Loss: 0.2700\n",
      "Epoch [3/300], Step [20/2], Loss: 1.0695\n",
      "Epoch [3/300], Step [30/2], Loss: 0.8357\n",
      "Epoch [3/300], Step [40/2], Loss: 0.4590\n",
      "Epoch [3/300], Step [50/2], Loss: 0.5879\n",
      "Epoch [3/300], Step [60/2], Loss: 0.8758\n",
      "Epoch [3/300], Step [70/2], Loss: 0.8855\n",
      "Epoch [3/300], Step [80/2], Loss: 1.0337\n",
      "Epoch [3/300], Step [90/2], Loss: 0.8505\n",
      "Epoch [3/300], Step [100/2], Loss: 0.5902\n",
      "Epoch [3/300], Step [110/2], Loss: 0.5736\n",
      "Epoch [3/300], Step [120/2], Loss: 0.5331\n",
      "Epoch [3/300], Step [130/2], Loss: 0.5718\n",
      "Epoch [3/300], Step [140/2], Loss: 0.4450\n",
      "Epoch [3/300], Step [150/2], Loss: 0.5373\n",
      "Epoch 2; loss = 0.6474\n",
      "Epoch [4/300], Step [10/2], Loss: 0.2707\n",
      "Epoch [4/300], Step [20/2], Loss: 1.0402\n",
      "Epoch [4/300], Step [30/2], Loss: 0.8860\n",
      "Epoch [4/300], Step [40/2], Loss: 0.4516\n",
      "Epoch [4/300], Step [50/2], Loss: 0.5589\n",
      "Epoch [4/300], Step [60/2], Loss: 0.9335\n",
      "Epoch [4/300], Step [70/2], Loss: 0.8855\n",
      "Epoch [4/300], Step [80/2], Loss: 1.0395\n",
      "Epoch [4/300], Step [90/2], Loss: 0.8922\n",
      "Epoch [4/300], Step [100/2], Loss: 0.6037\n",
      "Epoch [4/300], Step [110/2], Loss: 0.5830\n",
      "Epoch [4/300], Step [120/2], Loss: 0.5042\n",
      "Epoch [4/300], Step [130/2], Loss: 0.5829\n",
      "Epoch [4/300], Step [140/2], Loss: 0.4431\n",
      "Epoch [4/300], Step [150/2], Loss: 0.5139\n",
      "Epoch 3; loss = 0.6445\n",
      "Epoch [5/300], Step [10/2], Loss: 0.2691\n",
      "Epoch [5/300], Step [20/2], Loss: 1.0208\n",
      "Epoch [5/300], Step [30/2], Loss: 0.9133\n",
      "Epoch [5/300], Step [40/2], Loss: 0.4433\n",
      "Epoch [5/300], Step [50/2], Loss: 0.5461\n",
      "Epoch [5/300], Step [60/2], Loss: 0.9644\n",
      "Epoch [5/300], Step [70/2], Loss: 0.8846\n",
      "Epoch [5/300], Step [80/2], Loss: 1.0432\n",
      "Epoch [5/300], Step [90/2], Loss: 0.9154\n",
      "Epoch [5/300], Step [100/2], Loss: 0.6111\n",
      "Epoch [5/300], Step [110/2], Loss: 0.5865\n",
      "Epoch [5/300], Step [120/2], Loss: 0.4901\n",
      "Epoch [5/300], Step [130/2], Loss: 0.5903\n",
      "Epoch [5/300], Step [140/2], Loss: 0.4387\n",
      "Epoch [5/300], Step [150/2], Loss: 0.5017\n",
      "Epoch 4; loss = 0.6424\n",
      "Epoch [6/300], Step [10/2], Loss: 0.2649\n",
      "Epoch [6/300], Step [20/2], Loss: 1.0091\n",
      "Epoch [6/300], Step [30/2], Loss: 0.9264\n",
      "Epoch [6/300], Step [40/2], Loss: 0.4325\n",
      "Epoch [6/300], Step [50/2], Loss: 0.5424\n",
      "Epoch [6/300], Step [60/2], Loss: 0.9801\n",
      "Epoch [6/300], Step [70/2], Loss: 0.8825\n",
      "Epoch [6/300], Step [80/2], Loss: 1.0461\n",
      "Epoch [6/300], Step [90/2], Loss: 0.9291\n",
      "Epoch [6/300], Step [100/2], Loss: 0.6155\n",
      "Epoch [6/300], Step [110/2], Loss: 0.5870\n",
      "Epoch [6/300], Step [120/2], Loss: 0.4822\n",
      "Epoch [6/300], Step [130/2], Loss: 0.5970\n",
      "Epoch [6/300], Step [140/2], Loss: 0.4319\n",
      "Epoch [6/300], Step [150/2], Loss: 0.4942\n",
      "Epoch 5; loss = 0.6406\n",
      "Epoch [7/300], Step [10/2], Loss: 0.2574\n",
      "Epoch [7/300], Step [20/2], Loss: 1.0051\n",
      "Epoch [7/300], Step [30/2], Loss: 0.9334\n",
      "Epoch [7/300], Step [40/2], Loss: 0.4187\n",
      "Epoch [7/300], Step [50/2], Loss: 0.5434\n",
      "Epoch [7/300], Step [60/2], Loss: 0.9904\n",
      "Epoch [7/300], Step [70/2], Loss: 0.8783\n",
      "Epoch [7/300], Step [80/2], Loss: 1.0474\n",
      "Epoch [7/300], Step [90/2], Loss: 0.9393\n",
      "Epoch [7/300], Step [100/2], Loss: 0.6190\n",
      "Epoch [7/300], Step [110/2], Loss: 0.5862\n",
      "Epoch [7/300], Step [120/2], Loss: 0.4763\n",
      "Epoch [7/300], Step [130/2], Loss: 0.6053\n",
      "Epoch [7/300], Step [140/2], Loss: 0.4220\n",
      "Epoch [7/300], Step [150/2], Loss: 0.4880\n",
      "Epoch 6; loss = 0.6386\n",
      "Epoch [8/300], Step [10/2], Loss: 0.2447\n",
      "Epoch [8/300], Step [20/2], Loss: 1.0121\n",
      "Epoch [8/300], Step [30/2], Loss: 0.9374\n",
      "Epoch [8/300], Step [40/2], Loss: 0.3996\n",
      "Epoch [8/300], Step [50/2], Loss: 0.5487\n",
      "Epoch [8/300], Step [60/2], Loss: 0.9996\n",
      "Epoch [8/300], Step [70/2], Loss: 0.8699\n",
      "Epoch [8/300], Step [80/2], Loss: 1.0466\n",
      "Epoch [8/300], Step [90/2], Loss: 0.9496\n",
      "Epoch [8/300], Step [100/2], Loss: 0.6229\n",
      "Epoch [8/300], Step [110/2], Loss: 0.5847\n",
      "Epoch [8/300], Step [120/2], Loss: 0.4705\n",
      "Epoch [8/300], Step [130/2], Loss: 0.6181\n",
      "Epoch [8/300], Step [140/2], Loss: 0.4064\n",
      "Epoch [8/300], Step [150/2], Loss: 0.4813\n",
      "Epoch 7; loss = 0.6359\n",
      "Epoch [9/300], Step [10/2], Loss: 0.2226\n",
      "Epoch [9/300], Step [20/2], Loss: 1.0310\n",
      "Epoch [9/300], Step [30/2], Loss: 0.9406\n",
      "Epoch [9/300], Step [40/2], Loss: 0.3681\n",
      "Epoch [9/300], Step [50/2], Loss: 0.5617\n",
      "Epoch [9/300], Step [60/2], Loss: 1.0105\n",
      "Epoch [9/300], Step [70/2], Loss: 0.8461\n",
      "Epoch [9/300], Step [80/2], Loss: 1.0428\n",
      "Epoch [9/300], Step [90/2], Loss: 0.9651\n",
      "Epoch [9/300], Step [100/2], Loss: 0.6250\n",
      "Epoch [9/300], Step [110/2], Loss: 0.5743\n",
      "Epoch [9/300], Step [120/2], Loss: 0.4623\n",
      "Epoch [9/300], Step [130/2], Loss: 0.6394\n",
      "Epoch [9/300], Step [140/2], Loss: 0.3847\n",
      "Epoch [9/300], Step [150/2], Loss: 0.4746\n",
      "Epoch 8; loss = 0.6328\n",
      "Epoch [10/300], Step [10/2], Loss: 0.2172\n",
      "Epoch [10/300], Step [20/2], Loss: 1.0171\n",
      "Epoch [10/300], Step [30/2], Loss: 0.9349\n",
      "Epoch [10/300], Step [40/2], Loss: 0.2498\n",
      "Epoch [10/300], Step [50/2], Loss: 0.7546\n",
      "Epoch [10/300], Step [60/2], Loss: 0.9961\n",
      "Epoch [10/300], Step [70/2], Loss: 0.8013\n",
      "Epoch [10/300], Step [80/2], Loss: 1.0180\n",
      "Epoch [10/300], Step [90/2], Loss: 0.9537\n",
      "Epoch [10/300], Step [100/2], Loss: 0.6387\n",
      "Epoch [10/300], Step [110/2], Loss: 0.6223\n",
      "Epoch [10/300], Step [120/2], Loss: 0.4709\n",
      "Epoch [10/300], Step [130/2], Loss: 0.7177\n",
      "Epoch [10/300], Step [140/2], Loss: 0.4783\n",
      "Epoch [10/300], Step [150/2], Loss: 0.4828\n",
      "Epoch 9; loss = 0.6458\n",
      "Epoch [11/300], Step [10/2], Loss: 0.2969\n",
      "Epoch [11/300], Step [20/2], Loss: 0.8333\n",
      "Epoch [11/300], Step [30/2], Loss: 0.9759\n",
      "Epoch [11/300], Step [40/2], Loss: 0.2607\n",
      "Epoch [11/300], Step [50/2], Loss: 0.5376\n",
      "Epoch [11/300], Step [60/2], Loss: 1.0258\n",
      "Epoch [11/300], Step [70/2], Loss: 0.7976\n",
      "Epoch [11/300], Step [80/2], Loss: 1.0746\n",
      "Epoch [11/300], Step [90/2], Loss: 0.9816\n",
      "Epoch [11/300], Step [100/2], Loss: 0.6207\n",
      "Epoch [11/300], Step [110/2], Loss: 0.5724\n",
      "Epoch [11/300], Step [120/2], Loss: 0.4575\n",
      "Epoch [11/300], Step [130/2], Loss: 0.6734\n",
      "Epoch [11/300], Step [140/2], Loss: 0.3708\n",
      "Epoch [11/300], Step [150/2], Loss: 0.4697\n",
      "Epoch 10; loss = 0.6281\n",
      "Epoch [12/300], Step [10/2], Loss: 0.2755\n",
      "Epoch [12/300], Step [20/2], Loss: 0.8214\n",
      "Epoch [12/300], Step [30/2], Loss: 0.9928\n",
      "Epoch [12/300], Step [40/2], Loss: 0.2237\n",
      "Epoch [12/300], Step [50/2], Loss: 0.5453\n",
      "Epoch [12/300], Step [60/2], Loss: 1.0405\n",
      "Epoch [12/300], Step [70/2], Loss: 0.7772\n",
      "Epoch [12/300], Step [80/2], Loss: 1.0757\n",
      "Epoch [12/300], Step [90/2], Loss: 0.9953\n",
      "Epoch [12/300], Step [100/2], Loss: 0.6248\n",
      "Epoch [12/300], Step [110/2], Loss: 0.5136\n",
      "Epoch [12/300], Step [120/2], Loss: 0.4499\n",
      "Epoch [12/300], Step [130/2], Loss: 0.5790\n",
      "Epoch [12/300], Step [140/2], Loss: 0.2363\n",
      "Epoch [12/300], Step [150/2], Loss: 0.4561\n",
      "Epoch 11; loss = 0.6283\n",
      "Epoch [13/300], Step [10/2], Loss: 0.2235\n",
      "Epoch [13/300], Step [20/2], Loss: 0.8918\n",
      "Epoch [13/300], Step [30/2], Loss: 0.9518\n",
      "Epoch [13/300], Step [40/2], Loss: 0.3021\n",
      "Epoch [13/300], Step [50/2], Loss: 0.7324\n",
      "Epoch [13/300], Step [60/2], Loss: 1.0178\n",
      "Epoch [13/300], Step [70/2], Loss: 0.9340\n",
      "Epoch [13/300], Step [80/2], Loss: 1.1138\n",
      "Epoch [13/300], Step [90/2], Loss: 0.9881\n",
      "Epoch [13/300], Step [100/2], Loss: 0.5107\n",
      "Epoch [13/300], Step [110/2], Loss: 0.4825\n",
      "Epoch [13/300], Step [120/2], Loss: 0.4494\n",
      "Epoch [13/300], Step [130/2], Loss: 0.5220\n",
      "Epoch [13/300], Step [140/2], Loss: 0.3234\n",
      "Epoch [13/300], Step [150/2], Loss: 0.4452\n",
      "Epoch 12; loss = 0.6350\n",
      "Epoch [14/300], Step [10/2], Loss: 0.2538\n",
      "Epoch [14/300], Step [20/2], Loss: 0.8217\n",
      "Epoch [14/300], Step [30/2], Loss: 0.9892\n",
      "Epoch [14/300], Step [40/2], Loss: 0.3322\n",
      "Epoch [14/300], Step [50/2], Loss: 0.6657\n",
      "Epoch [14/300], Step [60/2], Loss: 1.0581\n",
      "Epoch [14/300], Step [70/2], Loss: 0.8804\n",
      "Epoch [14/300], Step [80/2], Loss: 1.1194\n",
      "Epoch [14/300], Step [90/2], Loss: 1.0180\n",
      "Epoch [14/300], Step [100/2], Loss: 0.5245\n",
      "Epoch [14/300], Step [110/2], Loss: 0.4972\n",
      "Epoch [14/300], Step [120/2], Loss: 0.4331\n",
      "Epoch [14/300], Step [130/2], Loss: 0.5315\n",
      "Epoch [14/300], Step [140/2], Loss: 0.3290\n",
      "Epoch [14/300], Step [150/2], Loss: 0.4327\n",
      "Epoch 13; loss = 0.6305\n",
      "Epoch [15/300], Step [10/2], Loss: 0.2456\n",
      "Epoch [15/300], Step [20/2], Loss: 0.8100\n",
      "Epoch [15/300], Step [30/2], Loss: 1.0071\n",
      "Epoch [15/300], Step [40/2], Loss: 0.3380\n",
      "Epoch [15/300], Step [50/2], Loss: 0.6326\n",
      "Epoch [15/300], Step [60/2], Loss: 1.0775\n",
      "Epoch [15/300], Step [70/2], Loss: 0.8674\n",
      "Epoch [15/300], Step [80/2], Loss: 1.1214\n",
      "Epoch [15/300], Step [90/2], Loss: 1.0336\n",
      "Epoch [15/300], Step [100/2], Loss: 0.5397\n",
      "Epoch [15/300], Step [110/2], Loss: 0.5053\n",
      "Epoch [15/300], Step [120/2], Loss: 0.4225\n",
      "Epoch [15/300], Step [130/2], Loss: 0.6109\n",
      "Epoch [15/300], Step [140/2], Loss: 0.3215\n",
      "Epoch [15/300], Step [150/2], Loss: 0.4247\n",
      "Epoch 14; loss = 0.6291\n",
      "Epoch [16/300], Step [10/2], Loss: 0.2371\n",
      "Epoch [16/300], Step [20/2], Loss: 0.7860\n",
      "Epoch [16/300], Step [30/2], Loss: 1.0175\n",
      "Epoch [16/300], Step [40/2], Loss: 0.3266\n",
      "Epoch [16/300], Step [50/2], Loss: 0.6278\n",
      "Epoch [16/300], Step [60/2], Loss: 1.0896\n",
      "Epoch [16/300], Step [70/2], Loss: 0.8638\n",
      "Epoch [16/300], Step [80/2], Loss: 1.1288\n",
      "Epoch [16/300], Step [90/2], Loss: 1.0426\n",
      "Epoch [16/300], Step [100/2], Loss: 0.5421\n",
      "Epoch [16/300], Step [110/2], Loss: 0.5068\n",
      "Epoch [16/300], Step [120/2], Loss: 0.4186\n",
      "Epoch [16/300], Step [130/2], Loss: 0.6338\n",
      "Epoch [16/300], Step [140/2], Loss: 0.3158\n",
      "Epoch [16/300], Step [150/2], Loss: 0.4221\n",
      "Epoch 15; loss = 0.6276\n",
      "Epoch [17/300], Step [10/2], Loss: 0.2311\n",
      "Epoch [17/300], Step [20/2], Loss: 0.7718\n",
      "Epoch [17/300], Step [30/2], Loss: 1.0206\n",
      "Epoch [17/300], Step [40/2], Loss: 0.3203\n",
      "Epoch [17/300], Step [50/2], Loss: 0.6275\n",
      "Epoch [17/300], Step [60/2], Loss: 1.0936\n",
      "Epoch [17/300], Step [70/2], Loss: 0.8562\n",
      "Epoch [17/300], Step [80/2], Loss: 1.1298\n",
      "Epoch [17/300], Step [90/2], Loss: 1.0470\n",
      "Epoch [17/300], Step [100/2], Loss: 0.5446\n",
      "Epoch [17/300], Step [110/2], Loss: 0.5084\n",
      "Epoch [17/300], Step [120/2], Loss: 0.4168\n",
      "Epoch [17/300], Step [130/2], Loss: 0.6427\n",
      "Epoch [17/300], Step [140/2], Loss: 0.3088\n",
      "Epoch [17/300], Step [150/2], Loss: 0.4206\n",
      "Epoch 16; loss = 0.6260\n",
      "Epoch [18/300], Step [10/2], Loss: 0.2277\n",
      "Epoch [18/300], Step [20/2], Loss: 0.7579\n",
      "Epoch [18/300], Step [30/2], Loss: 1.0222\n",
      "Epoch [18/300], Step [40/2], Loss: 0.3171\n",
      "Epoch [18/300], Step [50/2], Loss: 0.6318\n",
      "Epoch [18/300], Step [60/2], Loss: 1.0958\n",
      "Epoch [18/300], Step [70/2], Loss: 0.8454\n",
      "Epoch [18/300], Step [80/2], Loss: 1.1285\n",
      "Epoch [18/300], Step [90/2], Loss: 1.0507\n",
      "Epoch [18/300], Step [100/2], Loss: 0.5464\n",
      "Epoch [18/300], Step [110/2], Loss: 0.5086\n",
      "Epoch [18/300], Step [120/2], Loss: 0.4148\n",
      "Epoch [18/300], Step [130/2], Loss: 0.6377\n",
      "Epoch [18/300], Step [140/2], Loss: 0.3032\n",
      "Epoch [18/300], Step [150/2], Loss: 0.4190\n",
      "Epoch 17; loss = 0.6243\n",
      "Epoch [19/300], Step [10/2], Loss: 0.2251\n",
      "Epoch [19/300], Step [20/2], Loss: 0.7330\n",
      "Epoch [19/300], Step [30/2], Loss: 1.0250\n",
      "Epoch [19/300], Step [40/2], Loss: 0.3116\n",
      "Epoch [19/300], Step [50/2], Loss: 0.6378\n",
      "Epoch [19/300], Step [60/2], Loss: 1.0997\n",
      "Epoch [19/300], Step [70/2], Loss: 0.8341\n",
      "Epoch [19/300], Step [80/2], Loss: 1.1286\n",
      "Epoch [19/300], Step [90/2], Loss: 1.0540\n",
      "Epoch [19/300], Step [100/2], Loss: 0.5476\n",
      "Epoch [19/300], Step [110/2], Loss: 0.5088\n",
      "Epoch [19/300], Step [120/2], Loss: 0.4129\n",
      "Epoch [19/300], Step [130/2], Loss: 0.6736\n",
      "Epoch [19/300], Step [140/2], Loss: 0.2975\n",
      "Epoch [19/300], Step [150/2], Loss: 0.4181\n",
      "Epoch 18; loss = 0.6228\n",
      "Epoch [20/300], Step [10/2], Loss: 0.2202\n",
      "Epoch [20/300], Step [20/2], Loss: 0.7433\n",
      "Epoch [20/300], Step [30/2], Loss: 1.0237\n",
      "Epoch [20/300], Step [40/2], Loss: 0.3089\n",
      "Epoch [20/300], Step [50/2], Loss: 0.6465\n",
      "Epoch [20/300], Step [60/2], Loss: 1.0990\n",
      "Epoch [20/300], Step [70/2], Loss: 0.8463\n",
      "Epoch [20/300], Step [80/2], Loss: 1.1281\n",
      "Epoch [20/300], Step [90/2], Loss: 1.0611\n",
      "Epoch [20/300], Step [100/2], Loss: 0.5453\n",
      "Epoch [20/300], Step [110/2], Loss: 0.5077\n",
      "Epoch [20/300], Step [120/2], Loss: 0.4094\n",
      "Epoch [20/300], Step [130/2], Loss: 0.5695\n",
      "Epoch [20/300], Step [140/2], Loss: 0.2892\n",
      "Epoch [20/300], Step [150/2], Loss: 0.4591\n",
      "Epoch 19; loss = 0.6218\n",
      "Epoch [21/300], Step [10/2], Loss: 0.2131\n",
      "Epoch [21/300], Step [20/2], Loss: 0.8915\n",
      "Epoch [21/300], Step [30/2], Loss: 1.0323\n",
      "Epoch [21/300], Step [40/2], Loss: 0.3229\n",
      "Epoch [21/300], Step [50/2], Loss: 0.6147\n",
      "Epoch [21/300], Step [60/2], Loss: 1.0945\n",
      "Epoch [21/300], Step [70/2], Loss: 0.7977\n",
      "Epoch [21/300], Step [80/2], Loss: 1.1062\n",
      "Epoch [21/300], Step [90/2], Loss: 1.0558\n",
      "Epoch [21/300], Step [100/2], Loss: 0.5585\n",
      "Epoch [21/300], Step [110/2], Loss: 0.5181\n",
      "Epoch [21/300], Step [120/2], Loss: 0.4121\n",
      "Epoch [21/300], Step [130/2], Loss: 0.6098\n",
      "Epoch [21/300], Step [140/2], Loss: 0.2952\n",
      "Epoch [21/300], Step [150/2], Loss: 0.4570\n",
      "Epoch 20; loss = 0.6239\n",
      "Epoch [22/300], Step [10/2], Loss: 0.2117\n",
      "Epoch [22/300], Step [20/2], Loss: 0.8590\n",
      "Epoch [22/300], Step [30/2], Loss: 1.0293\n",
      "Epoch [22/300], Step [40/2], Loss: 0.3207\n",
      "Epoch [22/300], Step [50/2], Loss: 0.6401\n",
      "Epoch [22/300], Step [60/2], Loss: 1.0947\n",
      "Epoch [22/300], Step [70/2], Loss: 0.7986\n",
      "Epoch [22/300], Step [80/2], Loss: 1.1114\n",
      "Epoch [22/300], Step [90/2], Loss: 1.0525\n",
      "Epoch [22/300], Step [100/2], Loss: 0.5677\n",
      "Epoch [22/300], Step [110/2], Loss: 0.5300\n",
      "Epoch [22/300], Step [120/2], Loss: 0.4130\n",
      "Epoch [22/300], Step [130/2], Loss: 0.6449\n",
      "Epoch [22/300], Step [140/2], Loss: 0.2900\n",
      "Epoch [22/300], Step [150/2], Loss: 0.4156\n",
      "Epoch 21; loss = 0.6205\n",
      "Epoch [23/300], Step [10/2], Loss: 0.2134\n",
      "Epoch [23/300], Step [20/2], Loss: 0.7088\n",
      "Epoch [23/300], Step [30/2], Loss: 1.0361\n",
      "Epoch [23/300], Step [40/2], Loss: 0.3113\n",
      "Epoch [23/300], Step [50/2], Loss: 0.6578\n",
      "Epoch [23/300], Step [60/2], Loss: 1.1074\n",
      "Epoch [23/300], Step [70/2], Loss: 0.8125\n",
      "Epoch [23/300], Step [80/2], Loss: 1.1223\n",
      "Epoch [23/300], Step [90/2], Loss: 1.0607\n",
      "Epoch [23/300], Step [100/2], Loss: 0.5613\n",
      "Epoch [23/300], Step [110/2], Loss: 0.5286\n",
      "Epoch [23/300], Step [120/2], Loss: 0.4093\n",
      "Epoch [23/300], Step [130/2], Loss: 0.6553\n",
      "Epoch [23/300], Step [140/2], Loss: 0.2979\n",
      "Epoch [23/300], Step [150/2], Loss: 0.4124\n",
      "Epoch 22; loss = 0.6180\n",
      "Epoch [24/300], Step [10/2], Loss: 0.2115\n",
      "Epoch [24/300], Step [20/2], Loss: 0.6991\n",
      "Epoch [24/300], Step [30/2], Loss: 1.0406\n",
      "Epoch [24/300], Step [40/2], Loss: 0.3061\n",
      "Epoch [24/300], Step [50/2], Loss: 0.6672\n",
      "Epoch [24/300], Step [60/2], Loss: 1.1135\n",
      "Epoch [24/300], Step [70/2], Loss: 0.8162\n",
      "Epoch [24/300], Step [80/2], Loss: 1.1227\n",
      "Epoch [24/300], Step [90/2], Loss: 1.0644\n",
      "Epoch [24/300], Step [100/2], Loss: 0.5585\n",
      "Epoch [24/300], Step [110/2], Loss: 0.5290\n",
      "Epoch [24/300], Step [120/2], Loss: 0.4067\n",
      "Epoch [24/300], Step [130/2], Loss: 0.6601\n",
      "Epoch [24/300], Step [140/2], Loss: 0.2904\n",
      "Epoch [24/300], Step [150/2], Loss: 0.4107\n",
      "Epoch 23; loss = 0.6169\n",
      "Epoch [25/300], Step [10/2], Loss: 0.2118\n",
      "Epoch [25/300], Step [20/2], Loss: 0.6913\n",
      "Epoch [25/300], Step [30/2], Loss: 1.0433\n",
      "Epoch [25/300], Step [40/2], Loss: 0.3016\n",
      "Epoch [25/300], Step [50/2], Loss: 0.6775\n",
      "Epoch [25/300], Step [60/2], Loss: 1.1159\n",
      "Epoch [25/300], Step [70/2], Loss: 0.8212\n",
      "Epoch [25/300], Step [80/2], Loss: 1.1257\n",
      "Epoch [25/300], Step [90/2], Loss: 1.0665\n",
      "Epoch [25/300], Step [100/2], Loss: 0.5544\n",
      "Epoch [25/300], Step [110/2], Loss: 0.5271\n",
      "Epoch [25/300], Step [120/2], Loss: 0.4063\n",
      "Epoch [25/300], Step [130/2], Loss: 0.6446\n",
      "Epoch [25/300], Step [140/2], Loss: 0.2759\n",
      "Epoch [25/300], Step [150/2], Loss: 0.4074\n",
      "Epoch 24; loss = 0.6154\n",
      "Epoch [26/300], Step [10/2], Loss: 0.2109\n",
      "Epoch [26/300], Step [20/2], Loss: 0.6623\n",
      "Epoch [26/300], Step [30/2], Loss: 1.0506\n",
      "Epoch [26/300], Step [40/2], Loss: 0.2992\n",
      "Epoch [26/300], Step [50/2], Loss: 0.6743\n",
      "Epoch [26/300], Step [60/2], Loss: 1.1227\n",
      "Epoch [26/300], Step [70/2], Loss: 0.8204\n",
      "Epoch [26/300], Step [80/2], Loss: 1.1225\n",
      "Epoch [26/300], Step [90/2], Loss: 1.0645\n",
      "Epoch [26/300], Step [100/2], Loss: 0.5540\n",
      "Epoch [26/300], Step [110/2], Loss: 0.5362\n",
      "Epoch [26/300], Step [120/2], Loss: 0.4094\n",
      "Epoch [26/300], Step [130/2], Loss: 0.6498\n",
      "Epoch [26/300], Step [140/2], Loss: 0.3167\n",
      "Epoch [26/300], Step [150/2], Loss: 0.4104\n",
      "Epoch 25; loss = 0.6188\n",
      "Epoch [27/300], Step [10/2], Loss: 0.2322\n",
      "Epoch [27/300], Step [20/2], Loss: 0.6542\n",
      "Epoch [27/300], Step [30/2], Loss: 1.0494\n",
      "Epoch [27/300], Step [40/2], Loss: 0.2968\n",
      "Epoch [27/300], Step [50/2], Loss: 0.6645\n",
      "Epoch [27/300], Step [60/2], Loss: 1.1201\n",
      "Epoch [27/300], Step [70/2], Loss: 0.8175\n",
      "Epoch [27/300], Step [80/2], Loss: 1.1288\n",
      "Epoch [27/300], Step [90/2], Loss: 1.0672\n",
      "Epoch [27/300], Step [100/2], Loss: 0.5498\n",
      "Epoch [27/300], Step [110/2], Loss: 0.5273\n",
      "Epoch [27/300], Step [120/2], Loss: 0.4043\n",
      "Epoch [27/300], Step [130/2], Loss: 0.6818\n",
      "Epoch [27/300], Step [140/2], Loss: 0.3128\n",
      "Epoch [27/300], Step [150/2], Loss: 0.4098\n",
      "Epoch 26; loss = 0.6152\n",
      "Epoch [28/300], Step [10/2], Loss: 0.2179\n",
      "Epoch [28/300], Step [20/2], Loss: 0.6291\n",
      "Epoch [28/300], Step [30/2], Loss: 1.0437\n",
      "Epoch [28/300], Step [40/2], Loss: 0.2871\n",
      "Epoch [28/300], Step [50/2], Loss: 0.6830\n",
      "Epoch [28/300], Step [60/2], Loss: 1.1169\n",
      "Epoch [28/300], Step [70/2], Loss: 0.7875\n",
      "Epoch [28/300], Step [80/2], Loss: 1.1392\n",
      "Epoch [28/300], Step [90/2], Loss: 1.0625\n",
      "Epoch [28/300], Step [100/2], Loss: 0.5440\n",
      "Epoch [28/300], Step [110/2], Loss: 0.5805\n",
      "Epoch [28/300], Step [120/2], Loss: 0.4160\n",
      "Epoch [28/300], Step [130/2], Loss: 1.0012\n",
      "Epoch [28/300], Step [140/2], Loss: 0.3637\n",
      "Epoch [28/300], Step [150/2], Loss: 0.4464\n",
      "Epoch 27; loss = 0.6232\n",
      "Epoch [29/300], Step [10/2], Loss: 0.2372\n",
      "Epoch [29/300], Step [20/2], Loss: 0.5995\n",
      "Epoch [29/300], Step [30/2], Loss: 1.0425\n",
      "Epoch [29/300], Step [40/2], Loss: 0.2719\n",
      "Epoch [29/300], Step [50/2], Loss: 0.6088\n",
      "Epoch [29/300], Step [60/2], Loss: 1.0505\n",
      "Epoch [29/300], Step [70/2], Loss: 0.6506\n",
      "Epoch [29/300], Step [80/2], Loss: 1.1388\n",
      "Epoch [29/300], Step [90/2], Loss: 1.0096\n",
      "Epoch [29/300], Step [100/2], Loss: 0.5743\n",
      "Epoch [29/300], Step [110/2], Loss: 0.5770\n",
      "Epoch [29/300], Step [120/2], Loss: 0.4402\n",
      "Epoch [29/300], Step [130/2], Loss: 0.9064\n",
      "Epoch [29/300], Step [140/2], Loss: 0.3807\n",
      "Epoch [29/300], Step [150/2], Loss: 0.4555\n",
      "Epoch 28; loss = 0.6215\n",
      "Epoch [30/300], Step [10/2], Loss: 0.2381\n",
      "Epoch [30/300], Step [20/2], Loss: 0.6342\n",
      "Epoch [30/300], Step [30/2], Loss: 1.0188\n",
      "Epoch [30/300], Step [40/2], Loss: 0.2210\n",
      "Epoch [30/300], Step [50/2], Loss: 0.6216\n",
      "Epoch [30/300], Step [60/2], Loss: 1.0380\n",
      "Epoch [30/300], Step [70/2], Loss: 0.6707\n",
      "Epoch [30/300], Step [80/2], Loss: 1.1485\n",
      "Epoch [30/300], Step [90/2], Loss: 1.0031\n",
      "Epoch [30/300], Step [100/2], Loss: 0.5658\n",
      "Epoch [30/300], Step [110/2], Loss: 0.5601\n",
      "Epoch [30/300], Step [120/2], Loss: 0.4457\n",
      "Epoch [30/300], Step [130/2], Loss: 0.8580\n",
      "Epoch [30/300], Step [140/2], Loss: 0.3592\n",
      "Epoch [30/300], Step [150/2], Loss: 0.4603\n",
      "Epoch 29; loss = 0.6181\n",
      "Epoch [31/300], Step [10/2], Loss: 0.2248\n",
      "Epoch [31/300], Step [20/2], Loss: 0.6480\n",
      "Epoch [31/300], Step [30/2], Loss: 1.0067\n",
      "Epoch [31/300], Step [40/2], Loss: 0.2215\n",
      "Epoch [31/300], Step [50/2], Loss: 0.6274\n",
      "Epoch [31/300], Step [60/2], Loss: 1.0301\n",
      "Epoch [31/300], Step [70/2], Loss: 0.6860\n",
      "Epoch [31/300], Step [80/2], Loss: 1.1430\n",
      "Epoch [31/300], Step [90/2], Loss: 0.9982\n",
      "Epoch [31/300], Step [100/2], Loss: 0.5661\n",
      "Epoch [31/300], Step [110/2], Loss: 0.5588\n",
      "Epoch [31/300], Step [120/2], Loss: 0.4486\n",
      "Epoch [31/300], Step [130/2], Loss: 0.8425\n",
      "Epoch [31/300], Step [140/2], Loss: 0.3607\n",
      "Epoch [31/300], Step [150/2], Loss: 0.4620\n",
      "Epoch 30; loss = 0.6174\n",
      "Epoch [32/300], Step [10/2], Loss: 0.2390\n",
      "Epoch [32/300], Step [20/2], Loss: 0.6467\n",
      "Epoch [32/300], Step [30/2], Loss: 1.0096\n",
      "Epoch [32/300], Step [40/2], Loss: 0.1850\n",
      "Epoch [32/300], Step [50/2], Loss: 0.6099\n",
      "Epoch [32/300], Step [60/2], Loss: 1.0269\n",
      "Epoch [32/300], Step [70/2], Loss: 0.6904\n",
      "Epoch [32/300], Step [80/2], Loss: 1.1415\n",
      "Epoch [32/300], Step [90/2], Loss: 0.9959\n",
      "Epoch [32/300], Step [100/2], Loss: 0.5633\n",
      "Epoch [32/300], Step [110/2], Loss: 0.5594\n",
      "Epoch [32/300], Step [120/2], Loss: 0.4503\n",
      "Epoch [32/300], Step [130/2], Loss: 0.8272\n",
      "Epoch [32/300], Step [140/2], Loss: 0.3545\n",
      "Epoch [32/300], Step [150/2], Loss: 0.4628\n",
      "Epoch 31; loss = 0.6162\n",
      "Epoch [33/300], Step [10/2], Loss: 0.2392\n",
      "Epoch [33/300], Step [20/2], Loss: 0.6499\n",
      "Epoch [33/300], Step [30/2], Loss: 1.0154\n",
      "Epoch [33/300], Step [40/2], Loss: 0.2877\n",
      "Epoch [33/300], Step [50/2], Loss: 0.5008\n",
      "Epoch [33/300], Step [60/2], Loss: 1.0204\n",
      "Epoch [33/300], Step [70/2], Loss: 0.6557\n",
      "Epoch [33/300], Step [80/2], Loss: 1.0918\n",
      "Epoch [33/300], Step [90/2], Loss: 0.9854\n",
      "Epoch [33/300], Step [100/2], Loss: 0.6032\n",
      "Epoch [33/300], Step [110/2], Loss: 0.6081\n",
      "Epoch [33/300], Step [120/2], Loss: 0.4539\n",
      "Epoch [33/300], Step [130/2], Loss: 0.8654\n",
      "Epoch [33/300], Step [140/2], Loss: 0.4504\n",
      "Epoch [33/300], Step [150/2], Loss: 0.4721\n",
      "Epoch 32; loss = 0.6241\n",
      "Epoch [34/300], Step [10/2], Loss: 0.2643\n",
      "Epoch [34/300], Step [20/2], Loss: 0.6595\n",
      "Epoch [34/300], Step [30/2], Loss: 0.9976\n",
      "Epoch [34/300], Step [40/2], Loss: 0.1553\n",
      "Epoch [34/300], Step [50/2], Loss: 0.5541\n",
      "Epoch [34/300], Step [60/2], Loss: 1.0189\n",
      "Epoch [34/300], Step [70/2], Loss: 0.7003\n",
      "Epoch [34/300], Step [80/2], Loss: 1.1210\n",
      "Epoch [34/300], Step [90/2], Loss: 0.9883\n",
      "Epoch [34/300], Step [100/2], Loss: 0.6003\n",
      "Epoch [34/300], Step [110/2], Loss: 0.5924\n",
      "Epoch [34/300], Step [120/2], Loss: 0.4553\n",
      "Epoch [34/300], Step [130/2], Loss: 0.8449\n",
      "Epoch [34/300], Step [140/2], Loss: 0.4083\n",
      "Epoch [34/300], Step [150/2], Loss: 0.4715\n",
      "Epoch 33; loss = 0.6189\n",
      "Epoch [35/300], Step [10/2], Loss: 0.2558\n",
      "Epoch [35/300], Step [20/2], Loss: 0.6824\n",
      "Epoch [35/300], Step [30/2], Loss: 0.9871\n",
      "Epoch [35/300], Step [40/2], Loss: 0.1367\n",
      "Epoch [35/300], Step [50/2], Loss: 0.5590\n",
      "Epoch [35/300], Step [60/2], Loss: 1.0164\n",
      "Epoch [35/300], Step [70/2], Loss: 0.7096\n",
      "Epoch [35/300], Step [80/2], Loss: 1.1188\n",
      "Epoch [35/300], Step [90/2], Loss: 0.9902\n",
      "Epoch [35/300], Step [100/2], Loss: 0.5604\n",
      "Epoch [35/300], Step [110/2], Loss: 0.5373\n",
      "Epoch [35/300], Step [120/2], Loss: 0.4528\n",
      "Epoch [35/300], Step [130/2], Loss: 0.7885\n",
      "Epoch [35/300], Step [140/2], Loss: 0.3127\n",
      "Epoch [35/300], Step [150/2], Loss: 0.4647\n",
      "Epoch 34; loss = 0.6162\n",
      "Epoch [36/300], Step [10/2], Loss: 0.2234\n",
      "Epoch [36/300], Step [20/2], Loss: 0.6893\n",
      "Epoch [36/300], Step [30/2], Loss: 0.9775\n",
      "Epoch [36/300], Step [40/2], Loss: 0.1468\n",
      "Epoch [36/300], Step [50/2], Loss: 0.6354\n",
      "Epoch [36/300], Step [60/2], Loss: 1.0218\n",
      "Epoch [36/300], Step [70/2], Loss: 0.7261\n",
      "Epoch [36/300], Step [80/2], Loss: 1.1330\n",
      "Epoch [36/300], Step [90/2], Loss: 0.9905\n",
      "Epoch [36/300], Step [100/2], Loss: 0.5403\n",
      "Epoch [36/300], Step [110/2], Loss: 0.5563\n",
      "Epoch [36/300], Step [120/2], Loss: 0.4528\n",
      "Epoch [36/300], Step [130/2], Loss: 0.8006\n",
      "Epoch [36/300], Step [140/2], Loss: 0.3720\n",
      "Epoch [36/300], Step [150/2], Loss: 0.4647\n",
      "Epoch 35; loss = 0.6148\n",
      "Epoch [37/300], Step [10/2], Loss: 0.2352\n",
      "Epoch [37/300], Step [20/2], Loss: 0.6756\n",
      "Epoch [37/300], Step [30/2], Loss: 0.9860\n",
      "Epoch [37/300], Step [40/2], Loss: 0.1695\n",
      "Epoch [37/300], Step [50/2], Loss: 0.5969\n",
      "Epoch [37/300], Step [60/2], Loss: 1.0244\n",
      "Epoch [37/300], Step [70/2], Loss: 0.7117\n",
      "Epoch [37/300], Step [80/2], Loss: 1.1346\n",
      "Epoch [37/300], Step [90/2], Loss: 0.9919\n",
      "Epoch [37/300], Step [100/2], Loss: 0.5705\n",
      "Epoch [37/300], Step [110/2], Loss: 0.5750\n",
      "Epoch [37/300], Step [120/2], Loss: 0.4517\n",
      "Epoch [37/300], Step [130/2], Loss: 0.7927\n",
      "Epoch [37/300], Step [140/2], Loss: 0.3644\n",
      "Epoch [37/300], Step [150/2], Loss: 0.4649\n",
      "Epoch 36; loss = 0.6158\n",
      "Epoch [38/300], Step [10/2], Loss: 0.2318\n",
      "Epoch [38/300], Step [20/2], Loss: 0.6765\n",
      "Epoch [38/300], Step [30/2], Loss: 0.9820\n",
      "Epoch [38/300], Step [40/2], Loss: 0.1456\n",
      "Epoch [38/300], Step [50/2], Loss: 0.6040\n",
      "Epoch [38/300], Step [60/2], Loss: 1.0273\n",
      "Epoch [38/300], Step [70/2], Loss: 0.7161\n",
      "Epoch [38/300], Step [80/2], Loss: 1.1323\n",
      "Epoch [38/300], Step [90/2], Loss: 0.9943\n",
      "Epoch [38/300], Step [100/2], Loss: 0.6036\n",
      "Epoch [38/300], Step [110/2], Loss: 0.5848\n",
      "Epoch [38/300], Step [120/2], Loss: 0.4509\n",
      "Epoch [38/300], Step [130/2], Loss: 0.8362\n",
      "Epoch [38/300], Step [140/2], Loss: 0.3613\n",
      "Epoch [38/300], Step [150/2], Loss: 0.4680\n",
      "Epoch 37; loss = 0.6151\n",
      "Epoch [39/300], Step [10/2], Loss: 0.2264\n",
      "Epoch [39/300], Step [20/2], Loss: 0.6752\n",
      "Epoch [39/300], Step [30/2], Loss: 0.9781\n",
      "Epoch [39/300], Step [40/2], Loss: 0.1375\n",
      "Epoch [39/300], Step [50/2], Loss: 0.5714\n",
      "Epoch [39/300], Step [60/2], Loss: 1.0196\n",
      "Epoch [39/300], Step [70/2], Loss: 0.7053\n",
      "Epoch [39/300], Step [80/2], Loss: 1.0980\n",
      "Epoch [39/300], Step [90/2], Loss: 0.9864\n",
      "Epoch [39/300], Step [100/2], Loss: 0.6016\n",
      "Epoch [39/300], Step [110/2], Loss: 0.5895\n",
      "Epoch [39/300], Step [120/2], Loss: 0.4539\n",
      "Epoch [39/300], Step [130/2], Loss: 0.8353\n",
      "Epoch [39/300], Step [140/2], Loss: 0.3694\n",
      "Epoch [39/300], Step [150/2], Loss: 0.4700\n",
      "Epoch 38; loss = 0.6169\n",
      "Epoch [40/300], Step [10/2], Loss: 0.2250\n",
      "Epoch [40/300], Step [20/2], Loss: 0.6611\n",
      "Epoch [40/300], Step [30/2], Loss: 0.9677\n",
      "Epoch [40/300], Step [40/2], Loss: 0.1732\n",
      "Epoch [40/300], Step [50/2], Loss: 0.5864\n",
      "Epoch [40/300], Step [60/2], Loss: 1.0217\n",
      "Epoch [40/300], Step [70/2], Loss: 0.7337\n",
      "Epoch [40/300], Step [80/2], Loss: 1.1108\n",
      "Epoch [40/300], Step [90/2], Loss: 0.9906\n",
      "Epoch [40/300], Step [100/2], Loss: 0.5997\n",
      "Epoch [40/300], Step [110/2], Loss: 0.5835\n",
      "Epoch [40/300], Step [120/2], Loss: 0.4530\n",
      "Epoch [40/300], Step [130/2], Loss: 0.8279\n",
      "Epoch [40/300], Step [140/2], Loss: 0.3614\n",
      "Epoch [40/300], Step [150/2], Loss: 0.4699\n",
      "Epoch 39; loss = 0.6169\n",
      "Epoch [41/300], Step [10/2], Loss: 0.2587\n",
      "Epoch [41/300], Step [20/2], Loss: 0.6817\n",
      "Epoch [41/300], Step [30/2], Loss: 0.9665\n",
      "Epoch [41/300], Step [40/2], Loss: 0.1231\n",
      "Epoch [41/300], Step [50/2], Loss: 0.5957\n",
      "Epoch [41/300], Step [60/2], Loss: 1.0233\n",
      "Epoch [41/300], Step [70/2], Loss: 0.7314\n",
      "Epoch [41/300], Step [80/2], Loss: 1.1055\n",
      "Epoch [41/300], Step [90/2], Loss: 0.9937\n",
      "Epoch [41/300], Step [100/2], Loss: 0.6001\n",
      "Epoch [41/300], Step [110/2], Loss: 0.5810\n",
      "Epoch [41/300], Step [120/2], Loss: 0.4516\n",
      "Epoch [41/300], Step [130/2], Loss: 0.8262\n",
      "Epoch [41/300], Step [140/2], Loss: 0.3708\n",
      "Epoch [41/300], Step [150/2], Loss: 0.4683\n",
      "Epoch 40; loss = 0.6137\n",
      "Epoch [42/300], Step [10/2], Loss: 0.2627\n",
      "Epoch [42/300], Step [20/2], Loss: 0.6842\n",
      "Epoch [42/300], Step [30/2], Loss: 0.9658\n",
      "Epoch [42/300], Step [40/2], Loss: 0.1257\n",
      "Epoch [42/300], Step [50/2], Loss: 0.6018\n",
      "Epoch [42/300], Step [60/2], Loss: 1.0290\n",
      "Epoch [42/300], Step [70/2], Loss: 0.7305\n",
      "Epoch [42/300], Step [80/2], Loss: 1.1443\n",
      "Epoch [42/300], Step [90/2], Loss: 0.9981\n",
      "Epoch [42/300], Step [100/2], Loss: 0.5646\n",
      "Epoch [42/300], Step [110/2], Loss: 0.5765\n",
      "Epoch [42/300], Step [120/2], Loss: 0.4512\n",
      "Epoch [42/300], Step [130/2], Loss: 0.8199\n",
      "Epoch [42/300], Step [140/2], Loss: 0.3579\n",
      "Epoch [42/300], Step [150/2], Loss: 0.4665\n",
      "Epoch 41; loss = 0.6133\n",
      "Epoch [43/300], Step [10/2], Loss: 0.2324\n",
      "Epoch [43/300], Step [20/2], Loss: 0.6853\n",
      "Epoch [43/300], Step [30/2], Loss: 0.9605\n",
      "Epoch [43/300], Step [40/2], Loss: 0.1245\n",
      "Epoch [43/300], Step [50/2], Loss: 0.6069\n",
      "Epoch [43/300], Step [60/2], Loss: 1.0264\n",
      "Epoch [43/300], Step [70/2], Loss: 0.7197\n",
      "Epoch [43/300], Step [80/2], Loss: 1.1037\n",
      "Epoch [43/300], Step [90/2], Loss: 0.9919\n",
      "Epoch [43/300], Step [100/2], Loss: 0.5975\n",
      "Epoch [43/300], Step [110/2], Loss: 0.5811\n",
      "Epoch [43/300], Step [120/2], Loss: 0.4511\n",
      "Epoch [43/300], Step [130/2], Loss: 0.8251\n",
      "Epoch [43/300], Step [140/2], Loss: 0.3728\n",
      "Epoch [43/300], Step [150/2], Loss: 0.4683\n",
      "Epoch 42; loss = 0.6136\n",
      "Epoch [44/300], Step [10/2], Loss: 0.2152\n",
      "Epoch [44/300], Step [20/2], Loss: 0.6845\n",
      "Epoch [44/300], Step [30/2], Loss: 0.9557\n",
      "Epoch [44/300], Step [40/2], Loss: 0.1195\n",
      "Epoch [44/300], Step [50/2], Loss: 0.6207\n",
      "Epoch [44/300], Step [60/2], Loss: 1.0288\n",
      "Epoch [44/300], Step [70/2], Loss: 0.7239\n",
      "Epoch [44/300], Step [80/2], Loss: 1.1261\n",
      "Epoch [44/300], Step [90/2], Loss: 0.9954\n",
      "Epoch [44/300], Step [100/2], Loss: 0.6014\n",
      "Epoch [44/300], Step [110/2], Loss: 0.6294\n",
      "Epoch [44/300], Step [120/2], Loss: 0.4496\n",
      "Epoch [44/300], Step [130/2], Loss: 0.8261\n",
      "Epoch [44/300], Step [140/2], Loss: 0.4279\n",
      "Epoch [44/300], Step [150/2], Loss: 0.4651\n",
      "Epoch 43; loss = 0.6156\n",
      "Epoch [45/300], Step [10/2], Loss: 0.2604\n",
      "Epoch [45/300], Step [20/2], Loss: 0.7218\n",
      "Epoch [45/300], Step [30/2], Loss: 0.9644\n",
      "Epoch [45/300], Step [40/2], Loss: 0.1280\n",
      "Epoch [45/300], Step [50/2], Loss: 0.5798\n",
      "Epoch [45/300], Step [60/2], Loss: 1.0313\n",
      "Epoch [45/300], Step [70/2], Loss: 0.7656\n",
      "Epoch [45/300], Step [80/2], Loss: 1.1523\n",
      "Epoch [45/300], Step [90/2], Loss: 1.0004\n",
      "Epoch [45/300], Step [100/2], Loss: 0.5994\n",
      "Epoch [45/300], Step [110/2], Loss: 0.5864\n",
      "Epoch [45/300], Step [120/2], Loss: 0.4481\n",
      "Epoch [45/300], Step [130/2], Loss: 0.8088\n",
      "Epoch [45/300], Step [140/2], Loss: 0.3831\n",
      "Epoch [45/300], Step [150/2], Loss: 0.4635\n",
      "Epoch 44; loss = 0.6142\n",
      "Epoch [46/300], Step [10/2], Loss: 0.2286\n",
      "Epoch [46/300], Step [20/2], Loss: 0.7190\n",
      "Epoch [46/300], Step [30/2], Loss: 0.9537\n",
      "Epoch [46/300], Step [40/2], Loss: 0.1232\n",
      "Epoch [46/300], Step [50/2], Loss: 0.5976\n",
      "Epoch [46/300], Step [60/2], Loss: 1.0340\n",
      "Epoch [46/300], Step [70/2], Loss: 0.7680\n",
      "Epoch [46/300], Step [80/2], Loss: 1.1257\n",
      "Epoch [46/300], Step [90/2], Loss: 1.0027\n",
      "Epoch [46/300], Step [100/2], Loss: 0.5941\n",
      "Epoch [46/300], Step [110/2], Loss: 0.5761\n",
      "Epoch [46/300], Step [120/2], Loss: 0.4473\n",
      "Epoch [46/300], Step [130/2], Loss: 0.8173\n",
      "Epoch [46/300], Step [140/2], Loss: 0.3381\n",
      "Epoch [46/300], Step [150/2], Loss: 0.4609\n",
      "Epoch 45; loss = 0.6125\n",
      "Epoch [47/300], Step [10/2], Loss: 0.2000\n",
      "Epoch [47/300], Step [20/2], Loss: 0.7272\n",
      "Epoch [47/300], Step [30/2], Loss: 0.9416\n",
      "Epoch [47/300], Step [40/2], Loss: 0.1252\n",
      "Epoch [47/300], Step [50/2], Loss: 0.6134\n",
      "Epoch [47/300], Step [60/2], Loss: 1.0117\n",
      "Epoch [47/300], Step [70/2], Loss: 0.7579\n",
      "Epoch [47/300], Step [80/2], Loss: 1.1093\n",
      "Epoch [47/300], Step [90/2], Loss: 0.9890\n",
      "Epoch [47/300], Step [100/2], Loss: 0.5872\n",
      "Epoch [47/300], Step [110/2], Loss: 0.5765\n",
      "Epoch [47/300], Step [120/2], Loss: 0.4557\n",
      "Epoch [47/300], Step [130/2], Loss: 0.8092\n",
      "Epoch [47/300], Step [140/2], Loss: 0.4507\n",
      "Epoch [47/300], Step [150/2], Loss: 0.4644\n",
      "Epoch 46; loss = 0.6201\n",
      "Epoch [48/300], Step [10/2], Loss: 0.3004\n",
      "Epoch [48/300], Step [20/2], Loss: 0.6710\n",
      "Epoch [48/300], Step [30/2], Loss: 0.9749\n",
      "Epoch [48/300], Step [40/2], Loss: 0.2392\n",
      "Epoch [48/300], Step [50/2], Loss: 0.5587\n",
      "Epoch [48/300], Step [60/2], Loss: 1.0440\n",
      "Epoch [48/300], Step [70/2], Loss: 0.7738\n",
      "Epoch [48/300], Step [80/2], Loss: 1.1505\n",
      "Epoch [48/300], Step [90/2], Loss: 1.0099\n",
      "Epoch [48/300], Step [100/2], Loss: 0.5643\n",
      "Epoch [48/300], Step [110/2], Loss: 0.5679\n",
      "Epoch [48/300], Step [120/2], Loss: 0.4414\n",
      "Epoch [48/300], Step [130/2], Loss: 0.7390\n",
      "Epoch [48/300], Step [140/2], Loss: 0.3678\n",
      "Epoch [48/300], Step [150/2], Loss: 0.4505\n",
      "Epoch 47; loss = 0.6152\n",
      "Epoch [49/300], Step [10/2], Loss: 0.2495\n",
      "Epoch [49/300], Step [20/2], Loss: 0.6947\n",
      "Epoch [49/300], Step [30/2], Loss: 0.9787\n",
      "Epoch [49/300], Step [40/2], Loss: 0.1830\n",
      "Epoch [49/300], Step [50/2], Loss: 0.5578\n",
      "Epoch [49/300], Step [60/2], Loss: 1.0507\n",
      "Epoch [49/300], Step [70/2], Loss: 0.7677\n",
      "Epoch [49/300], Step [80/2], Loss: 1.1228\n",
      "Epoch [49/300], Step [90/2], Loss: 1.0146\n",
      "Epoch [49/300], Step [100/2], Loss: 0.6047\n",
      "Epoch [49/300], Step [110/2], Loss: 0.5998\n",
      "Epoch [49/300], Step [120/2], Loss: 0.4388\n",
      "Epoch [49/300], Step [130/2], Loss: 0.8299\n",
      "Epoch [49/300], Step [140/2], Loss: 0.3668\n",
      "Epoch [49/300], Step [150/2], Loss: 0.4535\n",
      "Epoch 48; loss = 0.6135\n",
      "Epoch [50/300], Step [10/2], Loss: 0.2276\n",
      "Epoch [50/300], Step [20/2], Loss: 0.7051\n",
      "Epoch [50/300], Step [30/2], Loss: 0.9689\n",
      "Epoch [50/300], Step [40/2], Loss: 0.1227\n",
      "Epoch [50/300], Step [50/2], Loss: 0.6166\n",
      "Epoch [50/300], Step [60/2], Loss: 1.0419\n",
      "Epoch [50/300], Step [70/2], Loss: 0.7702\n",
      "Epoch [50/300], Step [80/2], Loss: 1.1172\n",
      "Epoch [50/300], Step [90/2], Loss: 1.0087\n",
      "Epoch [50/300], Step [100/2], Loss: 0.6036\n",
      "Epoch [50/300], Step [110/2], Loss: 0.5865\n",
      "Epoch [50/300], Step [120/2], Loss: 0.4457\n",
      "Epoch [50/300], Step [130/2], Loss: 0.8292\n",
      "Epoch [50/300], Step [140/2], Loss: 0.3719\n",
      "Epoch [50/300], Step [150/2], Loss: 0.4627\n",
      "Epoch 49; loss = 0.6136\n",
      "Epoch [51/300], Step [10/2], Loss: 0.2254\n",
      "Epoch [51/300], Step [20/2], Loss: 0.6851\n",
      "Epoch [51/300], Step [30/2], Loss: 0.9538\n",
      "Epoch [51/300], Step [40/2], Loss: 0.1311\n",
      "Epoch [51/300], Step [50/2], Loss: 0.5982\n",
      "Epoch [51/300], Step [60/2], Loss: 1.0365\n",
      "Epoch [51/300], Step [70/2], Loss: 0.7790\n",
      "Epoch [51/300], Step [80/2], Loss: 1.1168\n",
      "Epoch [51/300], Step [90/2], Loss: 1.0057\n",
      "Epoch [51/300], Step [100/2], Loss: 0.5961\n",
      "Epoch [51/300], Step [110/2], Loss: 0.5738\n",
      "Epoch [51/300], Step [120/2], Loss: 0.4463\n",
      "Epoch [51/300], Step [130/2], Loss: 0.8082\n",
      "Epoch [51/300], Step [140/2], Loss: 0.3506\n",
      "Epoch [51/300], Step [150/2], Loss: 0.4606\n",
      "Epoch 50; loss = 0.6106\n",
      "Epoch [52/300], Step [10/2], Loss: 0.2176\n",
      "Epoch [52/300], Step [20/2], Loss: 0.6781\n",
      "Epoch [52/300], Step [30/2], Loss: 0.9531\n",
      "Epoch [52/300], Step [40/2], Loss: 0.1199\n",
      "Epoch [52/300], Step [50/2], Loss: 0.6281\n",
      "Epoch [52/300], Step [60/2], Loss: 1.0396\n",
      "Epoch [52/300], Step [70/2], Loss: 0.7704\n",
      "Epoch [52/300], Step [80/2], Loss: 1.1147\n",
      "Epoch [52/300], Step [90/2], Loss: 1.0081\n",
      "Epoch [52/300], Step [100/2], Loss: 0.5970\n",
      "Epoch [52/300], Step [110/2], Loss: 0.5790\n",
      "Epoch [52/300], Step [120/2], Loss: 0.4489\n",
      "Epoch [52/300], Step [130/2], Loss: 0.8133\n",
      "Epoch [52/300], Step [140/2], Loss: 0.3632\n",
      "Epoch [52/300], Step [150/2], Loss: 0.4655\n",
      "Epoch 51; loss = 0.6124\n",
      "Epoch [53/300], Step [10/2], Loss: 0.2207\n",
      "Epoch [53/300], Step [20/2], Loss: 0.6781\n",
      "Epoch [53/300], Step [30/2], Loss: 0.9455\n",
      "Epoch [53/300], Step [40/2], Loss: 0.1301\n",
      "Epoch [53/300], Step [50/2], Loss: 0.6240\n",
      "Epoch [53/300], Step [60/2], Loss: 1.0320\n",
      "Epoch [53/300], Step [70/2], Loss: 0.7731\n",
      "Epoch [53/300], Step [80/2], Loss: 1.1325\n",
      "Epoch [53/300], Step [90/2], Loss: 1.0029\n",
      "Epoch [53/300], Step [100/2], Loss: 0.5719\n",
      "Epoch [53/300], Step [110/2], Loss: 0.5541\n",
      "Epoch [53/300], Step [120/2], Loss: 0.4467\n",
      "Epoch [53/300], Step [130/2], Loss: 0.7839\n",
      "Epoch [53/300], Step [140/2], Loss: 0.3022\n",
      "Epoch [53/300], Step [150/2], Loss: 0.4575\n",
      "Epoch 52; loss = 0.6111\n",
      "Epoch [54/300], Step [10/2], Loss: 0.2065\n",
      "Epoch [54/300], Step [20/2], Loss: 0.6664\n",
      "Epoch [54/300], Step [30/2], Loss: 0.9491\n",
      "Epoch [54/300], Step [40/2], Loss: 0.1172\n",
      "Epoch [54/300], Step [50/2], Loss: 0.7273\n",
      "Epoch [54/300], Step [60/2], Loss: 1.0317\n",
      "Epoch [54/300], Step [70/2], Loss: 0.7592\n",
      "Epoch [54/300], Step [80/2], Loss: 1.1150\n",
      "Epoch [54/300], Step [90/2], Loss: 1.0013\n",
      "Epoch [54/300], Step [100/2], Loss: 0.5826\n",
      "Epoch [54/300], Step [110/2], Loss: 0.5531\n",
      "Epoch [54/300], Step [120/2], Loss: 0.4487\n",
      "Epoch [54/300], Step [130/2], Loss: 0.7864\n",
      "Epoch [54/300], Step [140/2], Loss: 0.3450\n",
      "Epoch [54/300], Step [150/2], Loss: 0.4600\n",
      "Epoch 53; loss = 0.6098\n",
      "Epoch [55/300], Step [10/2], Loss: 0.2258\n",
      "Epoch [55/300], Step [20/2], Loss: 0.6395\n",
      "Epoch [55/300], Step [30/2], Loss: 0.9578\n",
      "Epoch [55/300], Step [40/2], Loss: 0.1312\n",
      "Epoch [55/300], Step [50/2], Loss: 0.6264\n",
      "Epoch [55/300], Step [60/2], Loss: 1.0435\n",
      "Epoch [55/300], Step [70/2], Loss: 0.7547\n",
      "Epoch [55/300], Step [80/2], Loss: 1.1096\n",
      "Epoch [55/300], Step [90/2], Loss: 1.0107\n",
      "Epoch [55/300], Step [100/2], Loss: 0.5755\n",
      "Epoch [55/300], Step [110/2], Loss: 0.5690\n",
      "Epoch [55/300], Step [120/2], Loss: 0.4431\n",
      "Epoch [55/300], Step [130/2], Loss: 0.8186\n",
      "Epoch [55/300], Step [140/2], Loss: 0.3330\n",
      "Epoch [55/300], Step [150/2], Loss: 0.4584\n",
      "Epoch 54; loss = 0.6076\n",
      "Epoch [56/300], Step [10/2], Loss: 0.2091\n",
      "Epoch [56/300], Step [20/2], Loss: 0.6525\n",
      "Epoch [56/300], Step [30/2], Loss: 0.9474\n",
      "Epoch [56/300], Step [40/2], Loss: 0.1123\n",
      "Epoch [56/300], Step [50/2], Loss: 0.7519\n",
      "Epoch [56/300], Step [60/2], Loss: 1.0360\n",
      "Epoch [56/300], Step [70/2], Loss: 0.7466\n",
      "Epoch [56/300], Step [80/2], Loss: 1.1086\n",
      "Epoch [56/300], Step [90/2], Loss: 1.0027\n",
      "Epoch [56/300], Step [100/2], Loss: 0.5842\n",
      "Epoch [56/300], Step [110/2], Loss: 0.5604\n",
      "Epoch [56/300], Step [120/2], Loss: 0.4461\n",
      "Epoch [56/300], Step [130/2], Loss: 0.7990\n",
      "Epoch [56/300], Step [140/2], Loss: 0.3033\n",
      "Epoch [56/300], Step [150/2], Loss: 0.4569\n",
      "Epoch 55; loss = 0.6084\n",
      "Epoch [57/300], Step [10/2], Loss: 0.2060\n",
      "Epoch [57/300], Step [20/2], Loss: 0.6316\n",
      "Epoch [57/300], Step [30/2], Loss: 0.9490\n",
      "Epoch [57/300], Step [40/2], Loss: 0.1280\n",
      "Epoch [57/300], Step [50/2], Loss: 0.6960\n",
      "Epoch [57/300], Step [60/2], Loss: 1.0344\n",
      "Epoch [57/300], Step [70/2], Loss: 0.7472\n",
      "Epoch [57/300], Step [80/2], Loss: 1.1066\n",
      "Epoch [57/300], Step [90/2], Loss: 1.0022\n",
      "Epoch [57/300], Step [100/2], Loss: 0.5890\n",
      "Epoch [57/300], Step [110/2], Loss: 0.5656\n",
      "Epoch [57/300], Step [120/2], Loss: 0.4473\n",
      "Epoch [57/300], Step [130/2], Loss: 0.8058\n",
      "Epoch [57/300], Step [140/2], Loss: 0.2999\n",
      "Epoch [57/300], Step [150/2], Loss: 0.4584\n",
      "Epoch 56; loss = 0.6080\n",
      "Epoch [58/300], Step [10/2], Loss: 0.2103\n",
      "Epoch [58/300], Step [20/2], Loss: 0.6254\n",
      "Epoch [58/300], Step [30/2], Loss: 0.9509\n",
      "Epoch [58/300], Step [40/2], Loss: 0.1327\n",
      "Epoch [58/300], Step [50/2], Loss: 0.6662\n",
      "Epoch [58/300], Step [60/2], Loss: 1.0326\n",
      "Epoch [58/300], Step [70/2], Loss: 0.7433\n",
      "Epoch [58/300], Step [80/2], Loss: 1.0999\n",
      "Epoch [58/300], Step [90/2], Loss: 0.9996\n",
      "Epoch [58/300], Step [100/2], Loss: 0.5946\n",
      "Epoch [58/300], Step [110/2], Loss: 0.5692\n",
      "Epoch [58/300], Step [120/2], Loss: 0.4482\n",
      "Epoch [58/300], Step [130/2], Loss: 0.8227\n",
      "Epoch [58/300], Step [140/2], Loss: 0.3209\n",
      "Epoch [58/300], Step [150/2], Loss: 0.4599\n",
      "Epoch 57; loss = 0.6070\n",
      "Epoch [59/300], Step [10/2], Loss: 0.2141\n",
      "Epoch [59/300], Step [20/2], Loss: 0.6245\n",
      "Epoch [59/300], Step [30/2], Loss: 0.9487\n",
      "Epoch [59/300], Step [40/2], Loss: 0.1370\n",
      "Epoch [59/300], Step [50/2], Loss: 0.6603\n",
      "Epoch [59/300], Step [60/2], Loss: 1.0327\n",
      "Epoch [59/300], Step [70/2], Loss: 0.7501\n",
      "Epoch [59/300], Step [80/2], Loss: 1.0999\n",
      "Epoch [59/300], Step [90/2], Loss: 1.0005\n",
      "Epoch [59/300], Step [100/2], Loss: 0.5963\n",
      "Epoch [59/300], Step [110/2], Loss: 0.5640\n",
      "Epoch [59/300], Step [120/2], Loss: 0.4483\n",
      "Epoch [59/300], Step [130/2], Loss: 0.8025\n",
      "Epoch [59/300], Step [140/2], Loss: 0.3189\n",
      "Epoch [59/300], Step [150/2], Loss: 0.4589\n",
      "Epoch 58; loss = 0.6069\n",
      "Epoch [60/300], Step [10/2], Loss: 0.2136\n",
      "Epoch [60/300], Step [20/2], Loss: 0.6178\n",
      "Epoch [60/300], Step [30/2], Loss: 0.9495\n",
      "Epoch [60/300], Step [40/2], Loss: 0.1323\n",
      "Epoch [60/300], Step [50/2], Loss: 0.6432\n",
      "Epoch [60/300], Step [60/2], Loss: 1.0439\n",
      "Epoch [60/300], Step [70/2], Loss: 0.7366\n",
      "Epoch [60/300], Step [80/2], Loss: 1.1127\n",
      "Epoch [60/300], Step [90/2], Loss: 1.0113\n",
      "Epoch [60/300], Step [100/2], Loss: 0.5882\n",
      "Epoch [60/300], Step [110/2], Loss: 0.5680\n",
      "Epoch [60/300], Step [120/2], Loss: 0.4435\n",
      "Epoch [60/300], Step [130/2], Loss: 0.8001\n",
      "Epoch [60/300], Step [140/2], Loss: 0.3183\n",
      "Epoch [60/300], Step [150/2], Loss: 0.4548\n",
      "Epoch 59; loss = 0.6058\n",
      "Epoch [61/300], Step [10/2], Loss: 0.2141\n",
      "Epoch [61/300], Step [20/2], Loss: 0.6148\n",
      "Epoch [61/300], Step [30/2], Loss: 0.9583\n",
      "Epoch [61/300], Step [40/2], Loss: 0.1199\n",
      "Epoch [61/300], Step [50/2], Loss: 0.6607\n",
      "Epoch [61/300], Step [60/2], Loss: 1.0476\n",
      "Epoch [61/300], Step [70/2], Loss: 0.7453\n",
      "Epoch [61/300], Step [80/2], Loss: 1.1211\n",
      "Epoch [61/300], Step [90/2], Loss: 1.0121\n",
      "Epoch [61/300], Step [100/2], Loss: 0.5979\n",
      "Epoch [61/300], Step [110/2], Loss: 0.5742\n",
      "Epoch [61/300], Step [120/2], Loss: 0.4423\n",
      "Epoch [61/300], Step [130/2], Loss: 0.8232\n",
      "Epoch [61/300], Step [140/2], Loss: 0.3272\n",
      "Epoch [61/300], Step [150/2], Loss: 0.4591\n",
      "Epoch 60; loss = 0.6073\n",
      "Epoch [62/300], Step [10/2], Loss: 0.2201\n",
      "Epoch [62/300], Step [20/2], Loss: 0.6218\n",
      "Epoch [62/300], Step [30/2], Loss: 0.9541\n",
      "Epoch [62/300], Step [40/2], Loss: 0.1235\n",
      "Epoch [62/300], Step [50/2], Loss: 0.6591\n",
      "Epoch [62/300], Step [60/2], Loss: 1.0404\n",
      "Epoch [62/300], Step [70/2], Loss: 0.7422\n",
      "Epoch [62/300], Step [80/2], Loss: 1.1050\n",
      "Epoch [62/300], Step [90/2], Loss: 1.0064\n",
      "Epoch [62/300], Step [100/2], Loss: 0.5943\n",
      "Epoch [62/300], Step [110/2], Loss: 0.5838\n",
      "Epoch [62/300], Step [120/2], Loss: 0.4432\n",
      "Epoch [62/300], Step [130/2], Loss: 0.8249\n",
      "Epoch [62/300], Step [140/2], Loss: 0.3323\n",
      "Epoch [62/300], Step [150/2], Loss: 0.4594\n",
      "Epoch 61; loss = 0.6051\n",
      "Epoch [63/300], Step [10/2], Loss: 0.2228\n",
      "Epoch [63/300], Step [20/2], Loss: 0.5948\n",
      "Epoch [63/300], Step [30/2], Loss: 0.9529\n",
      "Epoch [63/300], Step [40/2], Loss: 0.2372\n",
      "Epoch [63/300], Step [50/2], Loss: 0.6275\n",
      "Epoch [63/300], Step [60/2], Loss: 1.0453\n",
      "Epoch [63/300], Step [70/2], Loss: 0.7499\n",
      "Epoch [63/300], Step [80/2], Loss: 1.1147\n",
      "Epoch [63/300], Step [90/2], Loss: 1.0139\n",
      "Epoch [63/300], Step [100/2], Loss: 0.6026\n",
      "Epoch [63/300], Step [110/2], Loss: 0.5811\n",
      "Epoch [63/300], Step [120/2], Loss: 0.4423\n",
      "Epoch [63/300], Step [130/2], Loss: 0.7860\n",
      "Epoch [63/300], Step [140/2], Loss: 0.2863\n",
      "Epoch [63/300], Step [150/2], Loss: 0.4541\n",
      "Epoch 62; loss = 0.6066\n",
      "Epoch [64/300], Step [10/2], Loss: 0.2523\n",
      "Epoch [64/300], Step [20/2], Loss: 0.5930\n",
      "Epoch [64/300], Step [30/2], Loss: 0.9569\n",
      "Epoch [64/300], Step [40/2], Loss: 0.1232\n",
      "Epoch [64/300], Step [50/2], Loss: 0.6762\n",
      "Epoch [64/300], Step [60/2], Loss: 1.0466\n",
      "Epoch [64/300], Step [70/2], Loss: 0.7412\n",
      "Epoch [64/300], Step [80/2], Loss: 1.1023\n",
      "Epoch [64/300], Step [90/2], Loss: 1.0114\n",
      "Epoch [64/300], Step [100/2], Loss: 0.6042\n",
      "Epoch [64/300], Step [110/2], Loss: 0.5857\n",
      "Epoch [64/300], Step [120/2], Loss: 0.4437\n",
      "Epoch [64/300], Step [130/2], Loss: 0.8283\n",
      "Epoch [64/300], Step [140/2], Loss: 0.3625\n",
      "Epoch [64/300], Step [150/2], Loss: 0.4571\n",
      "Epoch 63; loss = 0.6033\n",
      "Epoch [65/300], Step [10/2], Loss: 0.2716\n",
      "Epoch [65/300], Step [20/2], Loss: 0.6106\n",
      "Epoch [65/300], Step [30/2], Loss: 0.9557\n",
      "Epoch [65/300], Step [40/2], Loss: 0.1165\n",
      "Epoch [65/300], Step [50/2], Loss: 0.6935\n",
      "Epoch [65/300], Step [60/2], Loss: 1.0500\n",
      "Epoch [65/300], Step [70/2], Loss: 0.7429\n",
      "Epoch [65/300], Step [80/2], Loss: 1.1296\n",
      "Epoch [65/300], Step [90/2], Loss: 1.0148\n",
      "Epoch [65/300], Step [100/2], Loss: 0.5975\n",
      "Epoch [65/300], Step [110/2], Loss: 0.5836\n",
      "Epoch [65/300], Step [120/2], Loss: 0.4400\n",
      "Epoch [65/300], Step [130/2], Loss: 0.7845\n",
      "Epoch [65/300], Step [140/2], Loss: 0.2898\n",
      "Epoch [65/300], Step [150/2], Loss: 0.4520\n",
      "Epoch 64; loss = 0.6012\n",
      "Epoch [66/300], Step [10/2], Loss: 0.2114\n",
      "Epoch [66/300], Step [20/2], Loss: 0.6442\n",
      "Epoch [66/300], Step [30/2], Loss: 0.9491\n",
      "Epoch [66/300], Step [40/2], Loss: 0.1320\n",
      "Epoch [66/300], Step [50/2], Loss: 0.7745\n",
      "Epoch [66/300], Step [60/2], Loss: 1.0461\n",
      "Epoch [66/300], Step [70/2], Loss: 0.7402\n",
      "Epoch [66/300], Step [80/2], Loss: 1.1005\n",
      "Epoch [66/300], Step [90/2], Loss: 1.0102\n",
      "Epoch [66/300], Step [100/2], Loss: 0.6080\n",
      "Epoch [66/300], Step [110/2], Loss: 0.6074\n",
      "Epoch [66/300], Step [120/2], Loss: 0.4416\n",
      "Epoch [66/300], Step [130/2], Loss: 0.8296\n",
      "Epoch [66/300], Step [140/2], Loss: 0.3280\n",
      "Epoch [66/300], Step [150/2], Loss: 0.4536\n",
      "Epoch 65; loss = 0.6042\n",
      "Epoch [67/300], Step [10/2], Loss: 0.2174\n",
      "Epoch [67/300], Step [20/2], Loss: 0.6119\n",
      "Epoch [67/300], Step [30/2], Loss: 0.9518\n",
      "Epoch [67/300], Step [40/2], Loss: 0.1247\n",
      "Epoch [67/300], Step [50/2], Loss: 0.7238\n",
      "Epoch [67/300], Step [60/2], Loss: 1.0509\n",
      "Epoch [67/300], Step [70/2], Loss: 0.7524\n",
      "Epoch [67/300], Step [80/2], Loss: 1.1172\n",
      "Epoch [67/300], Step [90/2], Loss: 1.0099\n",
      "Epoch [67/300], Step [100/2], Loss: 0.6036\n",
      "Epoch [67/300], Step [110/2], Loss: 0.6054\n",
      "Epoch [67/300], Step [120/2], Loss: 0.4419\n",
      "Epoch [67/300], Step [130/2], Loss: 0.8028\n",
      "Epoch [67/300], Step [140/2], Loss: 0.3255\n",
      "Epoch [67/300], Step [150/2], Loss: 0.4547\n",
      "Epoch 66; loss = 0.6026\n",
      "Epoch [68/300], Step [10/2], Loss: 0.2264\n",
      "Epoch [68/300], Step [20/2], Loss: 0.6000\n",
      "Epoch [68/300], Step [30/2], Loss: 0.9545\n",
      "Epoch [68/300], Step [40/2], Loss: 0.1260\n",
      "Epoch [68/300], Step [50/2], Loss: 0.6676\n",
      "Epoch [68/300], Step [60/2], Loss: 1.0548\n",
      "Epoch [68/300], Step [70/2], Loss: 0.7533\n",
      "Epoch [68/300], Step [80/2], Loss: 1.1214\n",
      "Epoch [68/300], Step [90/2], Loss: 1.0142\n",
      "Epoch [68/300], Step [100/2], Loss: 0.5944\n",
      "Epoch [68/300], Step [110/2], Loss: 0.5921\n",
      "Epoch [68/300], Step [120/2], Loss: 0.4395\n",
      "Epoch [68/300], Step [130/2], Loss: 0.8036\n",
      "Epoch [68/300], Step [140/2], Loss: 0.3139\n",
      "Epoch [68/300], Step [150/2], Loss: 0.4539\n",
      "Epoch 67; loss = 0.6001\n",
      "Epoch [69/300], Step [10/2], Loss: 0.2223\n",
      "Epoch [69/300], Step [20/2], Loss: 0.6039\n",
      "Epoch [69/300], Step [30/2], Loss: 0.9507\n",
      "Epoch [69/300], Step [40/2], Loss: 0.1553\n",
      "Epoch [69/300], Step [50/2], Loss: 0.6093\n",
      "Epoch [69/300], Step [60/2], Loss: 1.0483\n",
      "Epoch [69/300], Step [70/2], Loss: 0.7489\n",
      "Epoch [69/300], Step [80/2], Loss: 1.1211\n",
      "Epoch [69/300], Step [90/2], Loss: 1.0094\n",
      "Epoch [69/300], Step [100/2], Loss: 0.5931\n",
      "Epoch [69/300], Step [110/2], Loss: 0.6052\n",
      "Epoch [69/300], Step [120/2], Loss: 0.4411\n",
      "Epoch [69/300], Step [130/2], Loss: 0.8063\n",
      "Epoch [69/300], Step [140/2], Loss: 0.3011\n",
      "Epoch [69/300], Step [150/2], Loss: 0.4536\n",
      "Epoch 68; loss = 0.6020\n",
      "Epoch [70/300], Step [10/2], Loss: 0.1793\n",
      "Epoch [70/300], Step [20/2], Loss: 0.8561\n",
      "Epoch [70/300], Step [30/2], Loss: 0.9366\n",
      "Epoch [70/300], Step [40/2], Loss: 0.1506\n",
      "Epoch [70/300], Step [50/2], Loss: 0.6660\n",
      "Epoch [70/300], Step [60/2], Loss: 1.0303\n",
      "Epoch [70/300], Step [70/2], Loss: 0.7619\n",
      "Epoch [70/300], Step [80/2], Loss: 1.0844\n",
      "Epoch [70/300], Step [90/2], Loss: 1.0170\n",
      "Epoch [70/300], Step [100/2], Loss: 0.6218\n",
      "Epoch [70/300], Step [110/2], Loss: 0.6207\n",
      "Epoch [70/300], Step [120/2], Loss: 0.4355\n",
      "Epoch [70/300], Step [130/2], Loss: 0.6496\n",
      "Epoch [70/300], Step [140/2], Loss: 0.3091\n",
      "Epoch [70/300], Step [150/2], Loss: 0.4351\n",
      "Epoch 69; loss = 0.6114\n",
      "Epoch [71/300], Step [10/2], Loss: 0.2694\n",
      "Epoch [71/300], Step [20/2], Loss: 0.5674\n",
      "Epoch [71/300], Step [30/2], Loss: 0.9881\n",
      "Epoch [71/300], Step [40/2], Loss: 0.1493\n",
      "Epoch [71/300], Step [50/2], Loss: 0.7589\n",
      "Epoch [71/300], Step [60/2], Loss: 1.0867\n",
      "Epoch [71/300], Step [70/2], Loss: 0.7760\n",
      "Epoch [71/300], Step [80/2], Loss: 1.1085\n",
      "Epoch [71/300], Step [90/2], Loss: 1.0386\n",
      "Epoch [71/300], Step [100/2], Loss: 0.5974\n",
      "Epoch [71/300], Step [110/2], Loss: 0.6089\n",
      "Epoch [71/300], Step [120/2], Loss: 0.4234\n",
      "Epoch [71/300], Step [130/2], Loss: 0.7480\n",
      "Epoch [71/300], Step [140/2], Loss: 0.3037\n",
      "Epoch [71/300], Step [150/2], Loss: 0.4286\n",
      "Epoch 70; loss = 0.6036\n",
      "Epoch [72/300], Step [10/2], Loss: 0.2148\n",
      "Epoch [72/300], Step [20/2], Loss: 0.6438\n",
      "Epoch [72/300], Step [30/2], Loss: 0.9920\n",
      "Epoch [72/300], Step [40/2], Loss: 0.1422\n",
      "Epoch [72/300], Step [50/2], Loss: 0.7363\n",
      "Epoch [72/300], Step [60/2], Loss: 1.0899\n",
      "Epoch [72/300], Step [70/2], Loss: 0.7626\n",
      "Epoch [72/300], Step [80/2], Loss: 1.1129\n",
      "Epoch [72/300], Step [90/2], Loss: 1.0417\n",
      "Epoch [72/300], Step [100/2], Loss: 0.5910\n",
      "Epoch [72/300], Step [110/2], Loss: 0.6010\n",
      "Epoch [72/300], Step [120/2], Loss: 0.4219\n",
      "Epoch [72/300], Step [130/2], Loss: 0.8287\n",
      "Epoch [72/300], Step [140/2], Loss: 0.3056\n",
      "Epoch [72/300], Step [150/2], Loss: 0.4310\n",
      "Epoch 71; loss = 0.6015\n",
      "Epoch [73/300], Step [10/2], Loss: 0.2154\n",
      "Epoch [73/300], Step [20/2], Loss: 0.6899\n",
      "Epoch [73/300], Step [30/2], Loss: 0.9772\n",
      "Epoch [73/300], Step [40/2], Loss: 0.1385\n",
      "Epoch [73/300], Step [50/2], Loss: 0.7587\n",
      "Epoch [73/300], Step [60/2], Loss: 1.0765\n",
      "Epoch [73/300], Step [70/2], Loss: 0.7673\n",
      "Epoch [73/300], Step [80/2], Loss: 1.1129\n",
      "Epoch [73/300], Step [90/2], Loss: 1.0386\n",
      "Epoch [73/300], Step [100/2], Loss: 0.5900\n",
      "Epoch [73/300], Step [110/2], Loss: 0.6050\n",
      "Epoch [73/300], Step [120/2], Loss: 0.4249\n",
      "Epoch [73/300], Step [130/2], Loss: 0.7389\n",
      "Epoch [73/300], Step [140/2], Loss: 0.3112\n",
      "Epoch [73/300], Step [150/2], Loss: 0.4286\n",
      "Epoch 72; loss = 0.6005\n",
      "Epoch [74/300], Step [10/2], Loss: 0.2240\n",
      "Epoch [74/300], Step [20/2], Loss: 0.5794\n",
      "Epoch [74/300], Step [30/2], Loss: 0.9844\n",
      "Epoch [74/300], Step [40/2], Loss: 0.1340\n",
      "Epoch [74/300], Step [50/2], Loss: 0.7323\n",
      "Epoch [74/300], Step [60/2], Loss: 1.0878\n",
      "Epoch [74/300], Step [70/2], Loss: 0.7759\n",
      "Epoch [74/300], Step [80/2], Loss: 1.1210\n",
      "Epoch [74/300], Step [90/2], Loss: 1.0480\n",
      "Epoch [74/300], Step [100/2], Loss: 0.5845\n",
      "Epoch [74/300], Step [110/2], Loss: 0.6030\n",
      "Epoch [74/300], Step [120/2], Loss: 0.4213\n",
      "Epoch [74/300], Step [130/2], Loss: 0.7090\n",
      "Epoch [74/300], Step [140/2], Loss: 0.3124\n",
      "Epoch [74/300], Step [150/2], Loss: 0.4260\n",
      "Epoch 73; loss = 0.5982\n",
      "Epoch [75/300], Step [10/2], Loss: 0.2391\n",
      "Epoch [75/300], Step [20/2], Loss: 0.5673\n",
      "Epoch [75/300], Step [30/2], Loss: 0.9872\n",
      "Epoch [75/300], Step [40/2], Loss: 0.1367\n",
      "Epoch [75/300], Step [50/2], Loss: 0.7485\n",
      "Epoch [75/300], Step [60/2], Loss: 1.0976\n",
      "Epoch [75/300], Step [70/2], Loss: 0.7769\n",
      "Epoch [75/300], Step [80/2], Loss: 1.1252\n",
      "Epoch [75/300], Step [90/2], Loss: 1.0516\n",
      "Epoch [75/300], Step [100/2], Loss: 0.5796\n",
      "Epoch [75/300], Step [110/2], Loss: 0.6014\n",
      "Epoch [75/300], Step [120/2], Loss: 0.4190\n",
      "Epoch [75/300], Step [130/2], Loss: 0.7821\n",
      "Epoch [75/300], Step [140/2], Loss: 0.3325\n",
      "Epoch [75/300], Step [150/2], Loss: 0.4265\n",
      "Epoch 74; loss = 0.5984\n",
      "Epoch [76/300], Step [10/2], Loss: 0.3676\n",
      "Epoch [76/300], Step [20/2], Loss: 0.6360\n",
      "Epoch [76/300], Step [30/2], Loss: 1.0177\n",
      "Epoch [76/300], Step [40/2], Loss: 0.1271\n",
      "Epoch [76/300], Step [50/2], Loss: 0.5992\n",
      "Epoch [76/300], Step [60/2], Loss: 1.0937\n",
      "Epoch [76/300], Step [70/2], Loss: 0.7528\n",
      "Epoch [76/300], Step [80/2], Loss: 1.1269\n",
      "Epoch [76/300], Step [90/2], Loss: 1.0536\n",
      "Epoch [76/300], Step [100/2], Loss: 0.5583\n",
      "Epoch [76/300], Step [110/2], Loss: 0.6494\n",
      "Epoch [76/300], Step [120/2], Loss: 0.4181\n",
      "Epoch [76/300], Step [130/2], Loss: 0.8185\n",
      "Epoch [76/300], Step [140/2], Loss: 0.3022\n",
      "Epoch [76/300], Step [150/2], Loss: 0.4293\n",
      "Epoch 75; loss = 0.6032\n",
      "Epoch [77/300], Step [10/2], Loss: 0.2265\n",
      "Epoch [77/300], Step [20/2], Loss: 0.6317\n",
      "Epoch [77/300], Step [30/2], Loss: 0.9702\n",
      "Epoch [77/300], Step [40/2], Loss: 0.1335\n",
      "Epoch [77/300], Step [50/2], Loss: 0.7684\n",
      "Epoch [77/300], Step [60/2], Loss: 1.0789\n",
      "Epoch [77/300], Step [70/2], Loss: 0.7537\n",
      "Epoch [77/300], Step [80/2], Loss: 1.1327\n",
      "Epoch [77/300], Step [90/2], Loss: 1.0407\n",
      "Epoch [77/300], Step [100/2], Loss: 0.5624\n",
      "Epoch [77/300], Step [110/2], Loss: 0.5892\n",
      "Epoch [77/300], Step [120/2], Loss: 0.4234\n",
      "Epoch [77/300], Step [130/2], Loss: 0.7465\n",
      "Epoch [77/300], Step [140/2], Loss: 0.2968\n",
      "Epoch [77/300], Step [150/2], Loss: 0.4296\n",
      "Epoch 76; loss = 0.6028\n",
      "Epoch [78/300], Step [10/2], Loss: 0.2147\n",
      "Epoch [78/300], Step [20/2], Loss: 0.7596\n",
      "Epoch [78/300], Step [30/2], Loss: 0.9707\n",
      "Epoch [78/300], Step [40/2], Loss: 0.1382\n",
      "Epoch [78/300], Step [50/2], Loss: 0.7349\n",
      "Epoch [78/300], Step [60/2], Loss: 1.0789\n",
      "Epoch [78/300], Step [70/2], Loss: 0.7789\n",
      "Epoch [78/300], Step [80/2], Loss: 1.1135\n",
      "Epoch [78/300], Step [90/2], Loss: 1.0399\n",
      "Epoch [78/300], Step [100/2], Loss: 0.5865\n",
      "Epoch [78/300], Step [110/2], Loss: 0.6153\n",
      "Epoch [78/300], Step [120/2], Loss: 0.4243\n",
      "Epoch [78/300], Step [130/2], Loss: 0.7886\n",
      "Epoch [78/300], Step [140/2], Loss: 0.3340\n",
      "Epoch [78/300], Step [150/2], Loss: 0.4315\n",
      "Epoch 77; loss = 0.6080\n",
      "Epoch [79/300], Step [10/2], Loss: 0.2794\n",
      "Epoch [79/300], Step [20/2], Loss: 0.5899\n",
      "Epoch [79/300], Step [30/2], Loss: 0.9771\n",
      "Epoch [79/300], Step [40/2], Loss: 0.1543\n",
      "Epoch [79/300], Step [50/2], Loss: 0.7520\n",
      "Epoch [79/300], Step [60/2], Loss: 1.0856\n",
      "Epoch [79/300], Step [70/2], Loss: 0.7908\n",
      "Epoch [79/300], Step [80/2], Loss: 1.1213\n",
      "Epoch [79/300], Step [90/2], Loss: 1.0486\n",
      "Epoch [79/300], Step [100/2], Loss: 0.5816\n",
      "Epoch [79/300], Step [110/2], Loss: 0.6166\n",
      "Epoch [79/300], Step [120/2], Loss: 0.4188\n",
      "Epoch [79/300], Step [130/2], Loss: 0.7465\n",
      "Epoch [79/300], Step [140/2], Loss: 0.3173\n",
      "Epoch [79/300], Step [150/2], Loss: 0.4257\n",
      "Epoch 78; loss = 0.5989\n",
      "Epoch [80/300], Step [10/2], Loss: 0.2779\n",
      "Epoch [80/300], Step [20/2], Loss: 0.5606\n",
      "Epoch [80/300], Step [30/2], Loss: 0.9829\n",
      "Epoch [80/300], Step [40/2], Loss: 0.1349\n",
      "Epoch [80/300], Step [50/2], Loss: 0.6859\n",
      "Epoch [80/300], Step [60/2], Loss: 1.0934\n",
      "Epoch [80/300], Step [70/2], Loss: 0.8156\n",
      "Epoch [80/300], Step [80/2], Loss: 1.0914\n",
      "Epoch [80/300], Step [90/2], Loss: 1.0543\n",
      "Epoch [80/300], Step [100/2], Loss: 0.5805\n",
      "Epoch [80/300], Step [110/2], Loss: 0.5940\n",
      "Epoch [80/300], Step [120/2], Loss: 0.4170\n",
      "Epoch [80/300], Step [130/2], Loss: 0.6463\n",
      "Epoch [80/300], Step [140/2], Loss: 0.3067\n",
      "Epoch [80/300], Step [150/2], Loss: 0.4206\n",
      "Epoch 79; loss = 0.5964\n",
      "Epoch [81/300], Step [10/2], Loss: 0.2616\n",
      "Epoch [81/300], Step [20/2], Loss: 0.5221\n",
      "Epoch [81/300], Step [30/2], Loss: 0.9965\n",
      "Epoch [81/300], Step [40/2], Loss: 0.1530\n",
      "Epoch [81/300], Step [50/2], Loss: 0.6899\n",
      "Epoch [81/300], Step [60/2], Loss: 1.1088\n",
      "Epoch [81/300], Step [70/2], Loss: 0.7679\n",
      "Epoch [81/300], Step [80/2], Loss: 1.1364\n",
      "Epoch [81/300], Step [90/2], Loss: 1.0563\n",
      "Epoch [81/300], Step [100/2], Loss: 0.5720\n",
      "Epoch [81/300], Step [110/2], Loss: 0.6090\n",
      "Epoch [81/300], Step [120/2], Loss: 0.4168\n",
      "Epoch [81/300], Step [130/2], Loss: 0.6384\n",
      "Epoch [81/300], Step [140/2], Loss: 0.2413\n",
      "Epoch [81/300], Step [150/2], Loss: 0.4346\n",
      "Epoch 80; loss = 0.5948\n",
      "Epoch [82/300], Step [10/2], Loss: 0.2112\n",
      "Epoch [82/300], Step [20/2], Loss: 0.8839\n",
      "Epoch [82/300], Step [30/2], Loss: 0.9973\n",
      "Epoch [82/300], Step [40/2], Loss: 0.1414\n",
      "Epoch [82/300], Step [50/2], Loss: 0.7046\n",
      "Epoch [82/300], Step [60/2], Loss: 1.0915\n",
      "Epoch [82/300], Step [70/2], Loss: 0.7274\n",
      "Epoch [82/300], Step [80/2], Loss: 1.0887\n",
      "Epoch [82/300], Step [90/2], Loss: 1.0548\n",
      "Epoch [82/300], Step [100/2], Loss: 0.5995\n",
      "Epoch [82/300], Step [110/2], Loss: 0.6442\n",
      "Epoch [82/300], Step [120/2], Loss: 0.4197\n",
      "Epoch [82/300], Step [130/2], Loss: 0.6567\n",
      "Epoch [82/300], Step [140/2], Loss: 0.2675\n",
      "Epoch [82/300], Step [150/2], Loss: 0.4357\n",
      "Epoch 81; loss = 0.6115\n",
      "Epoch [83/300], Step [10/2], Loss: 0.2377\n",
      "Epoch [83/300], Step [20/2], Loss: 0.7905\n",
      "Epoch [83/300], Step [30/2], Loss: 0.9952\n",
      "Epoch [83/300], Step [40/2], Loss: 0.1743\n",
      "Epoch [83/300], Step [50/2], Loss: 0.6878\n",
      "Epoch [83/300], Step [60/2], Loss: 1.0931\n",
      "Epoch [83/300], Step [70/2], Loss: 0.7160\n",
      "Epoch [83/300], Step [80/2], Loss: 1.0956\n",
      "Epoch [83/300], Step [90/2], Loss: 1.0542\n",
      "Epoch [83/300], Step [100/2], Loss: 0.5913\n",
      "Epoch [83/300], Step [110/2], Loss: 0.6369\n",
      "Epoch [83/300], Step [120/2], Loss: 0.4182\n",
      "Epoch [83/300], Step [130/2], Loss: 0.6975\n",
      "Epoch [83/300], Step [140/2], Loss: 0.2760\n",
      "Epoch [83/300], Step [150/2], Loss: 0.4351\n",
      "Epoch 82; loss = 0.6065\n",
      "Epoch [84/300], Step [10/2], Loss: 0.2455\n",
      "Epoch [84/300], Step [20/2], Loss: 0.7178\n",
      "Epoch [84/300], Step [30/2], Loss: 0.9877\n",
      "Epoch [84/300], Step [40/2], Loss: 0.1704\n",
      "Epoch [84/300], Step [50/2], Loss: 0.6911\n",
      "Epoch [84/300], Step [60/2], Loss: 1.0861\n",
      "Epoch [84/300], Step [70/2], Loss: 0.6977\n",
      "Epoch [84/300], Step [80/2], Loss: 1.0606\n",
      "Epoch [84/300], Step [90/2], Loss: 1.0315\n",
      "Epoch [84/300], Step [100/2], Loss: 0.5892\n",
      "Epoch [84/300], Step [110/2], Loss: 0.6360\n",
      "Epoch [84/300], Step [120/2], Loss: 0.4242\n",
      "Epoch [84/300], Step [130/2], Loss: 0.7534\n",
      "Epoch [84/300], Step [140/2], Loss: 0.2873\n",
      "Epoch [84/300], Step [150/2], Loss: 0.4381\n",
      "Epoch 83; loss = 0.6059\n",
      "Epoch [85/300], Step [10/2], Loss: 0.2335\n",
      "Epoch [85/300], Step [20/2], Loss: 0.6732\n",
      "Epoch [85/300], Step [30/2], Loss: 0.9873\n",
      "Epoch [85/300], Step [40/2], Loss: 0.1530\n",
      "Epoch [85/300], Step [50/2], Loss: 0.7943\n",
      "Epoch [85/300], Step [60/2], Loss: 1.0950\n",
      "Epoch [85/300], Step [70/2], Loss: 0.7128\n",
      "Epoch [85/300], Step [80/2], Loss: 1.1066\n",
      "Epoch [85/300], Step [90/2], Loss: 1.0372\n",
      "Epoch [85/300], Step [100/2], Loss: 0.5872\n",
      "Epoch [85/300], Step [110/2], Loss: 0.6419\n",
      "Epoch [85/300], Step [120/2], Loss: 0.4225\n",
      "Epoch [85/300], Step [130/2], Loss: 0.7965\n",
      "Epoch [85/300], Step [140/2], Loss: 0.4941\n",
      "Epoch [85/300], Step [150/2], Loss: 0.4401\n",
      "Epoch 84; loss = 0.6095\n",
      "Epoch [86/300], Step [10/2], Loss: 0.4181\n",
      "Epoch [86/300], Step [20/2], Loss: 0.6835\n",
      "Epoch [86/300], Step [30/2], Loss: 1.0593\n",
      "Epoch [86/300], Step [40/2], Loss: 0.1465\n",
      "Epoch [86/300], Step [50/2], Loss: 0.6468\n",
      "Epoch [86/300], Step [60/2], Loss: 1.1054\n",
      "Epoch [86/300], Step [70/2], Loss: 0.7325\n",
      "Epoch [86/300], Step [80/2], Loss: 1.1338\n",
      "Epoch [86/300], Step [90/2], Loss: 1.0367\n",
      "Epoch [86/300], Step [100/2], Loss: 0.5546\n",
      "Epoch [86/300], Step [110/2], Loss: 0.6806\n",
      "Epoch [86/300], Step [120/2], Loss: 0.4175\n",
      "Epoch [86/300], Step [130/2], Loss: 0.7022\n",
      "Epoch [86/300], Step [140/2], Loss: 0.3624\n",
      "Epoch [86/300], Step [150/2], Loss: 0.4465\n",
      "Epoch 85; loss = 0.6136\n",
      "Epoch [87/300], Step [10/2], Loss: 0.2989\n",
      "Epoch [87/300], Step [20/2], Loss: 0.7019\n",
      "Epoch [87/300], Step [30/2], Loss: 1.0393\n",
      "Epoch [87/300], Step [40/2], Loss: 0.1391\n",
      "Epoch [87/300], Step [50/2], Loss: 0.6779\n",
      "Epoch [87/300], Step [60/2], Loss: 1.1073\n",
      "Epoch [87/300], Step [70/2], Loss: 0.7414\n",
      "Epoch [87/300], Step [80/2], Loss: 1.1328\n",
      "Epoch [87/300], Step [90/2], Loss: 1.0408\n",
      "Epoch [87/300], Step [100/2], Loss: 0.5548\n",
      "Epoch [87/300], Step [110/2], Loss: 0.6991\n",
      "Epoch [87/300], Step [120/2], Loss: 0.4155\n",
      "Epoch [87/300], Step [130/2], Loss: 0.7027\n",
      "Epoch [87/300], Step [140/2], Loss: 0.3804\n",
      "Epoch [87/300], Step [150/2], Loss: 0.4412\n",
      "Epoch 86; loss = 0.6121\n",
      "Epoch [88/300], Step [10/2], Loss: 0.3018\n",
      "Epoch [88/300], Step [20/2], Loss: 0.6285\n",
      "Epoch [88/300], Step [30/2], Loss: 1.0473\n",
      "Epoch [88/300], Step [40/2], Loss: 0.1391\n",
      "Epoch [88/300], Step [50/2], Loss: 0.6892\n",
      "Epoch [88/300], Step [60/2], Loss: 1.1103\n",
      "Epoch [88/300], Step [70/2], Loss: 0.7281\n",
      "Epoch [88/300], Step [80/2], Loss: 1.1348\n",
      "Epoch [88/300], Step [90/2], Loss: 1.0392\n",
      "Epoch [88/300], Step [100/2], Loss: 0.5483\n",
      "Epoch [88/300], Step [110/2], Loss: 0.6670\n",
      "Epoch [88/300], Step [120/2], Loss: 0.4133\n",
      "Epoch [88/300], Step [130/2], Loss: 0.6943\n",
      "Epoch [88/300], Step [140/2], Loss: 0.3194\n",
      "Epoch [88/300], Step [150/2], Loss: 0.4422\n",
      "Epoch 87; loss = 0.6058\n",
      "Epoch [89/300], Step [10/2], Loss: 0.2696\n",
      "Epoch [89/300], Step [20/2], Loss: 0.6257\n",
      "Epoch [89/300], Step [30/2], Loss: 1.0261\n",
      "Epoch [89/300], Step [40/2], Loss: 0.1279\n",
      "Epoch [89/300], Step [50/2], Loss: 0.6734\n",
      "Epoch [89/300], Step [60/2], Loss: 1.1106\n",
      "Epoch [89/300], Step [70/2], Loss: 0.7265\n",
      "Epoch [89/300], Step [80/2], Loss: 1.1301\n",
      "Epoch [89/300], Step [90/2], Loss: 1.0684\n",
      "Epoch [89/300], Step [100/2], Loss: 0.5626\n",
      "Epoch [89/300], Step [110/2], Loss: 0.7297\n",
      "Epoch [89/300], Step [120/2], Loss: 0.4151\n",
      "Epoch [89/300], Step [130/2], Loss: 0.7153\n",
      "Epoch [89/300], Step [140/2], Loss: 0.3358\n",
      "Epoch [89/300], Step [150/2], Loss: 0.4385\n",
      "Epoch 88; loss = 0.6044\n",
      "Epoch [90/300], Step [10/2], Loss: 0.2781\n",
      "Epoch [90/300], Step [20/2], Loss: 0.6160\n",
      "Epoch [90/300], Step [30/2], Loss: 1.0306\n",
      "Epoch [90/300], Step [40/2], Loss: 0.1291\n",
      "Epoch [90/300], Step [50/2], Loss: 0.6869\n",
      "Epoch [90/300], Step [60/2], Loss: 1.1025\n",
      "Epoch [90/300], Step [70/2], Loss: 0.6974\n",
      "Epoch [90/300], Step [80/2], Loss: 1.1338\n",
      "Epoch [90/300], Step [90/2], Loss: 1.0621\n",
      "Epoch [90/300], Step [100/2], Loss: 0.5742\n",
      "Epoch [90/300], Step [110/2], Loss: 0.7089\n",
      "Epoch [90/300], Step [120/2], Loss: 0.4167\n",
      "Epoch [90/300], Step [130/2], Loss: 0.7137\n",
      "Epoch [90/300], Step [140/2], Loss: 0.2895\n",
      "Epoch [90/300], Step [150/2], Loss: 0.4514\n",
      "Epoch 89; loss = 0.6014\n",
      "Epoch [91/300], Step [10/2], Loss: 0.2404\n",
      "Epoch [91/300], Step [20/2], Loss: 0.6146\n",
      "Epoch [91/300], Step [30/2], Loss: 1.0132\n",
      "Epoch [91/300], Step [40/2], Loss: 0.1054\n",
      "Epoch [91/300], Step [50/2], Loss: 0.6940\n",
      "Epoch [91/300], Step [60/2], Loss: 1.0941\n",
      "Epoch [91/300], Step [70/2], Loss: 0.7769\n",
      "Epoch [91/300], Step [80/2], Loss: 1.1283\n",
      "Epoch [91/300], Step [90/2], Loss: 1.0570\n",
      "Epoch [91/300], Step [100/2], Loss: 0.5819\n",
      "Epoch [91/300], Step [110/2], Loss: 0.7232\n",
      "Epoch [91/300], Step [120/2], Loss: 0.4197\n",
      "Epoch [91/300], Step [130/2], Loss: 0.6978\n",
      "Epoch [91/300], Step [140/2], Loss: 0.3284\n",
      "Epoch [91/300], Step [150/2], Loss: 0.4487\n",
      "Epoch 90; loss = 0.6025\n",
      "Epoch [92/300], Step [10/2], Loss: 0.2598\n",
      "Epoch [92/300], Step [20/2], Loss: 0.5961\n",
      "Epoch [92/300], Step [30/2], Loss: 1.0138\n",
      "Epoch [92/300], Step [40/2], Loss: 0.1199\n",
      "Epoch [92/300], Step [50/2], Loss: 0.7248\n",
      "Epoch [92/300], Step [60/2], Loss: 1.0924\n",
      "Epoch [92/300], Step [70/2], Loss: 0.7007\n",
      "Epoch [92/300], Step [80/2], Loss: 1.1323\n",
      "Epoch [92/300], Step [90/2], Loss: 1.0567\n",
      "Epoch [92/300], Step [100/2], Loss: 0.5681\n",
      "Epoch [92/300], Step [110/2], Loss: 0.7330\n",
      "Epoch [92/300], Step [120/2], Loss: 0.4185\n",
      "Epoch [92/300], Step [130/2], Loss: 0.6992\n",
      "Epoch [92/300], Step [140/2], Loss: 0.3138\n",
      "Epoch [92/300], Step [150/2], Loss: 0.4435\n",
      "Epoch 91; loss = 0.6020\n",
      "Epoch [93/300], Step [10/2], Loss: 0.2534\n",
      "Epoch [93/300], Step [20/2], Loss: 0.6019\n",
      "Epoch [93/300], Step [30/2], Loss: 1.0112\n",
      "Epoch [93/300], Step [40/2], Loss: 0.1098\n",
      "Epoch [93/300], Step [50/2], Loss: 0.6794\n",
      "Epoch [93/300], Step [60/2], Loss: 1.0990\n",
      "Epoch [93/300], Step [70/2], Loss: 0.7159\n",
      "Epoch [93/300], Step [80/2], Loss: 1.1346\n",
      "Epoch [93/300], Step [90/2], Loss: 1.0601\n",
      "Epoch [93/300], Step [100/2], Loss: 0.5567\n",
      "Epoch [93/300], Step [110/2], Loss: 0.7279\n",
      "Epoch [93/300], Step [120/2], Loss: 0.4174\n",
      "Epoch [93/300], Step [130/2], Loss: 0.7477\n",
      "Epoch [93/300], Step [140/2], Loss: 0.3137\n",
      "Epoch [93/300], Step [150/2], Loss: 0.4406\n",
      "Epoch 92; loss = 0.5993\n",
      "Epoch [94/300], Step [10/2], Loss: 0.2404\n",
      "Epoch [94/300], Step [20/2], Loss: 0.5867\n",
      "Epoch [94/300], Step [30/2], Loss: 1.0113\n",
      "Epoch [94/300], Step [40/2], Loss: 0.1108\n",
      "Epoch [94/300], Step [50/2], Loss: 0.6784\n",
      "Epoch [94/300], Step [60/2], Loss: 1.1009\n",
      "Epoch [94/300], Step [70/2], Loss: 0.7264\n",
      "Epoch [94/300], Step [80/2], Loss: 1.1329\n",
      "Epoch [94/300], Step [90/2], Loss: 1.0608\n",
      "Epoch [94/300], Step [100/2], Loss: 0.5577\n",
      "Epoch [94/300], Step [110/2], Loss: 0.7353\n",
      "Epoch [94/300], Step [120/2], Loss: 0.4166\n",
      "Epoch [94/300], Step [130/2], Loss: 0.7628\n",
      "Epoch [94/300], Step [140/2], Loss: 0.2957\n",
      "Epoch [94/300], Step [150/2], Loss: 0.4403\n",
      "Epoch 93; loss = 0.5964\n",
      "Epoch [95/300], Step [10/2], Loss: 0.2353\n",
      "Epoch [95/300], Step [20/2], Loss: 0.5897\n",
      "Epoch [95/300], Step [30/2], Loss: 0.9955\n",
      "Epoch [95/300], Step [40/2], Loss: 0.1001\n",
      "Epoch [95/300], Step [50/2], Loss: 0.6934\n",
      "Epoch [95/300], Step [60/2], Loss: 1.0968\n",
      "Epoch [95/300], Step [70/2], Loss: 0.7147\n",
      "Epoch [95/300], Step [80/2], Loss: 1.1301\n",
      "Epoch [95/300], Step [90/2], Loss: 1.0570\n",
      "Epoch [95/300], Step [100/2], Loss: 0.5776\n",
      "Epoch [95/300], Step [110/2], Loss: 0.7603\n",
      "Epoch [95/300], Step [120/2], Loss: 0.4207\n",
      "Epoch [95/300], Step [130/2], Loss: 0.7494\n",
      "Epoch [95/300], Step [140/2], Loss: 0.3231\n",
      "Epoch [95/300], Step [150/2], Loss: 0.4312\n",
      "Epoch 94; loss = 0.5980\n",
      "Epoch [96/300], Step [10/2], Loss: 0.2429\n",
      "Epoch [96/300], Step [20/2], Loss: 0.5958\n",
      "Epoch [96/300], Step [30/2], Loss: 0.9955\n",
      "Epoch [96/300], Step [40/2], Loss: 0.0968\n",
      "Epoch [96/300], Step [50/2], Loss: 0.6766\n",
      "Epoch [96/300], Step [60/2], Loss: 1.0974\n",
      "Epoch [96/300], Step [70/2], Loss: 0.7151\n",
      "Epoch [96/300], Step [80/2], Loss: 1.1388\n",
      "Epoch [96/300], Step [90/2], Loss: 1.0574\n",
      "Epoch [96/300], Step [100/2], Loss: 0.5708\n",
      "Epoch [96/300], Step [110/2], Loss: 0.7576\n",
      "Epoch [96/300], Step [120/2], Loss: 0.4211\n",
      "Epoch [96/300], Step [130/2], Loss: 0.7483\n",
      "Epoch [96/300], Step [140/2], Loss: 0.3336\n",
      "Epoch [96/300], Step [150/2], Loss: 0.4330\n",
      "Epoch 95; loss = 0.5963\n",
      "Epoch [97/300], Step [10/2], Loss: 0.2444\n",
      "Epoch [97/300], Step [20/2], Loss: 0.5705\n",
      "Epoch [97/300], Step [30/2], Loss: 0.9925\n",
      "Epoch [97/300], Step [40/2], Loss: 0.0974\n",
      "Epoch [97/300], Step [50/2], Loss: 0.6884\n",
      "Epoch [97/300], Step [60/2], Loss: 1.0922\n",
      "Epoch [97/300], Step [70/2], Loss: 0.7328\n",
      "Epoch [97/300], Step [80/2], Loss: 1.1356\n",
      "Epoch [97/300], Step [90/2], Loss: 1.0517\n",
      "Epoch [97/300], Step [100/2], Loss: 0.5607\n",
      "Epoch [97/300], Step [110/2], Loss: 0.7526\n",
      "Epoch [97/300], Step [120/2], Loss: 0.4249\n",
      "Epoch [97/300], Step [130/2], Loss: 0.7681\n",
      "Epoch [97/300], Step [140/2], Loss: 0.3451\n",
      "Epoch [97/300], Step [150/2], Loss: 0.4352\n",
      "Epoch 96; loss = 0.5952\n",
      "Epoch [98/300], Step [10/2], Loss: 0.2412\n",
      "Epoch [98/300], Step [20/2], Loss: 0.6168\n",
      "Epoch [98/300], Step [30/2], Loss: 0.9882\n",
      "Epoch [98/300], Step [40/2], Loss: 0.0898\n",
      "Epoch [98/300], Step [50/2], Loss: 0.6952\n",
      "Epoch [98/300], Step [60/2], Loss: 1.0924\n",
      "Epoch [98/300], Step [70/2], Loss: 0.7359\n",
      "Epoch [98/300], Step [80/2], Loss: 1.1343\n",
      "Epoch [98/300], Step [90/2], Loss: 1.0519\n",
      "Epoch [98/300], Step [100/2], Loss: 0.5635\n",
      "Epoch [98/300], Step [110/2], Loss: 0.7456\n",
      "Epoch [98/300], Step [120/2], Loss: 0.4258\n",
      "Epoch [98/300], Step [130/2], Loss: 0.7208\n",
      "Epoch [98/300], Step [140/2], Loss: 0.3154\n",
      "Epoch [98/300], Step [150/2], Loss: 0.4359\n",
      "Epoch 97; loss = 0.5919\n",
      "Epoch [99/300], Step [10/2], Loss: 0.2445\n",
      "Epoch [99/300], Step [20/2], Loss: 0.5649\n",
      "Epoch [99/300], Step [30/2], Loss: 0.9841\n",
      "Epoch [99/300], Step [40/2], Loss: 0.0864\n",
      "Epoch [99/300], Step [50/2], Loss: 0.6908\n",
      "Epoch [99/300], Step [60/2], Loss: 1.0867\n",
      "Epoch [99/300], Step [70/2], Loss: 0.7450\n",
      "Epoch [99/300], Step [80/2], Loss: 1.1311\n",
      "Epoch [99/300], Step [90/2], Loss: 1.0447\n",
      "Epoch [99/300], Step [100/2], Loss: 0.5697\n",
      "Epoch [99/300], Step [110/2], Loss: 0.7447\n",
      "Epoch [99/300], Step [120/2], Loss: 0.4271\n",
      "Epoch [99/300], Step [130/2], Loss: 0.7289\n",
      "Epoch [99/300], Step [140/2], Loss: 0.3470\n",
      "Epoch [99/300], Step [150/2], Loss: 0.4354\n",
      "Epoch 98; loss = 0.5913\n",
      "Epoch [100/300], Step [10/2], Loss: 0.2615\n",
      "Epoch [100/300], Step [20/2], Loss: 0.5504\n",
      "Epoch [100/300], Step [30/2], Loss: 0.9817\n",
      "Epoch [100/300], Step [40/2], Loss: 0.0854\n",
      "Epoch [100/300], Step [50/2], Loss: 0.6824\n",
      "Epoch [100/300], Step [60/2], Loss: 1.0853\n",
      "Epoch [100/300], Step [70/2], Loss: 0.7114\n",
      "Epoch [100/300], Step [80/2], Loss: 1.1396\n",
      "Epoch [100/300], Step [90/2], Loss: 1.0464\n",
      "Epoch [100/300], Step [100/2], Loss: 0.5661\n",
      "Epoch [100/300], Step [110/2], Loss: 0.7479\n",
      "Epoch [100/300], Step [120/2], Loss: 0.4272\n",
      "Epoch [100/300], Step [130/2], Loss: 0.7016\n",
      "Epoch [100/300], Step [140/2], Loss: 0.3215\n",
      "Epoch [100/300], Step [150/2], Loss: 0.4351\n",
      "Epoch 99; loss = 0.5893\n",
      "Epoch [101/300], Step [10/2], Loss: 0.2434\n",
      "Epoch [101/300], Step [20/2], Loss: 0.5452\n",
      "Epoch [101/300], Step [30/2], Loss: 0.9806\n",
      "Epoch [101/300], Step [40/2], Loss: 0.0883\n",
      "Epoch [101/300], Step [50/2], Loss: 0.6638\n",
      "Epoch [101/300], Step [60/2], Loss: 1.0849\n",
      "Epoch [101/300], Step [70/2], Loss: 0.6922\n",
      "Epoch [101/300], Step [80/2], Loss: 1.1456\n",
      "Epoch [101/300], Step [90/2], Loss: 1.0452\n",
      "Epoch [101/300], Step [100/2], Loss: 0.5643\n",
      "Epoch [101/300], Step [110/2], Loss: 0.7077\n",
      "Epoch [101/300], Step [120/2], Loss: 0.4277\n",
      "Epoch [101/300], Step [130/2], Loss: 0.6992\n",
      "Epoch [101/300], Step [140/2], Loss: 0.3146\n",
      "Epoch [101/300], Step [150/2], Loss: 0.4353\n",
      "Epoch 100; loss = 0.5915\n",
      "Epoch [102/300], Step [10/2], Loss: 0.2525\n",
      "Epoch [102/300], Step [20/2], Loss: 0.5292\n",
      "Epoch [102/300], Step [30/2], Loss: 0.9829\n",
      "Epoch [102/300], Step [40/2], Loss: 0.0877\n",
      "Epoch [102/300], Step [50/2], Loss: 0.6328\n",
      "Epoch [102/300], Step [60/2], Loss: 1.0877\n",
      "Epoch [102/300], Step [70/2], Loss: 0.6561\n",
      "Epoch [102/300], Step [80/2], Loss: 1.1513\n",
      "Epoch [102/300], Step [90/2], Loss: 1.0475\n",
      "Epoch [102/300], Step [100/2], Loss: 0.5796\n",
      "Epoch [102/300], Step [110/2], Loss: 0.7625\n",
      "Epoch [102/300], Step [120/2], Loss: 0.4243\n",
      "Epoch [102/300], Step [130/2], Loss: 0.7487\n",
      "Epoch [102/300], Step [140/2], Loss: 0.3122\n",
      "Epoch [102/300], Step [150/2], Loss: 0.4326\n",
      "Epoch 101; loss = 0.5890\n",
      "Epoch [103/300], Step [10/2], Loss: 0.2476\n",
      "Epoch [103/300], Step [20/2], Loss: 0.5683\n",
      "Epoch [103/300], Step [30/2], Loss: 0.9806\n",
      "Epoch [103/300], Step [40/2], Loss: 0.0938\n",
      "Epoch [103/300], Step [50/2], Loss: 0.6253\n",
      "Epoch [103/300], Step [60/2], Loss: 1.0858\n",
      "Epoch [103/300], Step [70/2], Loss: 0.7242\n",
      "Epoch [103/300], Step [80/2], Loss: 1.0376\n",
      "Epoch [103/300], Step [90/2], Loss: 1.0485\n",
      "Epoch [103/300], Step [100/2], Loss: 0.5713\n",
      "Epoch [103/300], Step [110/2], Loss: 0.7439\n",
      "Epoch [103/300], Step [120/2], Loss: 0.4252\n",
      "Epoch [103/300], Step [130/2], Loss: 0.7310\n",
      "Epoch [103/300], Step [140/2], Loss: 0.2930\n",
      "Epoch [103/300], Step [150/2], Loss: 0.4336\n",
      "Epoch 102; loss = 0.5846\n",
      "Epoch [104/300], Step [10/2], Loss: 0.2170\n",
      "Epoch [104/300], Step [20/2], Loss: 0.5492\n",
      "Epoch [104/300], Step [30/2], Loss: 0.9791\n",
      "Epoch [104/300], Step [40/2], Loss: 0.0781\n",
      "Epoch [104/300], Step [50/2], Loss: 0.7455\n",
      "Epoch [104/300], Step [60/2], Loss: 1.0834\n",
      "Epoch [104/300], Step [70/2], Loss: 0.7657\n",
      "Epoch [104/300], Step [80/2], Loss: 1.1439\n",
      "Epoch [104/300], Step [90/2], Loss: 1.0414\n",
      "Epoch [104/300], Step [100/2], Loss: 0.5845\n",
      "Epoch [104/300], Step [110/2], Loss: 0.6640\n",
      "Epoch [104/300], Step [120/2], Loss: 0.4274\n",
      "Epoch [104/300], Step [130/2], Loss: 0.7208\n",
      "Epoch [104/300], Step [140/2], Loss: 0.2725\n",
      "Epoch [104/300], Step [150/2], Loss: 0.4345\n",
      "Epoch 103; loss = 0.5903\n",
      "Epoch [105/300], Step [10/2], Loss: 0.2316\n",
      "Epoch [105/300], Step [20/2], Loss: 0.5112\n",
      "Epoch [105/300], Step [30/2], Loss: 0.9767\n",
      "Epoch [105/300], Step [40/2], Loss: 0.1112\n",
      "Epoch [105/300], Step [50/2], Loss: 0.6934\n",
      "Epoch [105/300], Step [60/2], Loss: 1.0812\n",
      "Epoch [105/300], Step [70/2], Loss: 0.7402\n",
      "Epoch [105/300], Step [80/2], Loss: 1.1428\n",
      "Epoch [105/300], Step [90/2], Loss: 1.0438\n",
      "Epoch [105/300], Step [100/2], Loss: 0.5862\n",
      "Epoch [105/300], Step [110/2], Loss: 0.6758\n",
      "Epoch [105/300], Step [120/2], Loss: 0.4239\n",
      "Epoch [105/300], Step [130/2], Loss: 0.6865\n",
      "Epoch [105/300], Step [140/2], Loss: 0.2758\n",
      "Epoch [105/300], Step [150/2], Loss: 0.4337\n",
      "Epoch 104; loss = 0.5865\n",
      "Epoch [106/300], Step [10/2], Loss: 0.2433\n",
      "Epoch [106/300], Step [20/2], Loss: 0.5289\n",
      "Epoch [106/300], Step [30/2], Loss: 0.9808\n",
      "Epoch [106/300], Step [40/2], Loss: 0.0820\n",
      "Epoch [106/300], Step [50/2], Loss: 0.6738\n",
      "Epoch [106/300], Step [60/2], Loss: 1.0887\n",
      "Epoch [106/300], Step [70/2], Loss: 0.7828\n",
      "Epoch [106/300], Step [80/2], Loss: 1.0892\n",
      "Epoch [106/300], Step [90/2], Loss: 1.0497\n",
      "Epoch [106/300], Step [100/2], Loss: 0.5672\n",
      "Epoch [106/300], Step [110/2], Loss: 0.6445\n",
      "Epoch [106/300], Step [120/2], Loss: 0.4212\n",
      "Epoch [106/300], Step [130/2], Loss: 0.7656\n",
      "Epoch [106/300], Step [140/2], Loss: 0.2797\n",
      "Epoch [106/300], Step [150/2], Loss: 0.4315\n",
      "Epoch 105; loss = 0.5837\n",
      "Epoch [107/300], Step [10/2], Loss: 0.2434\n",
      "Epoch [107/300], Step [20/2], Loss: 0.5236\n",
      "Epoch [107/300], Step [30/2], Loss: 0.9809\n",
      "Epoch [107/300], Step [40/2], Loss: 0.0780\n",
      "Epoch [107/300], Step [50/2], Loss: 0.8463\n",
      "Epoch [107/300], Step [60/2], Loss: 1.0898\n",
      "Epoch [107/300], Step [70/2], Loss: 0.7594\n",
      "Epoch [107/300], Step [80/2], Loss: 1.0398\n",
      "Epoch [107/300], Step [90/2], Loss: 1.0495\n",
      "Epoch [107/300], Step [100/2], Loss: 0.5607\n",
      "Epoch [107/300], Step [110/2], Loss: 0.6455\n",
      "Epoch [107/300], Step [120/2], Loss: 0.4222\n",
      "Epoch [107/300], Step [130/2], Loss: 0.7180\n",
      "Epoch [107/300], Step [140/2], Loss: 0.2841\n",
      "Epoch [107/300], Step [150/2], Loss: 0.4304\n",
      "Epoch 106; loss = 0.5826\n",
      "Epoch [108/300], Step [10/2], Loss: 0.2433\n",
      "Epoch [108/300], Step [20/2], Loss: 0.5237\n",
      "Epoch [108/300], Step [30/2], Loss: 0.9852\n",
      "Epoch [108/300], Step [40/2], Loss: 0.0930\n",
      "Epoch [108/300], Step [50/2], Loss: 0.8251\n",
      "Epoch [108/300], Step [60/2], Loss: 1.0931\n",
      "Epoch [108/300], Step [70/2], Loss: 0.7815\n",
      "Epoch [108/300], Step [80/2], Loss: 1.1473\n",
      "Epoch [108/300], Step [90/2], Loss: 1.0483\n",
      "Epoch [108/300], Step [100/2], Loss: 0.5745\n",
      "Epoch [108/300], Step [110/2], Loss: 0.5920\n",
      "Epoch [108/300], Step [120/2], Loss: 0.4243\n",
      "Epoch [108/300], Step [130/2], Loss: 0.6282\n",
      "Epoch [108/300], Step [140/2], Loss: 0.3086\n",
      "Epoch [108/300], Step [150/2], Loss: 0.4289\n",
      "Epoch 107; loss = 0.5909\n",
      "Epoch [109/300], Step [10/2], Loss: 0.2663\n",
      "Epoch [109/300], Step [20/2], Loss: 0.4613\n",
      "Epoch [109/300], Step [30/2], Loss: 0.9801\n",
      "Epoch [109/300], Step [40/2], Loss: 0.1255\n",
      "Epoch [109/300], Step [50/2], Loss: 0.7483\n",
      "Epoch [109/300], Step [60/2], Loss: 1.0779\n",
      "Epoch [109/300], Step [70/2], Loss: 0.7343\n",
      "Epoch [109/300], Step [80/2], Loss: 1.1446\n",
      "Epoch [109/300], Step [90/2], Loss: 1.0389\n",
      "Epoch [109/300], Step [100/2], Loss: 0.5880\n",
      "Epoch [109/300], Step [110/2], Loss: 0.6948\n",
      "Epoch [109/300], Step [120/2], Loss: 0.4246\n",
      "Epoch [109/300], Step [130/2], Loss: 0.7234\n",
      "Epoch [109/300], Step [140/2], Loss: 0.3209\n",
      "Epoch [109/300], Step [150/2], Loss: 0.4308\n",
      "Epoch 108; loss = 0.5912\n",
      "Epoch [110/300], Step [10/2], Loss: 0.2753\n",
      "Epoch [110/300], Step [20/2], Loss: 0.5448\n",
      "Epoch [110/300], Step [30/2], Loss: 0.9800\n",
      "Epoch [110/300], Step [40/2], Loss: 0.1207\n",
      "Epoch [110/300], Step [50/2], Loss: 0.6543\n",
      "Epoch [110/300], Step [60/2], Loss: 1.0777\n",
      "Epoch [110/300], Step [70/2], Loss: 0.7983\n",
      "Epoch [110/300], Step [80/2], Loss: 1.1341\n",
      "Epoch [110/300], Step [90/2], Loss: 1.0414\n",
      "Epoch [110/300], Step [100/2], Loss: 0.5777\n",
      "Epoch [110/300], Step [110/2], Loss: 0.6669\n",
      "Epoch [110/300], Step [120/2], Loss: 0.4241\n",
      "Epoch [110/300], Step [130/2], Loss: 0.6761\n",
      "Epoch [110/300], Step [140/2], Loss: 0.3197\n",
      "Epoch [110/300], Step [150/2], Loss: 0.4285\n",
      "Epoch 109; loss = 0.5897\n",
      "Epoch [111/300], Step [10/2], Loss: 0.3038\n",
      "Epoch [111/300], Step [20/2], Loss: 0.5313\n",
      "Epoch [111/300], Step [30/2], Loss: 0.9888\n",
      "Epoch [111/300], Step [40/2], Loss: 0.1373\n",
      "Epoch [111/300], Step [50/2], Loss: 0.6314\n",
      "Epoch [111/300], Step [60/2], Loss: 1.0885\n",
      "Epoch [111/300], Step [70/2], Loss: 0.7671\n",
      "Epoch [111/300], Step [80/2], Loss: 1.1322\n",
      "Epoch [111/300], Step [90/2], Loss: 1.0476\n",
      "Epoch [111/300], Step [100/2], Loss: 0.5684\n",
      "Epoch [111/300], Step [110/2], Loss: 0.6491\n",
      "Epoch [111/300], Step [120/2], Loss: 0.4217\n",
      "Epoch [111/300], Step [130/2], Loss: 0.6361\n",
      "Epoch [111/300], Step [140/2], Loss: 0.3186\n",
      "Epoch [111/300], Step [150/2], Loss: 0.4227\n",
      "Epoch 110; loss = 0.5879\n",
      "Epoch [112/300], Step [10/2], Loss: 0.2789\n",
      "Epoch [112/300], Step [20/2], Loss: 0.4783\n",
      "Epoch [112/300], Step [30/2], Loss: 1.0055\n",
      "Epoch [112/300], Step [40/2], Loss: 0.1495\n"
     ]
    }
   ],
   "source": [
    "epochs = []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "    iteration_count = 0.\n",
    "    for i, sample in enumerate(eye_train):\n",
    "        iteration_count += 1.\n",
    "        labels, marks = sample['label'], sample['marks'] \n",
    "        marks = Variable(marks.view(-1, sequence_length, input_size))\n",
    "        labels = Variable(labels)\n",
    "        if cuda_enabled:\n",
    "            marks = marks.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = eye_model(marks)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(eye_train) // batch_size, loss.item()))\n",
    "    current_epoch_loss = loss_total / iteration_count\n",
    "    print('Epoch %d; loss = %0.4f' % (epoch, current_epoch_loss))\n",
    "    epochs.append(epoch)\n",
    "    losses.append(current_epoch_loss)\n",
    "    epoch_loss = current_epoch_loss\n",
    "timing['training'] = datetime.datetime.now() - timing['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca92e1-c077-4226-8dea-5435febca8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b008aaa-97fc-4520-9e98-945baf068f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(eye_model.state_dict(), 'ShouldIDrive_eye_tracking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c143963-f6fa-4389-a5c7-2e7a593307a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe25a2-e9f1-4efd-8081-ad8c89297c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_model.is_training = False\n",
    "timing['testing'] = datetime.datetime.now()\n",
    "print('Testing -----------------------------------------------')\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "eye_predicted_list = []\n",
    "eye_label_list = []\n",
    "for i, sample in enumerate(eye_test_loader):#test_loader\n",
    "    labels, marks = sample['label'], sample['marks'] \n",
    "    marks = Variable(marks.view(-1, sequence_length, input_size))\n",
    "    if cuda_enabled:\n",
    "        marks = marks.cuda()\n",
    "\n",
    "    outputs = eye_model(marks)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    # print(total)\n",
    "    for p, l in zip(_, labels):\n",
    "        eye_p = int(np.round(p).item())\n",
    "        eye_l = l[0].item()\n",
    "        eye_predicted_list.append(eye_p)\n",
    "        eye_label_list.append(eye_l)\n",
    "        if(eye_p == eye_l):\n",
    "            correct += 1.0\n",
    "    print(correct)\n",
    "print('Test Accuracy of the model on the {} test images: {} %'.format(total, 100 * correct / total)) \n",
    "\n",
    "timing['testing'] = datetime.datetime.now() - timing['testing']\n",
    "print(timing['testing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e5265-3d0d-4b5d-bfda-839660e6c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print('================')\n",
    "print(confusion_matrix(eye_label_list, eye_predicted_list))\n",
    "print('=============================================')\n",
    "print('Accuracy = %0.4f' % (accuracy_score(eye_label_list, eye_predicted_list)))\n",
    "print('=============================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ce5d3d-606a-48de-af9d-59088562877c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7fdd20d71be0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "cuda_enabled = torch.cuda.is_available()\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#cuda_enabled = False\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "cudnn.benchmark = True\n",
    "\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from __future__ import print_function, division\n",
    "import datetime\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c332384-b38b-4c5f-b33d-20b1339ea1c1",
   "metadata": {},
   "source": [
    "\n",
    "#### Bidirectional Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9f94e425-3dd0-47df-af7f-4cd3311d628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "# class BiRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(BiRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Set initial states\n",
    "#         h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "#         c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03db30d-3bf9-4d6a-860d-29802567c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.is_training = False\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        # (-1, sequence_length * input_size)\n",
    "        # self.linear = nn.Linear(3*244*244, num_classes)\n",
    "        # self.linear = nn.Linear(144, 256)\n",
    "        self.linear = nn.Linear(self.hidden_size*2, self.num_classes)\n",
    "        # self.linear = nn.Linear(hidden_size*2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        # print(x)\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        # print(h0)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        # print(c0)\n",
    "        # if cuda_enabled:\n",
    "        #     h0 = h0.cuda()  # 2 for bidirection\n",
    "        #     c0 = c0.cuda()\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        if self.is_training:\n",
    "            out = self.fc(out[:, -1, :])\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        # out = F.log_softmax(self.linear(out), dim=1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242dc378-25f2-4936-a0fc-2f540dab5295",
   "metadata": {},
   "source": [
    "#### Body and Eye Tracking Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50dce79-b872-4d4e-a08b-b92d47cbb858",
   "metadata": {},
   "source": [
    "Dataset class\n",
    "-------------\n",
    "\n",
    "``torch.utils.data.Dataset`` is an abstract class representing a\n",
    "dataset.\n",
    "Your custom dataset should inherit ``Dataset`` and override the following\n",
    "methods:\n",
    "\n",
    "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
    "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
    "   be used to get $i$\\ th sample.\n",
    "\n",
    "Let's create a dataset class for our face landmarks dataset. We will\n",
    "read the csv in ``__init__`` but leave the reading of images to\n",
    "``__getitem__``. This is memory efficient because all the images are not\n",
    "stored in the memory at once but read as required.\n",
    "\n",
    "Sample of our dataset will be a dict\n",
    "``{'image': image, 'landmarks': landmarks}``. Our dataset will take an\n",
    "optional argument ``transform`` so that any required processing can be\n",
    "applied on the sample. We will see the usefulness of ``transform`` in the\n",
    "next section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686bdda5-302e-4028-ad3a-bd522756a123",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class EyeLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):#root_dir = img_dir\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)#same as image label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
    "        name = self.landmarks_frame.iloc[idx, 0]\n",
    "        labels = np.zeros(4)\n",
    "        for i in range(4):\n",
    "            labels[i]=name\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        marks = []\n",
    "        for mark in range(len(landmarks)):\n",
    "            arr = literal_eval(landmarks[mark])[0]\n",
    "            arr1 = []\n",
    "            for i in range(len(arr)):\n",
    "                for j in range(len(arr[i])):\n",
    "                    # print(arr[i][j])\n",
    "                    arr1.append(arr[i][j])\n",
    "            marks.append(np.array(arr1))  \n",
    "            # return\n",
    "                          \n",
    "        marks = torch.Tensor(marks)\n",
    "        \n",
    "        sample = {'label': labels, 'marks': marks}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8315bfb-9621-4e91-b56e-b7374c23468e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BodyLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):#root_dir = img_dir\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)#same as image label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
    "        name = self.landmarks_frame.iloc[idx, 0]\n",
    "        labels = np.zeros(18)\n",
    "        for i in range(4):\n",
    "            labels[i]=name\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        marks = []\n",
    "        for mark in range(len(landmarks)):\n",
    "            arr=literal_eval(landmarks[mark])\n",
    "            # print(arr)\n",
    "            # arr = literal_eval(landmarks[mark])[0]\n",
    "            # print(arr)\n",
    "            arr = np.array(arr)\n",
    "            # print(arr)\n",
    "            marks.append(arr)\n",
    "            # return\n",
    "            \n",
    "                          \n",
    "        marks = torch.Tensor(marks)\n",
    "        \n",
    "        sample = {'label': labels, 'marks': marks}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5fa1a8-4f8d-4408-90f6-ea3f67ea78ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, torch.Size([4]), torch.Size([125, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyedataset = EyeLandmarksDataset(csv_file='CSVs/EyeTracking.csv')\n",
    "eyedataset.__len__(), eyedataset.__getitem__(0)['label'].shape, eyedataset.__getitem__(0)['marks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1f57251-e0d8-4b4f-8c67-3238117e2852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, torch.Size([18]), torch.Size([181, 36]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bodydataset = BodyLandmarksDataset(csv_file='CSVs/BodyTracking.csv')\n",
    "bodydataset.__len__(), bodydataset.__getitem__(0)['label'].shape, bodydataset.__getitem__(0)['marks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5314b415-739b-46b9-934e-e1d3817ef059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16., 24., 26., 26., 55., 28., 23., 23.],\n",
       "        [12., 22., 25., 25., 52., 25., 24., 24.],\n",
       "        [14., 24., 25., 25., 54., 28., 24., 24.],\n",
       "        [15., 24., 25., 25., 54., 28., 24., 24.],\n",
       "        [13., 23., 25., 25., 53., 28., 23., 23.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyedataset.__getitem__(5)['marks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "241275d1-33dd-4170-b7e3-5dac623ac516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.,  -1., 306., 360., 222., 360.,  -1.,  -1.,  -1.,  -1., 417., 360.,\n",
       "         417., 532., 389., 594.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1., 306., 360., 222., 360.,  -1.,  -1.,  -1.,  -1., 417., 360.,\n",
       "         417., 532., 389., 579.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1., 361., 266.],\n",
       "        [ -1.,  -1., 306., 360., 222., 360.,  -1.,  -1.,  -1.,  -1., 417., 360.,\n",
       "         417., 532., 389., 579.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1., 306., 360., 222., 360.,  -1.,  -1.,  -1.,  -1., 417., 360.,\n",
       "         417., 532., 389., 594.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
       "        [306., 219., 306., 360., 222., 360.,  -1.,  -1.,  -1.,  -1., 417., 360.,\n",
       "         417., 532., 389., 594.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "          -1.,  -1.,  -1.,  -1., 306., 203., 333., 203.,  -1.,  -1., 361., 219.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bodydataset.__getitem__(5)['marks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b144f026-f72d-4312-a6f2-b79edd39cfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Eye NN dataset -------------------------------------\n",
      "224 <__main__.EyeLandmarksDataset object at 0x7fdd2780fe20>\n",
      "156 <torch.utils.data.dataset.Subset object at 0x7fdd27f39e20>\n",
      "34 <torch.utils.data.dataset.Subset object at 0x7fdd27809640>\n",
      "34 <torch.utils.data.dataset.Subset object at 0x7fdd278097c0>\n",
      "Starting Eye NN loader -------------------------------------\n",
      "20 <torch.utils.data.dataloader.DataLoader object at 0x7fdd27809e20>\n",
      "5 <torch.utils.data.dataloader.DataLoader object at 0x7fdd29473f40>\n",
      "5 <torch.utils.data.dataloader.DataLoader object at 0x7fdd20f6a820>\n",
      "Finish  -------------------------------------\n",
      "Starting Body NN dataset -------------------------------------\n",
      "224 <__main__.BodyLandmarksDataset object at 0x7fdd27f36be0>\n",
      "156 <torch.utils.data.dataset.Subset object at 0x7fdd27f36e80>\n",
      "34 <torch.utils.data.dataset.Subset object at 0x7fdd2780fc40>\n",
      "34 <torch.utils.data.dataset.Subset object at 0x7fdd2780f910>\n",
      "Starting Body NN loader -------------------------------------\n",
      "20 <torch.utils.data.dataloader.DataLoader object at 0x7fdd2a3bd970>\n",
      "5 <torch.utils.data.dataloader.DataLoader object at 0x7fdd20f6a100>\n",
      "5 <torch.utils.data.dataloader.DataLoader object at 0x7fdd27f39f70>\n",
      "Finish  -------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#NN Data\n",
    "\n",
    "def train_total_dataset(dataset, total_split=0.30): #Training set be 70% of the dataset\n",
    "    train_idx, total_idx = train_test_split(list(range(len(dataset))), test_size=total_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['total_split'] = Subset(dataset, total_idx)\n",
    "    return datasets\n",
    "def test_val_dataset(dataset, total_split=0.5): #Testing and Validation set be 15% (50% of the remaining 30% of the dataset) \n",
    "    test_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=total_split)\n",
    "    datasets = {}\n",
    "    datasets['test'] = Subset(dataset, test_idx)\n",
    "    datasets['validation'] = Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "# hyperparams for the network\n",
    "batch_size = 8\n",
    "\n",
    "print('Starting Eye NN dataset -------------------------------------')\n",
    "eye_datasets = train_total_dataset(eyedataset)\n",
    "eye_train = eye_datasets['train']\n",
    "eye_datasets_split = test_val_dataset(eye_datasets['total_split'])\n",
    "eye_test = eye_datasets_split['test']\n",
    "eye_validation = eye_datasets_split['validation']\n",
    "# The original dataset is available in the Subset class\n",
    "print(len(eye_datasets['train'].dataset), eye_datasets['train'].dataset)\n",
    "print(len(eye_train), eye_train)\n",
    "print(len(eye_test), eye_test)\n",
    "print(len(eye_validation), eye_validation)\n",
    "\n",
    "print('Starting Eye NN loader -------------------------------------')\n",
    "eye_train_loader = DataLoader(dataset=eye_train, batch_size=batch_size, shuffle=True)\n",
    "eye_test_loader = DataLoader(dataset=eye_test, batch_size=batch_size, shuffle=False)\n",
    "eye_validation_loader = DataLoader(dataset=eye_validation, batch_size=batch_size, shuffle=False)\n",
    "print(len(eye_train_loader), eye_train_loader)\n",
    "print(len(eye_test_loader), eye_test_loader)\n",
    "print(len(eye_validation_loader), eye_validation_loader)\n",
    "print('Finish  -------------------------------------')\n",
    "\n",
    "print('Starting Body NN dataset -------------------------------------')\n",
    "body_datasets = train_total_dataset(bodydataset)\n",
    "body_train = body_datasets['train']\n",
    "body_datasets_split = test_val_dataset(body_datasets['total_split'])\n",
    "body_test = body_datasets_split['test']\n",
    "body_validation = body_datasets_split['validation']\n",
    "# The original dataset is available in the Subset class\n",
    "print(len(body_datasets['train'].dataset), body_datasets['train'].dataset)\n",
    "print(len(body_train), body_train)\n",
    "print(len(body_test), body_test)\n",
    "print(len(body_validation), body_validation)\n",
    "\n",
    "print('Starting Body NN loader -------------------------------------')\n",
    "body_train_loader = DataLoader(dataset=body_train, batch_size=batch_size, shuffle=True)\n",
    "body_test_loader = DataLoader(dataset=body_test, batch_size=batch_size, shuffle=False)\n",
    "body_validation_loader = DataLoader(dataset=body_validation, batch_size=batch_size, shuffle=False)\n",
    "print(len(body_train_loader), body_train_loader)\n",
    "print(len(body_test_loader), body_test_loader)\n",
    "print(len(body_validation_loader), body_validation_loader)\n",
    "print('Finish  -------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0f43eeec-8d7e-48c5-a559-0c8376a16bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7fb26bb37250>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbdd89-a0c9-4299-930a-10827ddf82c7",
   "metadata": {},
   "source": [
    "#### Body Tracking LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3341c464-4450-403b-b385-51601065aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init NN\n",
    "input_size = 2\n",
    "sequence_length = 181\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_classes = 2  # TODO: Determine this from the data\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "# The network\n",
    "body_model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "body_model.is_training = True\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(body_model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_loss = 5000000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b301f4ac-7bb1-4b23-ba0d-b8d5fd0aea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[ -1.,  -1.,  -1.,  -1., 695., 375.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
      "        [ -1.,  -1.,  -1.,  -1., 695., 375.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
      "        [ -1.,  -1.,  -1.,  -1., 695., 375., 166., 500., 166., 547.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
      "        [ -1.,  -1.,  -1.,  -1.,  -1.,  -1., 166., 500., 139., 547.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
      "        [ -1.,  -1.,  -1.,  -1.,  -1.,  -1., 166., 500., 139., 547.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]])\n"
     ]
    }
   ],
   "source": [
    "for i, (sample) in enumerate(body_train_loader):\n",
    "    labels, marks = sample['label'], sample['marks']\n",
    "    # marks = marks.flatten()\n",
    "    print(labels[0])\n",
    "    print(marks[0][:5])\n",
    "    # print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0553908-7a75-43b4-95ec-95baefff9602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/19], Loss: 0.7300\n",
      "Epoch [1/100], Step [20/19], Loss: 0.6532\n",
      "Epoch [1/100], Step [30/19], Loss: 0.6760\n",
      "Epoch [1/100], Step [40/19], Loss: 0.6579\n",
      "Epoch [1/100], Step [50/19], Loss: 0.6541\n",
      "Epoch [1/100], Step [60/19], Loss: 0.6422\n",
      "Epoch [1/100], Step [70/19], Loss: 0.6240\n",
      "Epoch [1/100], Step [80/19], Loss: 0.6218\n",
      "Epoch [1/100], Step [90/19], Loss: 0.5794\n",
      "Epoch [1/100], Step [100/19], Loss: 0.6155\n",
      "Epoch [1/100], Step [110/19], Loss: 0.6170\n",
      "Epoch [1/100], Step [120/19], Loss: 0.5942\n",
      "Epoch [1/100], Step [130/19], Loss: 0.5059\n",
      "Epoch [1/100], Step [140/19], Loss: 0.5816\n",
      "Epoch [1/100], Step [150/19], Loss: 0.5593\n",
      "Epoch 0; loss = 0.6172\n",
      "Epoch [2/100], Step [10/19], Loss: 0.4129\n",
      "Epoch [2/100], Step [20/19], Loss: 0.4026\n",
      "Epoch [2/100], Step [30/19], Loss: 0.5682\n",
      "Epoch [2/100], Step [40/19], Loss: 0.3081\n",
      "Epoch [2/100], Step [50/19], Loss: 0.5403\n",
      "Epoch [2/100], Step [60/19], Loss: 0.2538\n",
      "Epoch [2/100], Step [70/19], Loss: 0.4341\n",
      "Epoch [2/100], Step [80/19], Loss: 0.1954\n",
      "Epoch [2/100], Step [90/19], Loss: 0.1145\n",
      "Epoch [2/100], Step [100/19], Loss: 0.4739\n",
      "Epoch [2/100], Step [110/19], Loss: 0.5624\n",
      "Epoch [2/100], Step [120/19], Loss: 0.4722\n",
      "Epoch [2/100], Step [130/19], Loss: 0.1886\n",
      "Epoch [2/100], Step [140/19], Loss: 0.4416\n",
      "Epoch [2/100], Step [150/19], Loss: 0.4978\n",
      "Epoch 1; loss = 0.3623\n",
      "Epoch [3/100], Step [10/19], Loss: 0.1326\n",
      "Epoch [3/100], Step [20/19], Loss: 0.0512\n",
      "Epoch [3/100], Step [30/19], Loss: 0.7472\n",
      "Epoch [3/100], Step [40/19], Loss: 0.1257\n",
      "Epoch [3/100], Step [50/19], Loss: 0.7208\n",
      "Epoch [3/100], Step [60/19], Loss: 0.1341\n",
      "Epoch [3/100], Step [70/19], Loss: 0.4579\n",
      "Epoch [3/100], Step [80/19], Loss: 0.1382\n",
      "Epoch [3/100], Step [90/19], Loss: 0.0862\n",
      "Epoch [3/100], Step [100/19], Loss: 0.4665\n",
      "Epoch [3/100], Step [110/19], Loss: 0.5798\n",
      "Epoch [3/100], Step [120/19], Loss: 0.4778\n",
      "Epoch [3/100], Step [130/19], Loss: 0.1608\n",
      "Epoch [3/100], Step [140/19], Loss: 0.4492\n",
      "Epoch [3/100], Step [150/19], Loss: 0.4955\n",
      "Epoch 2; loss = 0.3141\n",
      "Epoch [4/100], Step [10/19], Loss: 0.1228\n",
      "Epoch [4/100], Step [20/19], Loss: 0.0493\n",
      "Epoch [4/100], Step [30/19], Loss: 0.7391\n",
      "Epoch [4/100], Step [40/19], Loss: 0.1145\n",
      "Epoch [4/100], Step [50/19], Loss: 0.7166\n",
      "Epoch [4/100], Step [60/19], Loss: 0.1190\n",
      "Epoch [4/100], Step [70/19], Loss: 0.4622\n",
      "Epoch [4/100], Step [80/19], Loss: 0.1338\n",
      "Epoch [4/100], Step [90/19], Loss: 0.0845\n",
      "Epoch [4/100], Step [100/19], Loss: 0.4671\n",
      "Epoch [4/100], Step [110/19], Loss: 0.5809\n",
      "Epoch [4/100], Step [120/19], Loss: 0.4766\n",
      "Epoch [4/100], Step [130/19], Loss: 0.1479\n",
      "Epoch [4/100], Step [140/19], Loss: 0.4495\n",
      "Epoch [4/100], Step [150/19], Loss: 0.4942\n",
      "Epoch 3; loss = 0.3091\n",
      "Epoch [5/100], Step [10/19], Loss: 0.1200\n",
      "Epoch [5/100], Step [20/19], Loss: 0.0515\n",
      "Epoch [5/100], Step [30/19], Loss: 0.7252\n",
      "Epoch [5/100], Step [40/19], Loss: 0.1097\n",
      "Epoch [5/100], Step [50/19], Loss: 0.7048\n",
      "Epoch [5/100], Step [60/19], Loss: 0.1084\n",
      "Epoch [5/100], Step [70/19], Loss: 0.4588\n",
      "Epoch [5/100], Step [80/19], Loss: 0.1381\n",
      "Epoch [5/100], Step [90/19], Loss: 0.0847\n",
      "Epoch [5/100], Step [100/19], Loss: 0.4674\n",
      "Epoch [5/100], Step [110/19], Loss: 0.5796\n",
      "Epoch [5/100], Step [120/19], Loss: 0.4773\n",
      "Epoch [5/100], Step [130/19], Loss: 0.1375\n",
      "Epoch [5/100], Step [140/19], Loss: 0.4390\n",
      "Epoch [5/100], Step [150/19], Loss: 0.4894\n",
      "Epoch 4; loss = 0.3053\n",
      "Epoch [6/100], Step [10/19], Loss: 0.1180\n",
      "Epoch [6/100], Step [20/19], Loss: 0.0549\n",
      "Epoch [6/100], Step [30/19], Loss: 0.7109\n",
      "Epoch [6/100], Step [40/19], Loss: 0.1051\n",
      "Epoch [6/100], Step [50/19], Loss: 0.6907\n",
      "Epoch [6/100], Step [60/19], Loss: 0.1018\n",
      "Epoch [6/100], Step [70/19], Loss: 0.4543\n",
      "Epoch [6/100], Step [80/19], Loss: 0.1393\n",
      "Epoch [6/100], Step [90/19], Loss: 0.0865\n",
      "Epoch [6/100], Step [100/19], Loss: 0.4709\n",
      "Epoch [6/100], Step [110/19], Loss: 0.5770\n",
      "Epoch [6/100], Step [120/19], Loss: 0.4778\n",
      "Epoch [6/100], Step [130/19], Loss: 0.1283\n",
      "Epoch [6/100], Step [140/19], Loss: 0.4347\n",
      "Epoch [6/100], Step [150/19], Loss: 0.4841\n",
      "Epoch 5; loss = 0.3017\n",
      "Epoch [7/100], Step [10/19], Loss: 0.1155\n",
      "Epoch [7/100], Step [20/19], Loss: 0.0592\n",
      "Epoch [7/100], Step [30/19], Loss: 0.6952\n",
      "Epoch [7/100], Step [40/19], Loss: 0.1021\n",
      "Epoch [7/100], Step [50/19], Loss: 0.6744\n",
      "Epoch [7/100], Step [60/19], Loss: 0.0965\n",
      "Epoch [7/100], Step [70/19], Loss: 0.4483\n",
      "Epoch [7/100], Step [80/19], Loss: 0.1396\n",
      "Epoch [7/100], Step [90/19], Loss: 0.0899\n",
      "Epoch [7/100], Step [100/19], Loss: 0.4709\n",
      "Epoch [7/100], Step [110/19], Loss: 0.5719\n",
      "Epoch [7/100], Step [120/19], Loss: 0.4770\n",
      "Epoch [7/100], Step [130/19], Loss: 0.1214\n",
      "Epoch [7/100], Step [140/19], Loss: 0.4284\n",
      "Epoch [7/100], Step [150/19], Loss: 0.4823\n",
      "Epoch 6; loss = 0.2984\n",
      "Epoch [8/100], Step [10/19], Loss: 0.1146\n",
      "Epoch [8/100], Step [20/19], Loss: 0.0646\n",
      "Epoch [8/100], Step [30/19], Loss: 0.6783\n",
      "Epoch [8/100], Step [40/19], Loss: 0.1001\n",
      "Epoch [8/100], Step [50/19], Loss: 0.6565\n",
      "Epoch [8/100], Step [60/19], Loss: 0.0915\n",
      "Epoch [8/100], Step [70/19], Loss: 0.4397\n",
      "Epoch [8/100], Step [80/19], Loss: 0.1396\n",
      "Epoch [8/100], Step [90/19], Loss: 0.0936\n",
      "Epoch [8/100], Step [100/19], Loss: 0.4702\n",
      "Epoch [8/100], Step [110/19], Loss: 0.5679\n",
      "Epoch [8/100], Step [120/19], Loss: 0.4753\n",
      "Epoch [8/100], Step [130/19], Loss: 0.1148\n",
      "Epoch [8/100], Step [140/19], Loss: 0.4229\n",
      "Epoch [8/100], Step [150/19], Loss: 0.4783\n",
      "Epoch 7; loss = 0.2951\n",
      "Epoch [9/100], Step [10/19], Loss: 0.1126\n",
      "Epoch [9/100], Step [20/19], Loss: 0.0712\n",
      "Epoch [9/100], Step [30/19], Loss: 0.6610\n",
      "Epoch [9/100], Step [40/19], Loss: 0.0979\n",
      "Epoch [9/100], Step [50/19], Loss: 0.6386\n",
      "Epoch [9/100], Step [60/19], Loss: 0.0852\n",
      "Epoch [9/100], Step [70/19], Loss: 0.4318\n",
      "Epoch [9/100], Step [80/19], Loss: 0.1387\n",
      "Epoch [9/100], Step [90/19], Loss: 0.0967\n",
      "Epoch [9/100], Step [100/19], Loss: 0.4687\n",
      "Epoch [9/100], Step [110/19], Loss: 0.5637\n",
      "Epoch [9/100], Step [120/19], Loss: 0.4737\n",
      "Epoch [9/100], Step [130/19], Loss: 0.1082\n",
      "Epoch [9/100], Step [140/19], Loss: 0.4160\n",
      "Epoch [9/100], Step [150/19], Loss: 0.4743\n",
      "Epoch 8; loss = 0.2919\n",
      "Epoch [10/100], Step [10/19], Loss: 0.1105\n",
      "Epoch [10/100], Step [20/19], Loss: 0.0783\n",
      "Epoch [10/100], Step [30/19], Loss: 0.6448\n",
      "Epoch [10/100], Step [40/19], Loss: 0.0963\n",
      "Epoch [10/100], Step [50/19], Loss: 0.6239\n",
      "Epoch [10/100], Step [60/19], Loss: 0.0796\n",
      "Epoch [10/100], Step [70/19], Loss: 0.4241\n",
      "Epoch [10/100], Step [80/19], Loss: 0.1373\n",
      "Epoch [10/100], Step [90/19], Loss: 0.0998\n",
      "Epoch [10/100], Step [100/19], Loss: 0.4668\n",
      "Epoch [10/100], Step [110/19], Loss: 0.5573\n",
      "Epoch [10/100], Step [120/19], Loss: 0.4716\n",
      "Epoch [10/100], Step [130/19], Loss: 0.1035\n",
      "Epoch [10/100], Step [140/19], Loss: 0.4084\n",
      "Epoch [10/100], Step [150/19], Loss: 0.4699\n",
      "Epoch 9; loss = 0.2890\n",
      "Epoch [11/100], Step [10/19], Loss: 0.1093\n",
      "Epoch [11/100], Step [20/19], Loss: 0.0846\n",
      "Epoch [11/100], Step [30/19], Loss: 0.6315\n",
      "Epoch [11/100], Step [40/19], Loss: 0.0948\n",
      "Epoch [11/100], Step [50/19], Loss: 0.6140\n",
      "Epoch [11/100], Step [60/19], Loss: 0.0750\n",
      "Epoch [11/100], Step [70/19], Loss: 0.4171\n",
      "Epoch [11/100], Step [80/19], Loss: 0.1362\n",
      "Epoch [11/100], Step [90/19], Loss: 0.1027\n",
      "Epoch [11/100], Step [100/19], Loss: 0.4650\n",
      "Epoch [11/100], Step [110/19], Loss: 0.5505\n",
      "Epoch [11/100], Step [120/19], Loss: 0.4693\n",
      "Epoch [11/100], Step [130/19], Loss: 0.1001\n",
      "Epoch [11/100], Step [140/19], Loss: 0.4009\n",
      "Epoch [11/100], Step [150/19], Loss: 0.4649\n",
      "Epoch 10; loss = 0.2864\n",
      "Epoch [12/100], Step [10/19], Loss: 0.1084\n",
      "Epoch [12/100], Step [20/19], Loss: 0.0892\n",
      "Epoch [12/100], Step [30/19], Loss: 0.6217\n",
      "Epoch [12/100], Step [40/19], Loss: 0.0936\n",
      "Epoch [12/100], Step [50/19], Loss: 0.6076\n",
      "Epoch [12/100], Step [60/19], Loss: 0.0717\n",
      "Epoch [12/100], Step [70/19], Loss: 0.4103\n",
      "Epoch [12/100], Step [80/19], Loss: 0.1358\n",
      "Epoch [12/100], Step [90/19], Loss: 0.1052\n",
      "Epoch [12/100], Step [100/19], Loss: 0.4630\n",
      "Epoch [12/100], Step [110/19], Loss: 0.5434\n",
      "Epoch [12/100], Step [120/19], Loss: 0.4671\n",
      "Epoch [12/100], Step [130/19], Loss: 0.0973\n",
      "Epoch [12/100], Step [140/19], Loss: 0.3933\n",
      "Epoch [12/100], Step [150/19], Loss: 0.4612\n",
      "Epoch 11; loss = 0.2841\n",
      "Epoch [13/100], Step [10/19], Loss: 0.1081\n",
      "Epoch [13/100], Step [20/19], Loss: 0.0919\n",
      "Epoch [13/100], Step [30/19], Loss: 0.6152\n",
      "Epoch [13/100], Step [40/19], Loss: 0.0926\n",
      "Epoch [13/100], Step [50/19], Loss: 0.6036\n",
      "Epoch [13/100], Step [60/19], Loss: 0.0691\n",
      "Epoch [13/100], Step [70/19], Loss: 0.4038\n",
      "Epoch [13/100], Step [80/19], Loss: 0.1362\n",
      "Epoch [13/100], Step [90/19], Loss: 0.1072\n",
      "Epoch [13/100], Step [100/19], Loss: 0.4608\n",
      "Epoch [13/100], Step [110/19], Loss: 0.5358\n",
      "Epoch [13/100], Step [120/19], Loss: 0.4650\n",
      "Epoch [13/100], Step [130/19], Loss: 0.0949\n",
      "Epoch [13/100], Step [140/19], Loss: 0.3860\n",
      "Epoch [13/100], Step [150/19], Loss: 0.4583\n",
      "Epoch 12; loss = 0.2820\n",
      "Epoch [14/100], Step [10/19], Loss: 0.1083\n",
      "Epoch [14/100], Step [20/19], Loss: 0.0931\n",
      "Epoch [14/100], Step [30/19], Loss: 0.6113\n",
      "Epoch [14/100], Step [40/19], Loss: 0.0918\n",
      "Epoch [14/100], Step [50/19], Loss: 0.6011\n",
      "Epoch [14/100], Step [60/19], Loss: 0.0671\n",
      "Epoch [14/100], Step [70/19], Loss: 0.3976\n",
      "Epoch [14/100], Step [80/19], Loss: 0.1371\n",
      "Epoch [14/100], Step [90/19], Loss: 0.1086\n",
      "Epoch [14/100], Step [100/19], Loss: 0.4581\n",
      "Epoch [14/100], Step [110/19], Loss: 0.5281\n",
      "Epoch [14/100], Step [120/19], Loss: 0.4629\n",
      "Epoch [14/100], Step [130/19], Loss: 0.0929\n",
      "Epoch [14/100], Step [140/19], Loss: 0.3788\n",
      "Epoch [14/100], Step [150/19], Loss: 0.4559\n",
      "Epoch 13; loss = 0.2800\n",
      "Epoch [15/100], Step [10/19], Loss: 0.1088\n",
      "Epoch [15/100], Step [20/19], Loss: 0.0935\n",
      "Epoch [15/100], Step [30/19], Loss: 0.6090\n",
      "Epoch [15/100], Step [40/19], Loss: 0.0909\n",
      "Epoch [15/100], Step [50/19], Loss: 0.5996\n",
      "Epoch [15/100], Step [60/19], Loss: 0.0655\n",
      "Epoch [15/100], Step [70/19], Loss: 0.3916\n",
      "Epoch [15/100], Step [80/19], Loss: 0.1384\n",
      "Epoch [15/100], Step [90/19], Loss: 0.1098\n",
      "Epoch [15/100], Step [100/19], Loss: 0.4557\n",
      "Epoch [15/100], Step [110/19], Loss: 0.5200\n",
      "Epoch [15/100], Step [120/19], Loss: 0.4609\n",
      "Epoch [15/100], Step [130/19], Loss: 0.0913\n",
      "Epoch [15/100], Step [140/19], Loss: 0.3720\n",
      "Epoch [15/100], Step [150/19], Loss: 0.4538\n",
      "Epoch 14; loss = 0.2782\n",
      "Epoch [16/100], Step [10/19], Loss: 0.1099\n",
      "Epoch [16/100], Step [20/19], Loss: 0.0933\n",
      "Epoch [16/100], Step [30/19], Loss: 0.6079\n",
      "Epoch [16/100], Step [40/19], Loss: 0.0908\n",
      "Epoch [16/100], Step [50/19], Loss: 0.5990\n",
      "Epoch [16/100], Step [60/19], Loss: 0.0644\n",
      "Epoch [16/100], Step [70/19], Loss: 0.3861\n",
      "Epoch [16/100], Step [80/19], Loss: 0.1395\n",
      "Epoch [16/100], Step [90/19], Loss: 0.1106\n",
      "Epoch [16/100], Step [100/19], Loss: 0.4538\n",
      "Epoch [16/100], Step [110/19], Loss: 0.5122\n",
      "Epoch [16/100], Step [120/19], Loss: 0.4597\n",
      "Epoch [16/100], Step [130/19], Loss: 0.0892\n",
      "Epoch [16/100], Step [140/19], Loss: 0.3659\n",
      "Epoch [16/100], Step [150/19], Loss: 0.4521\n",
      "Epoch 15; loss = 0.2764\n",
      "Epoch [17/100], Step [10/19], Loss: 0.1109\n",
      "Epoch [17/100], Step [20/19], Loss: 0.0929\n",
      "Epoch [17/100], Step [30/19], Loss: 0.6073\n",
      "Epoch [17/100], Step [40/19], Loss: 0.0908\n",
      "Epoch [17/100], Step [50/19], Loss: 0.5986\n",
      "Epoch [17/100], Step [60/19], Loss: 0.0637\n",
      "Epoch [17/100], Step [70/19], Loss: 0.3807\n",
      "Epoch [17/100], Step [80/19], Loss: 0.1402\n",
      "Epoch [17/100], Step [90/19], Loss: 0.1102\n",
      "Epoch [17/100], Step [100/19], Loss: 0.4544\n",
      "Epoch [17/100], Step [110/19], Loss: 0.5031\n",
      "Epoch [17/100], Step [120/19], Loss: 0.4606\n",
      "Epoch [17/100], Step [130/19], Loss: 0.0853\n",
      "Epoch [17/100], Step [140/19], Loss: 0.3639\n",
      "Epoch [17/100], Step [150/19], Loss: 0.4561\n",
      "Epoch 16; loss = 0.2747\n",
      "Epoch [18/100], Step [10/19], Loss: 0.1107\n",
      "Epoch [18/100], Step [20/19], Loss: 0.0925\n",
      "Epoch [18/100], Step [30/19], Loss: 0.6070\n",
      "Epoch [18/100], Step [40/19], Loss: 0.0911\n",
      "Epoch [18/100], Step [50/19], Loss: 0.5984\n",
      "Epoch [18/100], Step [60/19], Loss: 0.0634\n",
      "Epoch [18/100], Step [70/19], Loss: 0.3742\n",
      "Epoch [18/100], Step [80/19], Loss: 0.1414\n",
      "Epoch [18/100], Step [90/19], Loss: 0.1105\n",
      "Epoch [18/100], Step [100/19], Loss: 0.4492\n",
      "Epoch [18/100], Step [110/19], Loss: 0.4941\n",
      "Epoch [18/100], Step [120/19], Loss: 0.4599\n",
      "Epoch [18/100], Step [130/19], Loss: 0.0837\n",
      "Epoch [18/100], Step [140/19], Loss: 0.3581\n",
      "Epoch [18/100], Step [150/19], Loss: 0.4514\n",
      "Epoch 17; loss = 0.2731\n",
      "Epoch [19/100], Step [10/19], Loss: 0.1122\n",
      "Epoch [19/100], Step [20/19], Loss: 0.0919\n",
      "Epoch [19/100], Step [30/19], Loss: 0.6073\n",
      "Epoch [19/100], Step [40/19], Loss: 0.0916\n",
      "Epoch [19/100], Step [50/19], Loss: 0.5984\n",
      "Epoch [19/100], Step [60/19], Loss: 0.0641\n",
      "Epoch [19/100], Step [70/19], Loss: 0.3650\n",
      "Epoch [19/100], Step [80/19], Loss: 0.1427\n",
      "Epoch [19/100], Step [90/19], Loss: 0.1109\n",
      "Epoch [19/100], Step [100/19], Loss: 0.4449\n",
      "Epoch [19/100], Step [110/19], Loss: 0.4854\n",
      "Epoch [19/100], Step [120/19], Loss: 0.4558\n",
      "Epoch [19/100], Step [130/19], Loss: 0.0819\n",
      "Epoch [19/100], Step [140/19], Loss: 0.3535\n",
      "Epoch [19/100], Step [150/19], Loss: 0.4495\n",
      "Epoch 18; loss = 0.2714\n",
      "Epoch [20/100], Step [10/19], Loss: 0.1134\n",
      "Epoch [20/100], Step [20/19], Loss: 0.0912\n",
      "Epoch [20/100], Step [30/19], Loss: 0.6079\n",
      "Epoch [20/100], Step [40/19], Loss: 0.0918\n",
      "Epoch [20/100], Step [50/19], Loss: 0.5985\n",
      "Epoch [20/100], Step [60/19], Loss: 0.0643\n",
      "Epoch [20/100], Step [70/19], Loss: 0.3579\n",
      "Epoch [20/100], Step [80/19], Loss: 0.1437\n",
      "Epoch [20/100], Step [90/19], Loss: 0.1113\n",
      "Epoch [20/100], Step [100/19], Loss: 0.4432\n",
      "Epoch [20/100], Step [110/19], Loss: 0.4768\n",
      "Epoch [20/100], Step [120/19], Loss: 0.4558\n",
      "Epoch [20/100], Step [130/19], Loss: 0.0798\n",
      "Epoch [20/100], Step [140/19], Loss: 0.3483\n",
      "Epoch [20/100], Step [150/19], Loss: 0.4497\n",
      "Epoch 19; loss = 0.2696\n",
      "Epoch [21/100], Step [10/19], Loss: 0.1152\n",
      "Epoch [21/100], Step [20/19], Loss: 0.0910\n",
      "Epoch [21/100], Step [30/19], Loss: 0.6080\n",
      "Epoch [21/100], Step [40/19], Loss: 0.0923\n",
      "Epoch [21/100], Step [50/19], Loss: 0.5981\n",
      "Epoch [21/100], Step [60/19], Loss: 0.0646\n",
      "Epoch [21/100], Step [70/19], Loss: 0.3523\n",
      "Epoch [21/100], Step [80/19], Loss: 0.1452\n",
      "Epoch [21/100], Step [90/19], Loss: 0.1114\n",
      "Epoch [21/100], Step [100/19], Loss: 0.4415\n",
      "Epoch [21/100], Step [110/19], Loss: 0.4677\n",
      "Epoch [21/100], Step [120/19], Loss: 0.4566\n",
      "Epoch [21/100], Step [130/19], Loss: 0.0781\n",
      "Epoch [21/100], Step [140/19], Loss: 0.3641\n",
      "Epoch [21/100], Step [150/19], Loss: 0.4491\n",
      "Epoch 20; loss = 0.2682\n",
      "Epoch [22/100], Step [10/19], Loss: 0.1178\n",
      "Epoch [22/100], Step [20/19], Loss: 0.0907\n",
      "Epoch [22/100], Step [30/19], Loss: 0.6086\n",
      "Epoch [22/100], Step [40/19], Loss: 0.0929\n",
      "Epoch [22/100], Step [50/19], Loss: 0.5972\n",
      "Epoch [22/100], Step [60/19], Loss: 0.0651\n",
      "Epoch [22/100], Step [70/19], Loss: 0.3472\n",
      "Epoch [22/100], Step [80/19], Loss: 0.1474\n",
      "Epoch [22/100], Step [90/19], Loss: 0.1116\n",
      "Epoch [22/100], Step [100/19], Loss: 0.4393\n",
      "Epoch [22/100], Step [110/19], Loss: 0.4577\n",
      "Epoch [22/100], Step [120/19], Loss: 0.4571\n",
      "Epoch [22/100], Step [130/19], Loss: 0.0767\n",
      "Epoch [22/100], Step [140/19], Loss: 0.3457\n",
      "Epoch [22/100], Step [150/19], Loss: 0.4489\n",
      "Epoch 21; loss = 0.2670\n",
      "Epoch [23/100], Step [10/19], Loss: 0.1194\n",
      "Epoch [23/100], Step [20/19], Loss: 0.0897\n",
      "Epoch [23/100], Step [30/19], Loss: 0.6102\n",
      "Epoch [23/100], Step [40/19], Loss: 0.0926\n",
      "Epoch [23/100], Step [50/19], Loss: 0.5978\n",
      "Epoch [23/100], Step [60/19], Loss: 0.0653\n",
      "Epoch [23/100], Step [70/19], Loss: 0.3431\n",
      "Epoch [23/100], Step [80/19], Loss: 0.1483\n",
      "Epoch [23/100], Step [90/19], Loss: 0.1113\n",
      "Epoch [23/100], Step [100/19], Loss: 0.4368\n",
      "Epoch [23/100], Step [110/19], Loss: 0.4478\n",
      "Epoch [23/100], Step [120/19], Loss: 0.4585\n",
      "Epoch [23/100], Step [130/19], Loss: 0.0753\n",
      "Epoch [23/100], Step [140/19], Loss: 0.3430\n",
      "Epoch [23/100], Step [150/19], Loss: 0.4487\n",
      "Epoch 22; loss = 0.2654\n",
      "Epoch [24/100], Step [10/19], Loss: 0.1214\n",
      "Epoch [24/100], Step [20/19], Loss: 0.0890\n",
      "Epoch [24/100], Step [30/19], Loss: 0.6113\n",
      "Epoch [24/100], Step [40/19], Loss: 0.0929\n",
      "Epoch [24/100], Step [50/19], Loss: 0.5977\n",
      "Epoch [24/100], Step [60/19], Loss: 0.0656\n",
      "Epoch [24/100], Step [70/19], Loss: 0.3391\n",
      "Epoch [24/100], Step [80/19], Loss: 0.1494\n",
      "Epoch [24/100], Step [90/19], Loss: 0.1112\n",
      "Epoch [24/100], Step [100/19], Loss: 0.4337\n",
      "Epoch [24/100], Step [110/19], Loss: 0.4378\n",
      "Epoch [24/100], Step [120/19], Loss: 0.4594\n",
      "Epoch [24/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [24/100], Step [140/19], Loss: 0.3385\n",
      "Epoch [24/100], Step [150/19], Loss: 0.4483\n",
      "Epoch 23; loss = 0.2640\n",
      "Epoch [25/100], Step [10/19], Loss: 0.1241\n",
      "Epoch [25/100], Step [20/19], Loss: 0.0883\n",
      "Epoch [25/100], Step [30/19], Loss: 0.6126\n",
      "Epoch [25/100], Step [40/19], Loss: 0.0929\n",
      "Epoch [25/100], Step [50/19], Loss: 0.5976\n",
      "Epoch [25/100], Step [60/19], Loss: 0.0660\n",
      "Epoch [25/100], Step [70/19], Loss: 0.3354\n",
      "Epoch [25/100], Step [80/19], Loss: 0.1504\n",
      "Epoch [25/100], Step [90/19], Loss: 0.1111\n",
      "Epoch [25/100], Step [100/19], Loss: 0.4300\n",
      "Epoch [25/100], Step [110/19], Loss: 0.4278\n",
      "Epoch [25/100], Step [120/19], Loss: 0.4597\n",
      "Epoch [25/100], Step [130/19], Loss: 0.0740\n",
      "Epoch [25/100], Step [140/19], Loss: 0.3347\n",
      "Epoch [25/100], Step [150/19], Loss: 0.4482\n",
      "Epoch 24; loss = 0.2626\n",
      "Epoch [26/100], Step [10/19], Loss: 0.1250\n",
      "Epoch [26/100], Step [20/19], Loss: 0.0878\n",
      "Epoch [26/100], Step [30/19], Loss: 0.6138\n",
      "Epoch [26/100], Step [40/19], Loss: 0.0931\n",
      "Epoch [26/100], Step [50/19], Loss: 0.5975\n",
      "Epoch [26/100], Step [60/19], Loss: 0.0664\n",
      "Epoch [26/100], Step [70/19], Loss: 0.3315\n",
      "Epoch [26/100], Step [80/19], Loss: 0.1506\n",
      "Epoch [26/100], Step [90/19], Loss: 0.1108\n",
      "Epoch [26/100], Step [100/19], Loss: 0.4260\n",
      "Epoch [26/100], Step [110/19], Loss: 0.4185\n",
      "Epoch [26/100], Step [120/19], Loss: 0.4598\n",
      "Epoch [26/100], Step [130/19], Loss: 0.0738\n",
      "Epoch [26/100], Step [140/19], Loss: 0.3303\n",
      "Epoch [26/100], Step [150/19], Loss: 0.4479\n",
      "Epoch 25; loss = 0.2614\n",
      "Epoch [27/100], Step [10/19], Loss: 0.1265\n",
      "Epoch [27/100], Step [20/19], Loss: 0.0872\n",
      "Epoch [27/100], Step [30/19], Loss: 0.6152\n",
      "Epoch [27/100], Step [40/19], Loss: 0.0931\n",
      "Epoch [27/100], Step [50/19], Loss: 0.5972\n",
      "Epoch [27/100], Step [60/19], Loss: 0.0668\n",
      "Epoch [27/100], Step [70/19], Loss: 0.3283\n",
      "Epoch [27/100], Step [80/19], Loss: 0.1511\n",
      "Epoch [27/100], Step [90/19], Loss: 0.1106\n",
      "Epoch [27/100], Step [100/19], Loss: 0.4219\n",
      "Epoch [27/100], Step [110/19], Loss: 0.4099\n",
      "Epoch [27/100], Step [120/19], Loss: 0.4597\n",
      "Epoch [27/100], Step [130/19], Loss: 0.0737\n",
      "Epoch [27/100], Step [140/19], Loss: 0.3266\n",
      "Epoch [27/100], Step [150/19], Loss: 0.4477\n",
      "Epoch 26; loss = 0.2600\n",
      "Epoch [28/100], Step [10/19], Loss: 0.1276\n",
      "Epoch [28/100], Step [20/19], Loss: 0.0867\n",
      "Epoch [28/100], Step [30/19], Loss: 0.6165\n",
      "Epoch [28/100], Step [40/19], Loss: 0.0932\n",
      "Epoch [28/100], Step [50/19], Loss: 0.5968\n",
      "Epoch [28/100], Step [60/19], Loss: 0.0674\n",
      "Epoch [28/100], Step [70/19], Loss: 0.3252\n",
      "Epoch [28/100], Step [80/19], Loss: 0.1515\n",
      "Epoch [28/100], Step [90/19], Loss: 0.1104\n",
      "Epoch [28/100], Step [100/19], Loss: 0.4178\n",
      "Epoch [28/100], Step [110/19], Loss: 0.4022\n",
      "Epoch [28/100], Step [120/19], Loss: 0.4597\n",
      "Epoch [28/100], Step [130/19], Loss: 0.0737\n",
      "Epoch [28/100], Step [140/19], Loss: 0.3239\n",
      "Epoch [28/100], Step [150/19], Loss: 0.4476\n",
      "Epoch 27; loss = 0.2588\n",
      "Epoch [29/100], Step [10/19], Loss: 0.1286\n",
      "Epoch [29/100], Step [20/19], Loss: 0.0864\n",
      "Epoch [29/100], Step [30/19], Loss: 0.6177\n",
      "Epoch [29/100], Step [40/19], Loss: 0.0935\n",
      "Epoch [29/100], Step [50/19], Loss: 0.5966\n",
      "Epoch [29/100], Step [60/19], Loss: 0.0680\n",
      "Epoch [29/100], Step [70/19], Loss: 0.3224\n",
      "Epoch [29/100], Step [80/19], Loss: 0.1516\n",
      "Epoch [29/100], Step [90/19], Loss: 0.1100\n",
      "Epoch [29/100], Step [100/19], Loss: 0.4140\n",
      "Epoch [29/100], Step [110/19], Loss: 0.3952\n",
      "Epoch [29/100], Step [120/19], Loss: 0.4596\n",
      "Epoch [29/100], Step [130/19], Loss: 0.0737\n",
      "Epoch [29/100], Step [140/19], Loss: 0.3207\n",
      "Epoch [29/100], Step [150/19], Loss: 0.4477\n",
      "Epoch 28; loss = 0.2577\n",
      "Epoch [30/100], Step [10/19], Loss: 0.1294\n",
      "Epoch [30/100], Step [20/19], Loss: 0.0860\n",
      "Epoch [30/100], Step [30/19], Loss: 0.6190\n",
      "Epoch [30/100], Step [40/19], Loss: 0.0937\n",
      "Epoch [30/100], Step [50/19], Loss: 0.5962\n",
      "Epoch [30/100], Step [60/19], Loss: 0.0687\n",
      "Epoch [30/100], Step [70/19], Loss: 0.3198\n",
      "Epoch [30/100], Step [80/19], Loss: 0.1518\n",
      "Epoch [30/100], Step [90/19], Loss: 0.1097\n",
      "Epoch [30/100], Step [100/19], Loss: 0.4102\n",
      "Epoch [30/100], Step [110/19], Loss: 0.3888\n",
      "Epoch [30/100], Step [120/19], Loss: 0.4598\n",
      "Epoch [30/100], Step [130/19], Loss: 0.0738\n",
      "Epoch [30/100], Step [140/19], Loss: 0.3178\n",
      "Epoch [30/100], Step [150/19], Loss: 0.4480\n",
      "Epoch 29; loss = 0.2565\n",
      "Epoch [31/100], Step [10/19], Loss: 0.1301\n",
      "Epoch [31/100], Step [20/19], Loss: 0.0856\n",
      "Epoch [31/100], Step [30/19], Loss: 0.6203\n",
      "Epoch [31/100], Step [40/19], Loss: 0.0941\n",
      "Epoch [31/100], Step [50/19], Loss: 0.5957\n",
      "Epoch [31/100], Step [60/19], Loss: 0.0694\n",
      "Epoch [31/100], Step [70/19], Loss: 0.3175\n",
      "Epoch [31/100], Step [80/19], Loss: 0.1521\n",
      "Epoch [31/100], Step [90/19], Loss: 0.1093\n",
      "Epoch [31/100], Step [100/19], Loss: 0.4066\n",
      "Epoch [31/100], Step [110/19], Loss: 0.3828\n",
      "Epoch [31/100], Step [120/19], Loss: 0.4601\n",
      "Epoch [31/100], Step [130/19], Loss: 0.0739\n",
      "Epoch [31/100], Step [140/19], Loss: 0.3156\n",
      "Epoch [31/100], Step [150/19], Loss: 0.4482\n",
      "Epoch 30; loss = 0.2554\n",
      "Epoch [32/100], Step [10/19], Loss: 0.1309\n",
      "Epoch [32/100], Step [20/19], Loss: 0.0853\n",
      "Epoch [32/100], Step [30/19], Loss: 0.6216\n",
      "Epoch [32/100], Step [40/19], Loss: 0.0948\n",
      "Epoch [32/100], Step [50/19], Loss: 0.5953\n",
      "Epoch [32/100], Step [60/19], Loss: 0.0703\n",
      "Epoch [32/100], Step [70/19], Loss: 0.3152\n",
      "Epoch [32/100], Step [80/19], Loss: 0.1522\n",
      "Epoch [32/100], Step [90/19], Loss: 0.1090\n",
      "Epoch [32/100], Step [100/19], Loss: 0.4031\n",
      "Epoch [32/100], Step [110/19], Loss: 0.3770\n",
      "Epoch [32/100], Step [120/19], Loss: 0.4601\n",
      "Epoch [32/100], Step [130/19], Loss: 0.0740\n",
      "Epoch [32/100], Step [140/19], Loss: 0.3130\n",
      "Epoch [32/100], Step [150/19], Loss: 0.4485\n",
      "Epoch 31; loss = 0.2543\n",
      "Epoch [33/100], Step [10/19], Loss: 0.1317\n",
      "Epoch [33/100], Step [20/19], Loss: 0.0850\n",
      "Epoch [33/100], Step [30/19], Loss: 0.6230\n",
      "Epoch [33/100], Step [40/19], Loss: 0.0955\n",
      "Epoch [33/100], Step [50/19], Loss: 0.5948\n",
      "Epoch [33/100], Step [60/19], Loss: 0.0714\n",
      "Epoch [33/100], Step [70/19], Loss: 0.3131\n",
      "Epoch [33/100], Step [80/19], Loss: 0.1523\n",
      "Epoch [33/100], Step [90/19], Loss: 0.1086\n",
      "Epoch [33/100], Step [100/19], Loss: 0.3996\n",
      "Epoch [33/100], Step [110/19], Loss: 0.3714\n",
      "Epoch [33/100], Step [120/19], Loss: 0.4603\n",
      "Epoch [33/100], Step [130/19], Loss: 0.0741\n",
      "Epoch [33/100], Step [140/19], Loss: 0.3108\n",
      "Epoch [33/100], Step [150/19], Loss: 0.4488\n",
      "Epoch 32; loss = 0.2532\n",
      "Epoch [34/100], Step [10/19], Loss: 0.1325\n",
      "Epoch [34/100], Step [20/19], Loss: 0.0847\n",
      "Epoch [34/100], Step [30/19], Loss: 0.6244\n",
      "Epoch [34/100], Step [40/19], Loss: 0.0963\n",
      "Epoch [34/100], Step [50/19], Loss: 0.5944\n",
      "Epoch [34/100], Step [60/19], Loss: 0.0727\n",
      "Epoch [34/100], Step [70/19], Loss: 0.3110\n",
      "Epoch [34/100], Step [80/19], Loss: 0.1521\n",
      "Epoch [34/100], Step [90/19], Loss: 0.1082\n",
      "Epoch [34/100], Step [100/19], Loss: 0.3962\n",
      "Epoch [34/100], Step [110/19], Loss: 0.3658\n",
      "Epoch [34/100], Step [120/19], Loss: 0.4601\n",
      "Epoch [34/100], Step [130/19], Loss: 0.0743\n",
      "Epoch [34/100], Step [140/19], Loss: 0.3082\n",
      "Epoch [34/100], Step [150/19], Loss: 0.4492\n",
      "Epoch 33; loss = 0.2522\n",
      "Epoch [35/100], Step [10/19], Loss: 0.1332\n",
      "Epoch [35/100], Step [20/19], Loss: 0.0844\n",
      "Epoch [35/100], Step [30/19], Loss: 0.6259\n",
      "Epoch [35/100], Step [40/19], Loss: 0.0972\n",
      "Epoch [35/100], Step [50/19], Loss: 0.5938\n",
      "Epoch [35/100], Step [60/19], Loss: 0.0741\n",
      "Epoch [35/100], Step [70/19], Loss: 0.3091\n",
      "Epoch [35/100], Step [80/19], Loss: 0.1519\n",
      "Epoch [35/100], Step [90/19], Loss: 0.1077\n",
      "Epoch [35/100], Step [100/19], Loss: 0.3927\n",
      "Epoch [35/100], Step [110/19], Loss: 0.3602\n",
      "Epoch [35/100], Step [120/19], Loss: 0.4599\n",
      "Epoch [35/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [35/100], Step [140/19], Loss: 0.3061\n",
      "Epoch [35/100], Step [150/19], Loss: 0.4496\n",
      "Epoch 34; loss = 0.2511\n",
      "Epoch [36/100], Step [10/19], Loss: 0.1339\n",
      "Epoch [36/100], Step [20/19], Loss: 0.0840\n",
      "Epoch [36/100], Step [30/19], Loss: 0.6276\n",
      "Epoch [36/100], Step [40/19], Loss: 0.0981\n",
      "Epoch [36/100], Step [50/19], Loss: 0.5930\n",
      "Epoch [36/100], Step [60/19], Loss: 0.0756\n",
      "Epoch [36/100], Step [70/19], Loss: 0.3072\n",
      "Epoch [36/100], Step [80/19], Loss: 0.1516\n",
      "Epoch [36/100], Step [90/19], Loss: 0.1072\n",
      "Epoch [36/100], Step [100/19], Loss: 0.3891\n",
      "Epoch [36/100], Step [110/19], Loss: 0.3544\n",
      "Epoch [36/100], Step [120/19], Loss: 0.4594\n",
      "Epoch [36/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [36/100], Step [140/19], Loss: 0.3039\n",
      "Epoch [36/100], Step [150/19], Loss: 0.4502\n",
      "Epoch 35; loss = 0.2501\n",
      "Epoch [37/100], Step [10/19], Loss: 0.1345\n",
      "Epoch [37/100], Step [20/19], Loss: 0.0836\n",
      "Epoch [37/100], Step [30/19], Loss: 0.6293\n",
      "Epoch [37/100], Step [40/19], Loss: 0.0991\n",
      "Epoch [37/100], Step [50/19], Loss: 0.5920\n",
      "Epoch [37/100], Step [60/19], Loss: 0.0771\n",
      "Epoch [37/100], Step [70/19], Loss: 0.3056\n",
      "Epoch [37/100], Step [80/19], Loss: 0.1511\n",
      "Epoch [37/100], Step [90/19], Loss: 0.1066\n",
      "Epoch [37/100], Step [100/19], Loss: 0.3854\n",
      "Epoch [37/100], Step [110/19], Loss: 0.3481\n",
      "Epoch [37/100], Step [120/19], Loss: 0.4589\n",
      "Epoch [37/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [37/100], Step [140/19], Loss: 0.3012\n",
      "Epoch [37/100], Step [150/19], Loss: 0.4504\n",
      "Epoch 36; loss = 0.2490\n",
      "Epoch [38/100], Step [10/19], Loss: 0.1350\n",
      "Epoch [38/100], Step [20/19], Loss: 0.0832\n",
      "Epoch [38/100], Step [30/19], Loss: 0.6310\n",
      "Epoch [38/100], Step [40/19], Loss: 0.1001\n",
      "Epoch [38/100], Step [50/19], Loss: 0.5902\n",
      "Epoch [38/100], Step [60/19], Loss: 0.0787\n",
      "Epoch [38/100], Step [70/19], Loss: 0.3041\n",
      "Epoch [38/100], Step [80/19], Loss: 0.1507\n",
      "Epoch [38/100], Step [90/19], Loss: 0.1060\n",
      "Epoch [38/100], Step [100/19], Loss: 0.3816\n",
      "Epoch [38/100], Step [110/19], Loss: 0.3415\n",
      "Epoch [38/100], Step [120/19], Loss: 0.4588\n",
      "Epoch [38/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [38/100], Step [140/19], Loss: 0.2985\n",
      "Epoch [38/100], Step [150/19], Loss: 0.4504\n",
      "Epoch 37; loss = 0.2479\n",
      "Epoch [39/100], Step [10/19], Loss: 0.1352\n",
      "Epoch [39/100], Step [20/19], Loss: 0.0827\n",
      "Epoch [39/100], Step [30/19], Loss: 0.6331\n",
      "Epoch [39/100], Step [40/19], Loss: 0.1013\n",
      "Epoch [39/100], Step [50/19], Loss: 0.5877\n",
      "Epoch [39/100], Step [60/19], Loss: 0.0804\n",
      "Epoch [39/100], Step [70/19], Loss: 0.3026\n",
      "Epoch [39/100], Step [80/19], Loss: 0.1502\n",
      "Epoch [39/100], Step [90/19], Loss: 0.1051\n",
      "Epoch [39/100], Step [100/19], Loss: 0.3780\n",
      "Epoch [39/100], Step [110/19], Loss: 0.3346\n",
      "Epoch [39/100], Step [120/19], Loss: 0.4588\n",
      "Epoch [39/100], Step [130/19], Loss: 0.0746\n",
      "Epoch [39/100], Step [140/19], Loss: 0.2958\n",
      "Epoch [39/100], Step [150/19], Loss: 0.4501\n",
      "Epoch 38; loss = 0.2470\n",
      "Epoch [40/100], Step [10/19], Loss: 0.1351\n",
      "Epoch [40/100], Step [20/19], Loss: 0.0822\n",
      "Epoch [40/100], Step [30/19], Loss: 0.6356\n",
      "Epoch [40/100], Step [40/19], Loss: 0.1025\n",
      "Epoch [40/100], Step [50/19], Loss: 0.5847\n",
      "Epoch [40/100], Step [60/19], Loss: 0.0823\n",
      "Epoch [40/100], Step [70/19], Loss: 0.3007\n",
      "Epoch [40/100], Step [80/19], Loss: 0.1499\n",
      "Epoch [40/100], Step [90/19], Loss: 0.1044\n",
      "Epoch [40/100], Step [100/19], Loss: 0.3743\n",
      "Epoch [40/100], Step [110/19], Loss: 0.3274\n",
      "Epoch [40/100], Step [120/19], Loss: 0.4587\n",
      "Epoch [40/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [40/100], Step [140/19], Loss: 0.2942\n",
      "Epoch [40/100], Step [150/19], Loss: 0.4488\n",
      "Epoch 39; loss = 0.2459\n",
      "Epoch [41/100], Step [10/19], Loss: 0.1350\n",
      "Epoch [41/100], Step [20/19], Loss: 0.0815\n",
      "Epoch [41/100], Step [30/19], Loss: 0.6384\n",
      "Epoch [41/100], Step [40/19], Loss: 0.1037\n",
      "Epoch [41/100], Step [50/19], Loss: 0.5816\n",
      "Epoch [41/100], Step [60/19], Loss: 0.0836\n",
      "Epoch [41/100], Step [70/19], Loss: 0.2981\n",
      "Epoch [41/100], Step [80/19], Loss: 0.1495\n",
      "Epoch [41/100], Step [90/19], Loss: 0.1037\n",
      "Epoch [41/100], Step [100/19], Loss: 0.3701\n",
      "Epoch [41/100], Step [110/19], Loss: 0.3198\n",
      "Epoch [41/100], Step [120/19], Loss: 0.4569\n",
      "Epoch [41/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [41/100], Step [140/19], Loss: 0.2925\n",
      "Epoch [41/100], Step [150/19], Loss: 0.4434\n",
      "Epoch 40; loss = 0.2449\n",
      "Epoch [42/100], Step [10/19], Loss: 0.1343\n",
      "Epoch [42/100], Step [20/19], Loss: 0.0809\n",
      "Epoch [42/100], Step [30/19], Loss: 0.6418\n",
      "Epoch [42/100], Step [40/19], Loss: 0.1046\n",
      "Epoch [42/100], Step [50/19], Loss: 0.5781\n",
      "Epoch [42/100], Step [60/19], Loss: 0.0862\n",
      "Epoch [42/100], Step [70/19], Loss: 0.2951\n",
      "Epoch [42/100], Step [80/19], Loss: 0.1491\n",
      "Epoch [42/100], Step [90/19], Loss: 0.1034\n",
      "Epoch [42/100], Step [100/19], Loss: 0.3642\n",
      "Epoch [42/100], Step [110/19], Loss: 0.3116\n",
      "Epoch [42/100], Step [120/19], Loss: 0.4503\n",
      "Epoch [42/100], Step [130/19], Loss: 0.0737\n",
      "Epoch [42/100], Step [140/19], Loss: 0.2911\n",
      "Epoch [42/100], Step [150/19], Loss: 0.4384\n",
      "Epoch 41; loss = 0.2437\n",
      "Epoch [43/100], Step [10/19], Loss: 0.1331\n",
      "Epoch [43/100], Step [20/19], Loss: 0.0801\n",
      "Epoch [43/100], Step [30/19], Loss: 0.6460\n",
      "Epoch [43/100], Step [40/19], Loss: 0.1052\n",
      "Epoch [43/100], Step [50/19], Loss: 0.5734\n",
      "Epoch [43/100], Step [60/19], Loss: 0.0883\n",
      "Epoch [43/100], Step [70/19], Loss: 0.2910\n",
      "Epoch [43/100], Step [80/19], Loss: 0.1492\n",
      "Epoch [43/100], Step [90/19], Loss: 0.1029\n",
      "Epoch [43/100], Step [100/19], Loss: 0.3612\n",
      "Epoch [43/100], Step [110/19], Loss: 0.3021\n",
      "Epoch [43/100], Step [120/19], Loss: 0.4508\n",
      "Epoch [43/100], Step [130/19], Loss: 0.0729\n",
      "Epoch [43/100], Step [140/19], Loss: 0.2901\n",
      "Epoch [43/100], Step [150/19], Loss: 0.4389\n",
      "Epoch 42; loss = 0.2426\n",
      "Epoch [44/100], Step [10/19], Loss: 0.1327\n",
      "Epoch [44/100], Step [20/19], Loss: 0.0791\n",
      "Epoch [44/100], Step [30/19], Loss: 0.6525\n",
      "Epoch [44/100], Step [40/19], Loss: 0.1064\n",
      "Epoch [44/100], Step [50/19], Loss: 0.5678\n",
      "Epoch [44/100], Step [60/19], Loss: 0.0908\n",
      "Epoch [44/100], Step [70/19], Loss: 0.2838\n",
      "Epoch [44/100], Step [80/19], Loss: 0.1493\n",
      "Epoch [44/100], Step [90/19], Loss: 0.1016\n",
      "Epoch [44/100], Step [100/19], Loss: 0.3585\n",
      "Epoch [44/100], Step [110/19], Loss: 0.2899\n",
      "Epoch [44/100], Step [120/19], Loss: 0.4523\n",
      "Epoch [44/100], Step [130/19], Loss: 0.0716\n",
      "Epoch [44/100], Step [140/19], Loss: 0.2885\n",
      "Epoch [44/100], Step [150/19], Loss: 0.4424\n",
      "Epoch 43; loss = 0.2415\n",
      "Epoch [45/100], Step [10/19], Loss: 0.1359\n",
      "Epoch [45/100], Step [20/19], Loss: 0.0776\n",
      "Epoch [45/100], Step [30/19], Loss: 0.6577\n",
      "Epoch [45/100], Step [40/19], Loss: 0.1097\n",
      "Epoch [45/100], Step [50/19], Loss: 0.5707\n",
      "Epoch [45/100], Step [60/19], Loss: 0.0930\n",
      "Epoch [45/100], Step [70/19], Loss: 0.2797\n",
      "Epoch [45/100], Step [80/19], Loss: 0.1489\n",
      "Epoch [45/100], Step [90/19], Loss: 0.0988\n",
      "Epoch [45/100], Step [100/19], Loss: 0.3574\n",
      "Epoch [45/100], Step [110/19], Loss: 0.2842\n",
      "Epoch [45/100], Step [120/19], Loss: 0.4520\n",
      "Epoch [45/100], Step [130/19], Loss: 0.0705\n",
      "Epoch [45/100], Step [140/19], Loss: 0.2885\n",
      "Epoch [45/100], Step [150/19], Loss: 0.4405\n",
      "Epoch 44; loss = 0.2397\n",
      "Epoch [46/100], Step [10/19], Loss: 0.1317\n",
      "Epoch [46/100], Step [20/19], Loss: 0.0757\n",
      "Epoch [46/100], Step [30/19], Loss: 0.6679\n",
      "Epoch [46/100], Step [40/19], Loss: 0.1082\n",
      "Epoch [46/100], Step [50/19], Loss: 0.5604\n",
      "Epoch [46/100], Step [60/19], Loss: 0.0965\n",
      "Epoch [46/100], Step [70/19], Loss: 0.2627\n",
      "Epoch [46/100], Step [80/19], Loss: 0.1497\n",
      "Epoch [46/100], Step [90/19], Loss: 0.0965\n",
      "Epoch [46/100], Step [100/19], Loss: 0.3550\n",
      "Epoch [46/100], Step [110/19], Loss: 0.2577\n",
      "Epoch [46/100], Step [120/19], Loss: 0.4881\n",
      "Epoch [46/100], Step [130/19], Loss: 0.0722\n",
      "Epoch [46/100], Step [140/19], Loss: 0.2822\n",
      "Epoch [46/100], Step [150/19], Loss: 0.4370\n",
      "Epoch 45; loss = 0.2399\n",
      "Epoch [47/100], Step [10/19], Loss: 0.1354\n",
      "Epoch [47/100], Step [20/19], Loss: 0.0223\n",
      "Epoch [47/100], Step [30/19], Loss: 0.6178\n",
      "Epoch [47/100], Step [40/19], Loss: 0.1108\n",
      "Epoch [47/100], Step [50/19], Loss: 0.8983\n",
      "Epoch [47/100], Step [60/19], Loss: 0.0732\n",
      "Epoch [47/100], Step [70/19], Loss: 0.3273\n",
      "Epoch [47/100], Step [80/19], Loss: 0.1455\n",
      "Epoch [47/100], Step [90/19], Loss: 0.0796\n",
      "Epoch [47/100], Step [100/19], Loss: 0.3618\n",
      "Epoch [47/100], Step [110/19], Loss: 0.2681\n",
      "Epoch [47/100], Step [120/19], Loss: 0.4262\n",
      "Epoch [47/100], Step [130/19], Loss: 0.0770\n",
      "Epoch [47/100], Step [140/19], Loss: 0.2746\n",
      "Epoch [47/100], Step [150/19], Loss: 0.4281\n",
      "Epoch 46; loss = 0.2509\n",
      "Epoch [48/100], Step [10/19], Loss: 0.1500\n",
      "Epoch [48/100], Step [20/19], Loss: 0.0920\n",
      "Epoch [48/100], Step [30/19], Loss: 1.0526\n",
      "Epoch [48/100], Step [40/19], Loss: 0.0970\n",
      "Epoch [48/100], Step [50/19], Loss: 0.5662\n",
      "Epoch [48/100], Step [60/19], Loss: 0.1007\n",
      "Epoch [48/100], Step [70/19], Loss: 0.2481\n",
      "Epoch [48/100], Step [80/19], Loss: 0.1414\n",
      "Epoch [48/100], Step [90/19], Loss: 0.0757\n",
      "Epoch [48/100], Step [100/19], Loss: 0.3474\n",
      "Epoch [48/100], Step [110/19], Loss: 0.2834\n",
      "Epoch [48/100], Step [120/19], Loss: 0.4081\n",
      "Epoch [48/100], Step [130/19], Loss: 0.0763\n",
      "Epoch [48/100], Step [140/19], Loss: 0.2865\n",
      "Epoch [48/100], Step [150/19], Loss: 0.4155\n",
      "Epoch 47; loss = 0.2410\n",
      "Epoch [49/100], Step [10/19], Loss: 0.1388\n",
      "Epoch [49/100], Step [20/19], Loss: 0.0581\n",
      "Epoch [49/100], Step [30/19], Loss: 1.0432\n",
      "Epoch [49/100], Step [40/19], Loss: 0.0983\n",
      "Epoch [49/100], Step [50/19], Loss: 0.4666\n",
      "Epoch [49/100], Step [60/19], Loss: 0.0931\n",
      "Epoch [49/100], Step [70/19], Loss: 0.2481\n",
      "Epoch [49/100], Step [80/19], Loss: 0.1350\n",
      "Epoch [49/100], Step [90/19], Loss: 0.0700\n",
      "Epoch [49/100], Step [100/19], Loss: 0.3442\n",
      "Epoch [49/100], Step [110/19], Loss: 0.2819\n",
      "Epoch [49/100], Step [120/19], Loss: 0.4082\n",
      "Epoch [49/100], Step [130/19], Loss: 0.0748\n",
      "Epoch [49/100], Step [140/19], Loss: 0.2879\n",
      "Epoch [49/100], Step [150/19], Loss: 0.4238\n",
      "Epoch 48; loss = 0.2367\n",
      "Epoch [50/100], Step [10/19], Loss: 0.1369\n",
      "Epoch [50/100], Step [20/19], Loss: 0.0542\n",
      "Epoch [50/100], Step [30/19], Loss: 1.0199\n",
      "Epoch [50/100], Step [40/19], Loss: 0.1024\n",
      "Epoch [50/100], Step [50/19], Loss: 0.4731\n",
      "Epoch [50/100], Step [60/19], Loss: 0.0896\n",
      "Epoch [50/100], Step [70/19], Loss: 0.2690\n",
      "Epoch [50/100], Step [80/19], Loss: 0.1336\n",
      "Epoch [50/100], Step [90/19], Loss: 0.0705\n",
      "Epoch [50/100], Step [100/19], Loss: 0.3420\n",
      "Epoch [50/100], Step [110/19], Loss: 0.2821\n",
      "Epoch [50/100], Step [120/19], Loss: 0.4021\n",
      "Epoch [50/100], Step [130/19], Loss: 0.0747\n",
      "Epoch [50/100], Step [140/19], Loss: 0.2867\n",
      "Epoch [50/100], Step [150/19], Loss: 0.4170\n",
      "Epoch 49; loss = 0.2369\n",
      "Epoch [51/100], Step [10/19], Loss: 0.1365\n",
      "Epoch [51/100], Step [20/19], Loss: 0.0880\n",
      "Epoch [51/100], Step [30/19], Loss: 1.0494\n",
      "Epoch [51/100], Step [40/19], Loss: 0.0940\n",
      "Epoch [51/100], Step [50/19], Loss: 0.4842\n",
      "Epoch [51/100], Step [60/19], Loss: 0.0892\n",
      "Epoch [51/100], Step [70/19], Loss: 0.2530\n",
      "Epoch [51/100], Step [80/19], Loss: 0.1323\n",
      "Epoch [51/100], Step [90/19], Loss: 0.0668\n",
      "Epoch [51/100], Step [100/19], Loss: 0.3405\n",
      "Epoch [51/100], Step [110/19], Loss: 0.2752\n",
      "Epoch [51/100], Step [120/19], Loss: 0.4117\n",
      "Epoch [51/100], Step [130/19], Loss: 0.0740\n",
      "Epoch [51/100], Step [140/19], Loss: 0.2852\n",
      "Epoch [51/100], Step [150/19], Loss: 0.4173\n",
      "Epoch 50; loss = 0.2358\n",
      "Epoch [52/100], Step [10/19], Loss: 0.1359\n",
      "Epoch [52/100], Step [20/19], Loss: 0.0514\n",
      "Epoch [52/100], Step [30/19], Loss: 0.9809\n",
      "Epoch [52/100], Step [40/19], Loss: 0.1168\n",
      "Epoch [52/100], Step [50/19], Loss: 0.4994\n",
      "Epoch [52/100], Step [60/19], Loss: 0.0880\n",
      "Epoch [52/100], Step [70/19], Loss: 0.2745\n",
      "Epoch [52/100], Step [80/19], Loss: 0.1305\n",
      "Epoch [52/100], Step [90/19], Loss: 0.0671\n",
      "Epoch [52/100], Step [100/19], Loss: 0.3384\n",
      "Epoch [52/100], Step [110/19], Loss: 0.2889\n",
      "Epoch [52/100], Step [120/19], Loss: 0.4011\n",
      "Epoch [52/100], Step [130/19], Loss: 0.0751\n",
      "Epoch [52/100], Step [140/19], Loss: 0.2841\n",
      "Epoch [52/100], Step [150/19], Loss: 0.4137\n",
      "Epoch 51; loss = 0.2369\n",
      "Epoch [53/100], Step [10/19], Loss: 0.1398\n",
      "Epoch [53/100], Step [20/19], Loss: 0.1113\n",
      "Epoch [53/100], Step [30/19], Loss: 1.0362\n",
      "Epoch [53/100], Step [40/19], Loss: 0.0937\n",
      "Epoch [53/100], Step [50/19], Loss: 0.5080\n",
      "Epoch [53/100], Step [60/19], Loss: 0.0868\n",
      "Epoch [53/100], Step [70/19], Loss: 0.2566\n",
      "Epoch [53/100], Step [80/19], Loss: 0.1314\n",
      "Epoch [53/100], Step [90/19], Loss: 0.0619\n",
      "Epoch [53/100], Step [100/19], Loss: 0.3439\n",
      "Epoch [53/100], Step [110/19], Loss: 0.2748\n",
      "Epoch [53/100], Step [120/19], Loss: 0.3839\n",
      "Epoch [53/100], Step [130/19], Loss: 0.0752\n",
      "Epoch [53/100], Step [140/19], Loss: 0.2819\n",
      "Epoch [53/100], Step [150/19], Loss: 0.4256\n",
      "Epoch 52; loss = 0.2354\n",
      "Epoch [54/100], Step [10/19], Loss: 0.1364\n",
      "Epoch [54/100], Step [20/19], Loss: 0.0513\n",
      "Epoch [54/100], Step [30/19], Loss: 0.9461\n",
      "Epoch [54/100], Step [40/19], Loss: 0.1127\n",
      "Epoch [54/100], Step [50/19], Loss: 0.4946\n",
      "Epoch [54/100], Step [60/19], Loss: 0.0874\n",
      "Epoch [54/100], Step [70/19], Loss: 0.2761\n",
      "Epoch [54/100], Step [80/19], Loss: 0.1324\n",
      "Epoch [54/100], Step [90/19], Loss: 0.0686\n",
      "Epoch [54/100], Step [100/19], Loss: 0.3383\n",
      "Epoch [54/100], Step [110/19], Loss: 0.2829\n",
      "Epoch [54/100], Step [120/19], Loss: 0.3844\n",
      "Epoch [54/100], Step [130/19], Loss: 0.0762\n",
      "Epoch [54/100], Step [140/19], Loss: 0.2796\n",
      "Epoch [54/100], Step [150/19], Loss: 0.4342\n",
      "Epoch 53; loss = 0.2359\n",
      "Epoch [55/100], Step [10/19], Loss: 0.1392\n",
      "Epoch [55/100], Step [20/19], Loss: 0.1038\n",
      "Epoch [55/100], Step [30/19], Loss: 0.6893\n",
      "Epoch [55/100], Step [40/19], Loss: 0.1106\n",
      "Epoch [55/100], Step [50/19], Loss: 0.5793\n",
      "Epoch [55/100], Step [60/19], Loss: 0.0862\n",
      "Epoch [55/100], Step [70/19], Loss: 0.2665\n",
      "Epoch [55/100], Step [80/19], Loss: 0.1333\n",
      "Epoch [55/100], Step [90/19], Loss: 0.0667\n",
      "Epoch [55/100], Step [100/19], Loss: 0.3407\n",
      "Epoch [55/100], Step [110/19], Loss: 0.2706\n",
      "Epoch [55/100], Step [120/19], Loss: 0.3806\n",
      "Epoch [55/100], Step [130/19], Loss: 0.0762\n",
      "Epoch [55/100], Step [140/19], Loss: 0.2796\n",
      "Epoch [55/100], Step [150/19], Loss: 0.4318\n",
      "Epoch 54; loss = 0.2339\n",
      "Epoch [56/100], Step [10/19], Loss: 0.1378\n",
      "Epoch [56/100], Step [20/19], Loss: 0.0679\n",
      "Epoch [56/100], Step [30/19], Loss: 0.9012\n",
      "Epoch [56/100], Step [40/19], Loss: 0.1066\n",
      "Epoch [56/100], Step [50/19], Loss: 0.5227\n",
      "Epoch [56/100], Step [60/19], Loss: 0.0887\n",
      "Epoch [56/100], Step [70/19], Loss: 0.2637\n",
      "Epoch [56/100], Step [80/19], Loss: 0.1328\n",
      "Epoch [56/100], Step [90/19], Loss: 0.0639\n",
      "Epoch [56/100], Step [100/19], Loss: 0.3394\n",
      "Epoch [56/100], Step [110/19], Loss: 0.2665\n",
      "Epoch [56/100], Step [120/19], Loss: 0.3785\n",
      "Epoch [56/100], Step [130/19], Loss: 0.0761\n",
      "Epoch [56/100], Step [140/19], Loss: 0.2786\n",
      "Epoch [56/100], Step [150/19], Loss: 0.4274\n",
      "Epoch 55; loss = 0.2331\n",
      "Epoch [57/100], Step [10/19], Loss: 0.1378\n",
      "Epoch [57/100], Step [20/19], Loss: 0.0629\n",
      "Epoch [57/100], Step [30/19], Loss: 0.9350\n",
      "Epoch [57/100], Step [40/19], Loss: 0.1098\n",
      "Epoch [57/100], Step [50/19], Loss: 0.4915\n",
      "Epoch [57/100], Step [60/19], Loss: 0.0891\n",
      "Epoch [57/100], Step [70/19], Loss: 0.2658\n",
      "Epoch [57/100], Step [80/19], Loss: 0.1311\n",
      "Epoch [57/100], Step [90/19], Loss: 0.0625\n",
      "Epoch [57/100], Step [100/19], Loss: 0.3373\n",
      "Epoch [57/100], Step [110/19], Loss: 0.2668\n",
      "Epoch [57/100], Step [120/19], Loss: 0.3784\n",
      "Epoch [57/100], Step [130/19], Loss: 0.0761\n",
      "Epoch [57/100], Step [140/19], Loss: 0.2779\n",
      "Epoch [57/100], Step [150/19], Loss: 0.4267\n",
      "Epoch 56; loss = 0.2325\n",
      "Epoch [58/100], Step [10/19], Loss: 0.1378\n",
      "Epoch [58/100], Step [20/19], Loss: 0.0698\n",
      "Epoch [58/100], Step [30/19], Loss: 0.9363\n",
      "Epoch [58/100], Step [40/19], Loss: 0.1096\n",
      "Epoch [58/100], Step [50/19], Loss: 0.4945\n",
      "Epoch [58/100], Step [60/19], Loss: 0.0891\n",
      "Epoch [58/100], Step [70/19], Loss: 0.2665\n",
      "Epoch [58/100], Step [80/19], Loss: 0.1302\n",
      "Epoch [58/100], Step [90/19], Loss: 0.0617\n",
      "Epoch [58/100], Step [100/19], Loss: 0.3363\n",
      "Epoch [58/100], Step [110/19], Loss: 0.2665\n",
      "Epoch [58/100], Step [120/19], Loss: 0.3772\n",
      "Epoch [58/100], Step [130/19], Loss: 0.0760\n",
      "Epoch [58/100], Step [140/19], Loss: 0.2769\n",
      "Epoch [58/100], Step [150/19], Loss: 0.4284\n",
      "Epoch 57; loss = 0.2323\n",
      "Epoch [59/100], Step [10/19], Loss: 0.1378\n",
      "Epoch [59/100], Step [20/19], Loss: 0.0740\n",
      "Epoch [59/100], Step [30/19], Loss: 0.9205\n",
      "Epoch [59/100], Step [40/19], Loss: 0.1095\n",
      "Epoch [59/100], Step [50/19], Loss: 0.5085\n",
      "Epoch [59/100], Step [60/19], Loss: 0.0886\n",
      "Epoch [59/100], Step [70/19], Loss: 0.2639\n",
      "Epoch [59/100], Step [80/19], Loss: 0.1295\n",
      "Epoch [59/100], Step [90/19], Loss: 0.0608\n",
      "Epoch [59/100], Step [100/19], Loss: 0.3354\n",
      "Epoch [59/100], Step [110/19], Loss: 0.2640\n",
      "Epoch [59/100], Step [120/19], Loss: 0.3759\n",
      "Epoch [59/100], Step [130/19], Loss: 0.0757\n",
      "Epoch [59/100], Step [140/19], Loss: 0.2763\n",
      "Epoch [59/100], Step [150/19], Loss: 0.4272\n",
      "Epoch 58; loss = 0.2318\n",
      "Epoch [60/100], Step [10/19], Loss: 0.1376\n",
      "Epoch [60/100], Step [20/19], Loss: 0.0723\n",
      "Epoch [60/100], Step [30/19], Loss: 0.9262\n",
      "Epoch [60/100], Step [40/19], Loss: 0.1103\n",
      "Epoch [60/100], Step [50/19], Loss: 0.5013\n",
      "Epoch [60/100], Step [60/19], Loss: 0.0885\n",
      "Epoch [60/100], Step [70/19], Loss: 0.2637\n",
      "Epoch [60/100], Step [80/19], Loss: 0.1287\n",
      "Epoch [60/100], Step [90/19], Loss: 0.0603\n",
      "Epoch [60/100], Step [100/19], Loss: 0.3341\n",
      "Epoch [60/100], Step [110/19], Loss: 0.2629\n",
      "Epoch [60/100], Step [120/19], Loss: 0.3747\n",
      "Epoch [60/100], Step [130/19], Loss: 0.0756\n",
      "Epoch [60/100], Step [140/19], Loss: 0.2755\n",
      "Epoch [60/100], Step [150/19], Loss: 0.4269\n",
      "Epoch 59; loss = 0.2313\n",
      "Epoch [61/100], Step [10/19], Loss: 0.1374\n",
      "Epoch [61/100], Step [20/19], Loss: 0.0756\n",
      "Epoch [61/100], Step [30/19], Loss: 0.9232\n",
      "Epoch [61/100], Step [40/19], Loss: 0.1102\n",
      "Epoch [61/100], Step [50/19], Loss: 0.5074\n",
      "Epoch [61/100], Step [60/19], Loss: 0.0883\n",
      "Epoch [61/100], Step [70/19], Loss: 0.2627\n",
      "Epoch [61/100], Step [80/19], Loss: 0.1283\n",
      "Epoch [61/100], Step [90/19], Loss: 0.0600\n",
      "Epoch [61/100], Step [100/19], Loss: 0.3331\n",
      "Epoch [61/100], Step [110/19], Loss: 0.2610\n",
      "Epoch [61/100], Step [120/19], Loss: 0.3729\n",
      "Epoch [61/100], Step [130/19], Loss: 0.0754\n",
      "Epoch [61/100], Step [140/19], Loss: 0.2746\n",
      "Epoch [61/100], Step [150/19], Loss: 0.4270\n",
      "Epoch 60; loss = 0.2310\n",
      "Epoch [62/100], Step [10/19], Loss: 0.1372\n",
      "Epoch [62/100], Step [20/19], Loss: 0.0768\n",
      "Epoch [62/100], Step [30/19], Loss: 0.9168\n",
      "Epoch [62/100], Step [40/19], Loss: 0.1104\n",
      "Epoch [62/100], Step [50/19], Loss: 0.5150\n",
      "Epoch [62/100], Step [60/19], Loss: 0.0881\n",
      "Epoch [62/100], Step [70/19], Loss: 0.2611\n",
      "Epoch [62/100], Step [80/19], Loss: 0.1279\n",
      "Epoch [62/100], Step [90/19], Loss: 0.0595\n",
      "Epoch [62/100], Step [100/19], Loss: 0.3321\n",
      "Epoch [62/100], Step [110/19], Loss: 0.2587\n",
      "Epoch [62/100], Step [120/19], Loss: 0.3709\n",
      "Epoch [62/100], Step [130/19], Loss: 0.0753\n",
      "Epoch [62/100], Step [140/19], Loss: 0.2738\n",
      "Epoch [62/100], Step [150/19], Loss: 0.4265\n",
      "Epoch 61; loss = 0.2305\n",
      "Epoch [63/100], Step [10/19], Loss: 0.1369\n",
      "Epoch [63/100], Step [20/19], Loss: 0.0764\n",
      "Epoch [63/100], Step [30/19], Loss: 0.9161\n",
      "Epoch [63/100], Step [40/19], Loss: 0.1109\n",
      "Epoch [63/100], Step [50/19], Loss: 0.5154\n",
      "Epoch [63/100], Step [60/19], Loss: 0.0880\n",
      "Epoch [63/100], Step [70/19], Loss: 0.2604\n",
      "Epoch [63/100], Step [80/19], Loss: 0.1276\n",
      "Epoch [63/100], Step [90/19], Loss: 0.0592\n",
      "Epoch [63/100], Step [100/19], Loss: 0.3310\n",
      "Epoch [63/100], Step [110/19], Loss: 0.2567\n",
      "Epoch [63/100], Step [120/19], Loss: 0.3682\n",
      "Epoch [63/100], Step [130/19], Loss: 0.0752\n",
      "Epoch [63/100], Step [140/19], Loss: 0.2729\n",
      "Epoch [63/100], Step [150/19], Loss: 0.4263\n",
      "Epoch 62; loss = 0.2301\n",
      "Epoch [64/100], Step [10/19], Loss: 0.1366\n",
      "Epoch [64/100], Step [20/19], Loss: 0.0776\n",
      "Epoch [64/100], Step [30/19], Loss: 0.9111\n",
      "Epoch [64/100], Step [40/19], Loss: 0.1113\n",
      "Epoch [64/100], Step [50/19], Loss: 0.5233\n",
      "Epoch [64/100], Step [60/19], Loss: 0.0878\n",
      "Epoch [64/100], Step [70/19], Loss: 0.2592\n",
      "Epoch [64/100], Step [80/19], Loss: 0.1272\n",
      "Epoch [64/100], Step [90/19], Loss: 0.0587\n",
      "Epoch [64/100], Step [100/19], Loss: 0.3300\n",
      "Epoch [64/100], Step [110/19], Loss: 0.2547\n",
      "Epoch [64/100], Step [120/19], Loss: 0.3659\n",
      "Epoch [64/100], Step [130/19], Loss: 0.0750\n",
      "Epoch [64/100], Step [140/19], Loss: 0.2721\n",
      "Epoch [64/100], Step [150/19], Loss: 0.4260\n",
      "Epoch 63; loss = 0.2297\n",
      "Epoch [65/100], Step [10/19], Loss: 0.1361\n",
      "Epoch [65/100], Step [20/19], Loss: 0.0776\n",
      "Epoch [65/100], Step [30/19], Loss: 0.9079\n",
      "Epoch [65/100], Step [40/19], Loss: 0.1114\n",
      "Epoch [65/100], Step [50/19], Loss: 0.5261\n",
      "Epoch [65/100], Step [60/19], Loss: 0.0877\n",
      "Epoch [65/100], Step [70/19], Loss: 0.2580\n",
      "Epoch [65/100], Step [80/19], Loss: 0.1267\n",
      "Epoch [65/100], Step [90/19], Loss: 0.0583\n",
      "Epoch [65/100], Step [100/19], Loss: 0.3288\n",
      "Epoch [65/100], Step [110/19], Loss: 0.2527\n",
      "Epoch [65/100], Step [120/19], Loss: 0.3645\n",
      "Epoch [65/100], Step [130/19], Loss: 0.0749\n",
      "Epoch [65/100], Step [140/19], Loss: 0.2712\n",
      "Epoch [65/100], Step [150/19], Loss: 0.4256\n",
      "Epoch 64; loss = 0.2292\n",
      "Epoch [66/100], Step [10/19], Loss: 0.1359\n",
      "Epoch [66/100], Step [20/19], Loss: 0.0765\n",
      "Epoch [66/100], Step [30/19], Loss: 0.9074\n",
      "Epoch [66/100], Step [40/19], Loss: 0.1118\n",
      "Epoch [66/100], Step [50/19], Loss: 0.5241\n",
      "Epoch [66/100], Step [60/19], Loss: 0.0877\n",
      "Epoch [66/100], Step [70/19], Loss: 0.2574\n",
      "Epoch [66/100], Step [80/19], Loss: 0.1262\n",
      "Epoch [66/100], Step [90/19], Loss: 0.0580\n",
      "Epoch [66/100], Step [100/19], Loss: 0.3275\n",
      "Epoch [66/100], Step [110/19], Loss: 0.2511\n",
      "Epoch [66/100], Step [120/19], Loss: 0.3632\n",
      "Epoch [66/100], Step [130/19], Loss: 0.0749\n",
      "Epoch [66/100], Step [140/19], Loss: 0.2702\n",
      "Epoch [66/100], Step [150/19], Loss: 0.4254\n",
      "Epoch 65; loss = 0.2288\n",
      "Epoch [67/100], Step [10/19], Loss: 0.1358\n",
      "Epoch [67/100], Step [20/19], Loss: 0.0767\n",
      "Epoch [67/100], Step [30/19], Loss: 0.9042\n",
      "Epoch [67/100], Step [40/19], Loss: 0.1120\n",
      "Epoch [67/100], Step [50/19], Loss: 0.5284\n",
      "Epoch [67/100], Step [60/19], Loss: 0.0877\n",
      "Epoch [67/100], Step [70/19], Loss: 0.2563\n",
      "Epoch [67/100], Step [80/19], Loss: 0.1257\n",
      "Epoch [67/100], Step [90/19], Loss: 0.0577\n",
      "Epoch [67/100], Step [100/19], Loss: 0.3263\n",
      "Epoch [67/100], Step [110/19], Loss: 0.2493\n",
      "Epoch [67/100], Step [120/19], Loss: 0.3617\n",
      "Epoch [67/100], Step [130/19], Loss: 0.0748\n",
      "Epoch [67/100], Step [140/19], Loss: 0.2693\n",
      "Epoch [67/100], Step [150/19], Loss: 0.4250\n",
      "Epoch 66; loss = 0.2283\n",
      "Epoch [68/100], Step [10/19], Loss: 0.1357\n",
      "Epoch [68/100], Step [20/19], Loss: 0.0753\n",
      "Epoch [68/100], Step [30/19], Loss: 0.9038\n",
      "Epoch [68/100], Step [40/19], Loss: 0.1125\n",
      "Epoch [68/100], Step [50/19], Loss: 0.5253\n",
      "Epoch [68/100], Step [60/19], Loss: 0.0878\n",
      "Epoch [68/100], Step [70/19], Loss: 0.2559\n",
      "Epoch [68/100], Step [80/19], Loss: 0.1253\n",
      "Epoch [68/100], Step [90/19], Loss: 0.0576\n",
      "Epoch [68/100], Step [100/19], Loss: 0.3247\n",
      "Epoch [68/100], Step [110/19], Loss: 0.2477\n",
      "Epoch [68/100], Step [120/19], Loss: 0.3606\n",
      "Epoch [68/100], Step [130/19], Loss: 0.0749\n",
      "Epoch [68/100], Step [140/19], Loss: 0.2684\n",
      "Epoch [68/100], Step [150/19], Loss: 0.4252\n",
      "Epoch 67; loss = 0.2279\n",
      "Epoch [69/100], Step [10/19], Loss: 0.1358\n",
      "Epoch [69/100], Step [20/19], Loss: 0.0752\n",
      "Epoch [69/100], Step [30/19], Loss: 0.8984\n",
      "Epoch [69/100], Step [40/19], Loss: 0.1127\n",
      "Epoch [69/100], Step [50/19], Loss: 0.5318\n",
      "Epoch [69/100], Step [60/19], Loss: 0.0878\n",
      "Epoch [69/100], Step [70/19], Loss: 0.2547\n",
      "Epoch [69/100], Step [80/19], Loss: 0.1249\n",
      "Epoch [69/100], Step [90/19], Loss: 0.0575\n",
      "Epoch [69/100], Step [100/19], Loss: 0.3236\n",
      "Epoch [69/100], Step [110/19], Loss: 0.2456\n",
      "Epoch [69/100], Step [120/19], Loss: 0.3593\n",
      "Epoch [69/100], Step [130/19], Loss: 0.0749\n",
      "Epoch [69/100], Step [140/19], Loss: 0.2676\n",
      "Epoch [69/100], Step [150/19], Loss: 0.4248\n",
      "Epoch 68; loss = 0.2275\n",
      "Epoch [70/100], Step [10/19], Loss: 0.1358\n",
      "Epoch [70/100], Step [20/19], Loss: 0.0740\n",
      "Epoch [70/100], Step [30/19], Loss: 0.8978\n",
      "Epoch [70/100], Step [40/19], Loss: 0.1129\n",
      "Epoch [70/100], Step [50/19], Loss: 0.5314\n",
      "Epoch [70/100], Step [60/19], Loss: 0.0879\n",
      "Epoch [70/100], Step [70/19], Loss: 0.2539\n",
      "Epoch [70/100], Step [80/19], Loss: 0.1244\n",
      "Epoch [70/100], Step [90/19], Loss: 0.0575\n",
      "Epoch [70/100], Step [100/19], Loss: 0.3225\n",
      "Epoch [70/100], Step [110/19], Loss: 0.2441\n",
      "Epoch [70/100], Step [120/19], Loss: 0.3579\n",
      "Epoch [70/100], Step [130/19], Loss: 0.0748\n",
      "Epoch [70/100], Step [140/19], Loss: 0.2669\n",
      "Epoch [70/100], Step [150/19], Loss: 0.4248\n",
      "Epoch 69; loss = 0.2271\n",
      "Epoch [71/100], Step [10/19], Loss: 0.1357\n",
      "Epoch [71/100], Step [20/19], Loss: 0.0736\n",
      "Epoch [71/100], Step [30/19], Loss: 0.8962\n",
      "Epoch [71/100], Step [40/19], Loss: 0.1130\n",
      "Epoch [71/100], Step [50/19], Loss: 0.5345\n",
      "Epoch [71/100], Step [60/19], Loss: 0.0880\n",
      "Epoch [71/100], Step [70/19], Loss: 0.2529\n",
      "Epoch [71/100], Step [80/19], Loss: 0.1240\n",
      "Epoch [71/100], Step [90/19], Loss: 0.0573\n",
      "Epoch [71/100], Step [100/19], Loss: 0.3214\n",
      "Epoch [71/100], Step [110/19], Loss: 0.2424\n",
      "Epoch [71/100], Step [120/19], Loss: 0.3563\n",
      "Epoch [71/100], Step [130/19], Loss: 0.0747\n",
      "Epoch [71/100], Step [140/19], Loss: 0.2662\n",
      "Epoch [71/100], Step [150/19], Loss: 0.4247\n",
      "Epoch 70; loss = 0.2267\n",
      "Epoch [72/100], Step [10/19], Loss: 0.1355\n",
      "Epoch [72/100], Step [20/19], Loss: 0.0726\n",
      "Epoch [72/100], Step [30/19], Loss: 0.8973\n",
      "Epoch [72/100], Step [40/19], Loss: 0.1131\n",
      "Epoch [72/100], Step [50/19], Loss: 0.5335\n",
      "Epoch [72/100], Step [60/19], Loss: 0.0880\n",
      "Epoch [72/100], Step [70/19], Loss: 0.2522\n",
      "Epoch [72/100], Step [80/19], Loss: 0.1236\n",
      "Epoch [72/100], Step [90/19], Loss: 0.0573\n",
      "Epoch [72/100], Step [100/19], Loss: 0.3203\n",
      "Epoch [72/100], Step [110/19], Loss: 0.2409\n",
      "Epoch [72/100], Step [120/19], Loss: 0.3547\n",
      "Epoch [72/100], Step [130/19], Loss: 0.0746\n",
      "Epoch [72/100], Step [140/19], Loss: 0.2655\n",
      "Epoch [72/100], Step [150/19], Loss: 0.4250\n",
      "Epoch 71; loss = 0.2262\n",
      "Epoch [73/100], Step [10/19], Loss: 0.1353\n",
      "Epoch [73/100], Step [20/19], Loss: 0.0719\n",
      "Epoch [73/100], Step [30/19], Loss: 0.8974\n",
      "Epoch [73/100], Step [40/19], Loss: 0.1130\n",
      "Epoch [73/100], Step [50/19], Loss: 0.5349\n",
      "Epoch [73/100], Step [60/19], Loss: 0.0878\n",
      "Epoch [73/100], Step [70/19], Loss: 0.2513\n",
      "Epoch [73/100], Step [80/19], Loss: 0.1232\n",
      "Epoch [73/100], Step [90/19], Loss: 0.0571\n",
      "Epoch [73/100], Step [100/19], Loss: 0.3192\n",
      "Epoch [73/100], Step [110/19], Loss: 0.2393\n",
      "Epoch [73/100], Step [120/19], Loss: 0.3531\n",
      "Epoch [73/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [73/100], Step [140/19], Loss: 0.2648\n",
      "Epoch [73/100], Step [150/19], Loss: 0.4248\n",
      "Epoch 72; loss = 0.2258\n",
      "Epoch [74/100], Step [10/19], Loss: 0.1351\n",
      "Epoch [74/100], Step [20/19], Loss: 0.0708\n",
      "Epoch [74/100], Step [30/19], Loss: 0.8988\n",
      "Epoch [74/100], Step [40/19], Loss: 0.1130\n",
      "Epoch [74/100], Step [50/19], Loss: 0.5341\n",
      "Epoch [74/100], Step [60/19], Loss: 0.0877\n",
      "Epoch [74/100], Step [70/19], Loss: 0.2507\n",
      "Epoch [74/100], Step [80/19], Loss: 0.1229\n",
      "Epoch [74/100], Step [90/19], Loss: 0.0571\n",
      "Epoch [74/100], Step [100/19], Loss: 0.3180\n",
      "Epoch [74/100], Step [110/19], Loss: 0.2378\n",
      "Epoch [74/100], Step [120/19], Loss: 0.3516\n",
      "Epoch [74/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [74/100], Step [140/19], Loss: 0.2641\n",
      "Epoch [74/100], Step [150/19], Loss: 0.4248\n",
      "Epoch 73; loss = 0.2254\n",
      "Epoch [75/100], Step [10/19], Loss: 0.1349\n",
      "Epoch [75/100], Step [20/19], Loss: 0.0700\n",
      "Epoch [75/100], Step [30/19], Loss: 0.8985\n",
      "Epoch [75/100], Step [40/19], Loss: 0.1128\n",
      "Epoch [75/100], Step [50/19], Loss: 0.5365\n",
      "Epoch [75/100], Step [60/19], Loss: 0.0876\n",
      "Epoch [75/100], Step [70/19], Loss: 0.2498\n",
      "Epoch [75/100], Step [80/19], Loss: 0.1229\n",
      "Epoch [75/100], Step [90/19], Loss: 0.0569\n",
      "Epoch [75/100], Step [100/19], Loss: 0.3170\n",
      "Epoch [75/100], Step [110/19], Loss: 0.2362\n",
      "Epoch [75/100], Step [120/19], Loss: 0.3501\n",
      "Epoch [75/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [75/100], Step [140/19], Loss: 0.2634\n",
      "Epoch [75/100], Step [150/19], Loss: 0.4245\n",
      "Epoch 74; loss = 0.2250\n",
      "Epoch [76/100], Step [10/19], Loss: 0.1346\n",
      "Epoch [76/100], Step [20/19], Loss: 0.0690\n",
      "Epoch [76/100], Step [30/19], Loss: 0.9003\n",
      "Epoch [76/100], Step [40/19], Loss: 0.1127\n",
      "Epoch [76/100], Step [50/19], Loss: 0.5360\n",
      "Epoch [76/100], Step [60/19], Loss: 0.0876\n",
      "Epoch [76/100], Step [70/19], Loss: 0.2490\n",
      "Epoch [76/100], Step [80/19], Loss: 0.1226\n",
      "Epoch [76/100], Step [90/19], Loss: 0.0568\n",
      "Epoch [76/100], Step [100/19], Loss: 0.3159\n",
      "Epoch [76/100], Step [110/19], Loss: 0.2348\n",
      "Epoch [76/100], Step [120/19], Loss: 0.3486\n",
      "Epoch [76/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [76/100], Step [140/19], Loss: 0.2626\n",
      "Epoch [76/100], Step [150/19], Loss: 0.4244\n",
      "Epoch 75; loss = 0.2246\n",
      "Epoch [77/100], Step [10/19], Loss: 0.1344\n",
      "Epoch [77/100], Step [20/19], Loss: 0.0682\n",
      "Epoch [77/100], Step [30/19], Loss: 0.9008\n",
      "Epoch [77/100], Step [40/19], Loss: 0.1125\n",
      "Epoch [77/100], Step [50/19], Loss: 0.5376\n",
      "Epoch [77/100], Step [60/19], Loss: 0.0875\n",
      "Epoch [77/100], Step [70/19], Loss: 0.2481\n",
      "Epoch [77/100], Step [80/19], Loss: 0.1224\n",
      "Epoch [77/100], Step [90/19], Loss: 0.0567\n",
      "Epoch [77/100], Step [100/19], Loss: 0.3149\n",
      "Epoch [77/100], Step [110/19], Loss: 0.2332\n",
      "Epoch [77/100], Step [120/19], Loss: 0.3471\n",
      "Epoch [77/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [77/100], Step [140/19], Loss: 0.2618\n",
      "Epoch [77/100], Step [150/19], Loss: 0.4241\n",
      "Epoch 76; loss = 0.2242\n",
      "Epoch [78/100], Step [10/19], Loss: 0.1341\n",
      "Epoch [78/100], Step [20/19], Loss: 0.0673\n",
      "Epoch [78/100], Step [30/19], Loss: 0.9027\n",
      "Epoch [78/100], Step [40/19], Loss: 0.1123\n",
      "Epoch [78/100], Step [50/19], Loss: 0.5373\n",
      "Epoch [78/100], Step [60/19], Loss: 0.0875\n",
      "Epoch [78/100], Step [70/19], Loss: 0.2473\n",
      "Epoch [78/100], Step [80/19], Loss: 0.1223\n",
      "Epoch [78/100], Step [90/19], Loss: 0.0567\n",
      "Epoch [78/100], Step [100/19], Loss: 0.3138\n",
      "Epoch [78/100], Step [110/19], Loss: 0.2318\n",
      "Epoch [78/100], Step [120/19], Loss: 0.3456\n",
      "Epoch [78/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [78/100], Step [140/19], Loss: 0.2610\n",
      "Epoch [78/100], Step [150/19], Loss: 0.4240\n",
      "Epoch 77; loss = 0.2239\n",
      "Epoch [79/100], Step [10/19], Loss: 0.1339\n",
      "Epoch [79/100], Step [20/19], Loss: 0.0664\n",
      "Epoch [79/100], Step [30/19], Loss: 0.9038\n",
      "Epoch [79/100], Step [40/19], Loss: 0.1121\n",
      "Epoch [79/100], Step [50/19], Loss: 0.5380\n",
      "Epoch [79/100], Step [60/19], Loss: 0.0874\n",
      "Epoch [79/100], Step [70/19], Loss: 0.2464\n",
      "Epoch [79/100], Step [80/19], Loss: 0.1221\n",
      "Epoch [79/100], Step [90/19], Loss: 0.0566\n",
      "Epoch [79/100], Step [100/19], Loss: 0.3128\n",
      "Epoch [79/100], Step [110/19], Loss: 0.2303\n",
      "Epoch [79/100], Step [120/19], Loss: 0.3441\n",
      "Epoch [79/100], Step [130/19], Loss: 0.0743\n",
      "Epoch [79/100], Step [140/19], Loss: 0.2603\n",
      "Epoch [79/100], Step [150/19], Loss: 0.4244\n",
      "Epoch 78; loss = 0.2235\n",
      "Epoch [80/100], Step [10/19], Loss: 0.1345\n",
      "Epoch [80/100], Step [20/19], Loss: 0.0655\n",
      "Epoch [80/100], Step [30/19], Loss: 0.9053\n",
      "Epoch [80/100], Step [40/19], Loss: 0.1119\n",
      "Epoch [80/100], Step [50/19], Loss: 0.5366\n",
      "Epoch [80/100], Step [60/19], Loss: 0.0874\n",
      "Epoch [80/100], Step [70/19], Loss: 0.2459\n",
      "Epoch [80/100], Step [80/19], Loss: 0.1220\n",
      "Epoch [80/100], Step [90/19], Loss: 0.0566\n",
      "Epoch [80/100], Step [100/19], Loss: 0.3119\n",
      "Epoch [80/100], Step [110/19], Loss: 0.2289\n",
      "Epoch [80/100], Step [120/19], Loss: 0.3428\n",
      "Epoch [80/100], Step [130/19], Loss: 0.0743\n",
      "Epoch [80/100], Step [140/19], Loss: 0.2595\n",
      "Epoch [80/100], Step [150/19], Loss: 0.4243\n",
      "Epoch 79; loss = 0.2231\n",
      "Epoch [81/100], Step [10/19], Loss: 0.1341\n",
      "Epoch [81/100], Step [20/19], Loss: 0.0649\n",
      "Epoch [81/100], Step [30/19], Loss: 0.9054\n",
      "Epoch [81/100], Step [40/19], Loss: 0.1117\n",
      "Epoch [81/100], Step [50/19], Loss: 0.5383\n",
      "Epoch [81/100], Step [60/19], Loss: 0.0874\n",
      "Epoch [81/100], Step [70/19], Loss: 0.2448\n",
      "Epoch [81/100], Step [80/19], Loss: 0.1220\n",
      "Epoch [81/100], Step [90/19], Loss: 0.0565\n",
      "Epoch [81/100], Step [100/19], Loss: 0.3111\n",
      "Epoch [81/100], Step [110/19], Loss: 0.2274\n",
      "Epoch [81/100], Step [120/19], Loss: 0.3416\n",
      "Epoch [81/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [81/100], Step [140/19], Loss: 0.2587\n",
      "Epoch [81/100], Step [150/19], Loss: 0.4238\n",
      "Epoch 80; loss = 0.2228\n",
      "Epoch [82/100], Step [10/19], Loss: 0.1336\n",
      "Epoch [82/100], Step [20/19], Loss: 0.0639\n",
      "Epoch [82/100], Step [30/19], Loss: 0.9079\n",
      "Epoch [82/100], Step [40/19], Loss: 0.1114\n",
      "Epoch [82/100], Step [50/19], Loss: 0.5383\n",
      "Epoch [82/100], Step [60/19], Loss: 0.0874\n",
      "Epoch [82/100], Step [70/19], Loss: 0.2442\n",
      "Epoch [82/100], Step [80/19], Loss: 0.1219\n",
      "Epoch [82/100], Step [90/19], Loss: 0.0565\n",
      "Epoch [82/100], Step [100/19], Loss: 0.3102\n",
      "Epoch [82/100], Step [110/19], Loss: 0.2260\n",
      "Epoch [82/100], Step [120/19], Loss: 0.3407\n",
      "Epoch [82/100], Step [130/19], Loss: 0.0744\n",
      "Epoch [82/100], Step [140/19], Loss: 0.2579\n",
      "Epoch [82/100], Step [150/19], Loss: 0.4235\n",
      "Epoch 81; loss = 0.2224\n",
      "Epoch [83/100], Step [10/19], Loss: 0.1333\n",
      "Epoch [83/100], Step [20/19], Loss: 0.0632\n",
      "Epoch [83/100], Step [30/19], Loss: 0.9093\n",
      "Epoch [83/100], Step [40/19], Loss: 0.1111\n",
      "Epoch [83/100], Step [50/19], Loss: 0.5395\n",
      "Epoch [83/100], Step [60/19], Loss: 0.0874\n",
      "Epoch [83/100], Step [70/19], Loss: 0.2435\n",
      "Epoch [83/100], Step [80/19], Loss: 0.1219\n",
      "Epoch [83/100], Step [90/19], Loss: 0.0566\n",
      "Epoch [83/100], Step [100/19], Loss: 0.3093\n",
      "Epoch [83/100], Step [110/19], Loss: 0.2246\n",
      "Epoch [83/100], Step [120/19], Loss: 0.3397\n",
      "Epoch [83/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [83/100], Step [140/19], Loss: 0.2571\n",
      "Epoch [83/100], Step [150/19], Loss: 0.4232\n",
      "Epoch 82; loss = 0.2221\n",
      "Epoch [84/100], Step [10/19], Loss: 0.1330\n",
      "Epoch [84/100], Step [20/19], Loss: 0.0626\n",
      "Epoch [84/100], Step [30/19], Loss: 0.9109\n",
      "Epoch [84/100], Step [40/19], Loss: 0.1109\n",
      "Epoch [84/100], Step [50/19], Loss: 0.5399\n",
      "Epoch [84/100], Step [60/19], Loss: 0.0875\n",
      "Epoch [84/100], Step [70/19], Loss: 0.2429\n",
      "Epoch [84/100], Step [80/19], Loss: 0.1220\n",
      "Epoch [84/100], Step [90/19], Loss: 0.0567\n",
      "Epoch [84/100], Step [100/19], Loss: 0.3084\n",
      "Epoch [84/100], Step [110/19], Loss: 0.2232\n",
      "Epoch [84/100], Step [120/19], Loss: 0.3386\n",
      "Epoch [84/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [84/100], Step [140/19], Loss: 0.2562\n",
      "Epoch [84/100], Step [150/19], Loss: 0.4229\n",
      "Epoch 83; loss = 0.2217\n",
      "Epoch [85/100], Step [10/19], Loss: 0.1326\n",
      "Epoch [85/100], Step [20/19], Loss: 0.0621\n",
      "Epoch [85/100], Step [30/19], Loss: 0.9124\n",
      "Epoch [85/100], Step [40/19], Loss: 0.1107\n",
      "Epoch [85/100], Step [50/19], Loss: 0.5403\n",
      "Epoch [85/100], Step [60/19], Loss: 0.0876\n",
      "Epoch [85/100], Step [70/19], Loss: 0.2424\n",
      "Epoch [85/100], Step [80/19], Loss: 0.1221\n",
      "Epoch [85/100], Step [90/19], Loss: 0.0568\n",
      "Epoch [85/100], Step [100/19], Loss: 0.3076\n",
      "Epoch [85/100], Step [110/19], Loss: 0.2219\n",
      "Epoch [85/100], Step [120/19], Loss: 0.3376\n",
      "Epoch [85/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [85/100], Step [140/19], Loss: 0.2551\n",
      "Epoch [85/100], Step [150/19], Loss: 0.4225\n",
      "Epoch 84; loss = 0.2214\n",
      "Epoch [86/100], Step [10/19], Loss: 0.1322\n",
      "Epoch [86/100], Step [20/19], Loss: 0.0617\n",
      "Epoch [86/100], Step [30/19], Loss: 0.9139\n",
      "Epoch [86/100], Step [40/19], Loss: 0.1106\n",
      "Epoch [86/100], Step [50/19], Loss: 0.5399\n",
      "Epoch [86/100], Step [60/19], Loss: 0.0877\n",
      "Epoch [86/100], Step [70/19], Loss: 0.2419\n",
      "Epoch [86/100], Step [80/19], Loss: 0.1221\n",
      "Epoch [86/100], Step [90/19], Loss: 0.0569\n",
      "Epoch [86/100], Step [100/19], Loss: 0.3068\n",
      "Epoch [86/100], Step [110/19], Loss: 0.2207\n",
      "Epoch [86/100], Step [120/19], Loss: 0.3364\n",
      "Epoch [86/100], Step [130/19], Loss: 0.0745\n",
      "Epoch [86/100], Step [140/19], Loss: 0.2537\n",
      "Epoch [86/100], Step [150/19], Loss: 0.4223\n",
      "Epoch 85; loss = 0.2211\n",
      "Epoch [87/100], Step [10/19], Loss: 0.1318\n",
      "Epoch [87/100], Step [20/19], Loss: 0.0615\n",
      "Epoch [87/100], Step [30/19], Loss: 0.9146\n",
      "Epoch [87/100], Step [40/19], Loss: 0.1106\n",
      "Epoch [87/100], Step [50/19], Loss: 0.5392\n",
      "Epoch [87/100], Step [60/19], Loss: 0.0878\n",
      "Epoch [87/100], Step [70/19], Loss: 0.2414\n",
      "Epoch [87/100], Step [80/19], Loss: 0.1220\n",
      "Epoch [87/100], Step [90/19], Loss: 0.0571\n",
      "Epoch [87/100], Step [100/19], Loss: 0.3060\n",
      "Epoch [87/100], Step [110/19], Loss: 0.2196\n",
      "Epoch [87/100], Step [120/19], Loss: 0.3351\n",
      "Epoch [87/100], Step [130/19], Loss: 0.0746\n",
      "Epoch [87/100], Step [140/19], Loss: 0.2527\n",
      "Epoch [87/100], Step [150/19], Loss: 0.4221\n",
      "Epoch 86; loss = 0.2207\n",
      "Epoch [88/100], Step [10/19], Loss: 0.1313\n",
      "Epoch [88/100], Step [20/19], Loss: 0.0614\n",
      "Epoch [88/100], Step [30/19], Loss: 0.9144\n",
      "Epoch [88/100], Step [40/19], Loss: 0.1106\n",
      "Epoch [88/100], Step [50/19], Loss: 0.5391\n",
      "Epoch [88/100], Step [60/19], Loss: 0.0879\n",
      "Epoch [88/100], Step [70/19], Loss: 0.2409\n",
      "Epoch [88/100], Step [80/19], Loss: 0.1219\n",
      "Epoch [88/100], Step [90/19], Loss: 0.0573\n",
      "Epoch [88/100], Step [100/19], Loss: 0.3053\n",
      "Epoch [88/100], Step [110/19], Loss: 0.2186\n",
      "Epoch [88/100], Step [120/19], Loss: 0.3335\n",
      "Epoch [88/100], Step [130/19], Loss: 0.0746\n",
      "Epoch [88/100], Step [140/19], Loss: 0.2518\n",
      "Epoch [88/100], Step [150/19], Loss: 0.4219\n",
      "Epoch 87; loss = 0.2204\n",
      "Epoch [89/100], Step [10/19], Loss: 0.1307\n",
      "Epoch [89/100], Step [20/19], Loss: 0.0615\n",
      "Epoch [89/100], Step [30/19], Loss: 0.9135\n",
      "Epoch [89/100], Step [40/19], Loss: 0.1108\n",
      "Epoch [89/100], Step [50/19], Loss: 0.5389\n",
      "Epoch [89/100], Step [60/19], Loss: 0.0880\n",
      "Epoch [89/100], Step [70/19], Loss: 0.2405\n",
      "Epoch [89/100], Step [80/19], Loss: 0.1219\n",
      "Epoch [89/100], Step [90/19], Loss: 0.0575\n",
      "Epoch [89/100], Step [100/19], Loss: 0.3046\n",
      "Epoch [89/100], Step [110/19], Loss: 0.2177\n",
      "Epoch [89/100], Step [120/19], Loss: 0.3318\n",
      "Epoch [89/100], Step [130/19], Loss: 0.0747\n",
      "Epoch [89/100], Step [140/19], Loss: 0.2510\n",
      "Epoch [89/100], Step [150/19], Loss: 0.4219\n",
      "Epoch 88; loss = 0.2200\n",
      "Epoch [90/100], Step [10/19], Loss: 0.1301\n",
      "Epoch [90/100], Step [20/19], Loss: 0.0618\n",
      "Epoch [90/100], Step [30/19], Loss: 0.9111\n",
      "Epoch [90/100], Step [40/19], Loss: 0.1110\n",
      "Epoch [90/100], Step [50/19], Loss: 0.5399\n",
      "Epoch [90/100], Step [60/19], Loss: 0.0881\n",
      "Epoch [90/100], Step [70/19], Loss: 0.2400\n",
      "Epoch [90/100], Step [80/19], Loss: 0.1220\n",
      "Epoch [90/100], Step [90/19], Loss: 0.0577\n",
      "Epoch [90/100], Step [100/19], Loss: 0.3040\n",
      "Epoch [90/100], Step [110/19], Loss: 0.2170\n",
      "Epoch [90/100], Step [120/19], Loss: 0.3298\n",
      "Epoch [90/100], Step [130/19], Loss: 0.0747\n",
      "Epoch [90/100], Step [140/19], Loss: 0.2502\n",
      "Epoch [90/100], Step [150/19], Loss: 0.4218\n",
      "Epoch 89; loss = 0.2197\n",
      "Epoch [91/100], Step [10/19], Loss: 0.1292\n",
      "Epoch [91/100], Step [20/19], Loss: 0.0621\n",
      "Epoch [91/100], Step [30/19], Loss: 0.9086\n",
      "Epoch [91/100], Step [40/19], Loss: 0.1112\n",
      "Epoch [91/100], Step [50/19], Loss: 0.5411\n",
      "Epoch [91/100], Step [60/19], Loss: 0.0883\n",
      "Epoch [91/100], Step [70/19], Loss: 0.2393\n",
      "Epoch [91/100], Step [80/19], Loss: 0.1222\n",
      "Epoch [91/100], Step [90/19], Loss: 0.0578\n",
      "Epoch [91/100], Step [100/19], Loss: 0.3033\n",
      "Epoch [91/100], Step [110/19], Loss: 0.2167\n",
      "Epoch [91/100], Step [120/19], Loss: 0.3273\n",
      "Epoch [91/100], Step [130/19], Loss: 0.0748\n",
      "Epoch [91/100], Step [140/19], Loss: 0.2493\n",
      "Epoch [91/100], Step [150/19], Loss: 0.4212\n",
      "Epoch 90; loss = 0.2193\n",
      "Epoch [92/100], Step [10/19], Loss: 0.1270\n",
      "Epoch [92/100], Step [20/19], Loss: 0.0628\n",
      "Epoch [92/100], Step [30/19], Loss: 0.9050\n",
      "Epoch [92/100], Step [40/19], Loss: 0.1105\n",
      "Epoch [92/100], Step [50/19], Loss: 0.5419\n",
      "Epoch [92/100], Step [60/19], Loss: 0.0886\n",
      "Epoch [92/100], Step [70/19], Loss: 0.2388\n",
      "Epoch [92/100], Step [80/19], Loss: 0.1226\n",
      "Epoch [92/100], Step [90/19], Loss: 0.0582\n",
      "Epoch [92/100], Step [100/19], Loss: 0.3028\n",
      "Epoch [92/100], Step [110/19], Loss: 0.2190\n",
      "Epoch [92/100], Step [120/19], Loss: 0.3233\n",
      "Epoch [92/100], Step [130/19], Loss: 0.0752\n",
      "Epoch [92/100], Step [140/19], Loss: 0.2488\n",
      "Epoch [92/100], Step [150/19], Loss: 0.4195\n",
      "Epoch 91; loss = 0.2188\n",
      "Epoch [93/100], Step [10/19], Loss: 0.1209\n",
      "Epoch [93/100], Step [20/19], Loss: 0.0645\n",
      "Epoch [93/100], Step [30/19], Loss: 0.9035\n",
      "Epoch [93/100], Step [40/19], Loss: 0.1084\n",
      "Epoch [93/100], Step [50/19], Loss: 0.5479\n",
      "Epoch [93/100], Step [60/19], Loss: 0.0890\n",
      "Epoch [93/100], Step [70/19], Loss: 0.2376\n",
      "Epoch [93/100], Step [80/19], Loss: 0.1232\n",
      "Epoch [93/100], Step [90/19], Loss: 0.0587\n",
      "Epoch [93/100], Step [100/19], Loss: 0.3016\n",
      "Epoch [93/100], Step [110/19], Loss: 0.2205\n",
      "Epoch [93/100], Step [120/19], Loss: 0.3198\n",
      "Epoch [93/100], Step [130/19], Loss: 0.0756\n",
      "Epoch [93/100], Step [140/19], Loss: 0.2483\n",
      "Epoch [93/100], Step [150/19], Loss: 0.4188\n",
      "Epoch 92; loss = 0.2182\n",
      "Epoch [94/100], Step [10/19], Loss: 0.1200\n",
      "Epoch [94/100], Step [20/19], Loss: 0.0658\n",
      "Epoch [94/100], Step [30/19], Loss: 0.9020\n",
      "Epoch [94/100], Step [40/19], Loss: 0.1091\n",
      "Epoch [94/100], Step [50/19], Loss: 0.5532\n",
      "Epoch [94/100], Step [60/19], Loss: 0.0892\n",
      "Epoch [94/100], Step [70/19], Loss: 0.2364\n",
      "Epoch [94/100], Step [80/19], Loss: 0.1241\n",
      "Epoch [94/100], Step [90/19], Loss: 0.0591\n",
      "Epoch [94/100], Step [100/19], Loss: 0.3000\n",
      "Epoch [94/100], Step [110/19], Loss: 0.2191\n",
      "Epoch [94/100], Step [120/19], Loss: 0.3169\n",
      "Epoch [94/100], Step [130/19], Loss: 0.0757\n",
      "Epoch [94/100], Step [140/19], Loss: 0.2466\n",
      "Epoch [94/100], Step [150/19], Loss: 0.4188\n",
      "Epoch 93; loss = 0.2179\n",
      "Epoch [95/100], Step [10/19], Loss: 0.1202\n",
      "Epoch [95/100], Step [20/19], Loss: 0.0663\n",
      "Epoch [95/100], Step [30/19], Loss: 0.8999\n",
      "Epoch [95/100], Step [40/19], Loss: 0.1098\n",
      "Epoch [95/100], Step [50/19], Loss: 0.5542\n",
      "Epoch [95/100], Step [60/19], Loss: 0.0893\n",
      "Epoch [95/100], Step [70/19], Loss: 0.2360\n",
      "Epoch [95/100], Step [80/19], Loss: 0.1248\n",
      "Epoch [95/100], Step [90/19], Loss: 0.0593\n",
      "Epoch [95/100], Step [100/19], Loss: 0.2987\n",
      "Epoch [95/100], Step [110/19], Loss: 0.2183\n",
      "Epoch [95/100], Step [120/19], Loss: 0.3146\n",
      "Epoch [95/100], Step [130/19], Loss: 0.0757\n",
      "Epoch [95/100], Step [140/19], Loss: 0.2454\n",
      "Epoch [95/100], Step [150/19], Loss: 0.4190\n",
      "Epoch 94; loss = 0.2176\n",
      "Epoch [96/100], Step [10/19], Loss: 0.1203\n",
      "Epoch [96/100], Step [20/19], Loss: 0.0668\n",
      "Epoch [96/100], Step [30/19], Loss: 0.8952\n",
      "Epoch [96/100], Step [40/19], Loss: 0.1104\n",
      "Epoch [96/100], Step [50/19], Loss: 0.5575\n",
      "Epoch [96/100], Step [60/19], Loss: 0.0893\n",
      "Epoch [96/100], Step [70/19], Loss: 0.2354\n",
      "Epoch [96/100], Step [80/19], Loss: 0.1253\n",
      "Epoch [96/100], Step [90/19], Loss: 0.0593\n",
      "Epoch [96/100], Step [100/19], Loss: 0.2978\n",
      "Epoch [96/100], Step [110/19], Loss: 0.2172\n",
      "Epoch [96/100], Step [120/19], Loss: 0.3125\n",
      "Epoch [96/100], Step [130/19], Loss: 0.0759\n",
      "Epoch [96/100], Step [140/19], Loss: 0.2445\n",
      "Epoch [96/100], Step [150/19], Loss: 0.4190\n",
      "Epoch 95; loss = 0.2173\n",
      "Epoch [97/100], Step [10/19], Loss: 0.1202\n",
      "Epoch [97/100], Step [20/19], Loss: 0.0670\n",
      "Epoch [97/100], Step [30/19], Loss: 0.8925\n",
      "Epoch [97/100], Step [40/19], Loss: 0.1108\n",
      "Epoch [97/100], Step [50/19], Loss: 0.5590\n",
      "Epoch [97/100], Step [60/19], Loss: 0.0893\n",
      "Epoch [97/100], Step [70/19], Loss: 0.2350\n",
      "Epoch [97/100], Step [80/19], Loss: 0.1255\n",
      "Epoch [97/100], Step [90/19], Loss: 0.0593\n",
      "Epoch [97/100], Step [100/19], Loss: 0.2971\n",
      "Epoch [97/100], Step [110/19], Loss: 0.2160\n",
      "Epoch [97/100], Step [120/19], Loss: 0.3107\n",
      "Epoch [97/100], Step [130/19], Loss: 0.0762\n",
      "Epoch [97/100], Step [140/19], Loss: 0.2440\n",
      "Epoch [97/100], Step [150/19], Loss: 0.4194\n",
      "Epoch 96; loss = 0.2170\n",
      "Epoch [98/100], Step [10/19], Loss: 0.1199\n",
      "Epoch [98/100], Step [20/19], Loss: 0.0667\n",
      "Epoch [98/100], Step [30/19], Loss: 0.8900\n",
      "Epoch [98/100], Step [40/19], Loss: 0.1111\n",
      "Epoch [98/100], Step [50/19], Loss: 0.5594\n",
      "Epoch [98/100], Step [60/19], Loss: 0.0893\n",
      "Epoch [98/100], Step [70/19], Loss: 0.2346\n",
      "Epoch [98/100], Step [80/19], Loss: 0.1256\n",
      "Epoch [98/100], Step [90/19], Loss: 0.0593\n",
      "Epoch [98/100], Step [100/19], Loss: 0.2968\n",
      "Epoch [98/100], Step [110/19], Loss: 0.2144\n",
      "Epoch [98/100], Step [120/19], Loss: 0.3093\n",
      "Epoch [98/100], Step [130/19], Loss: 0.0768\n",
      "Epoch [98/100], Step [140/19], Loss: 0.2435\n",
      "Epoch [98/100], Step [150/19], Loss: 0.4201\n",
      "Epoch 97; loss = 0.2167\n",
      "Epoch [99/100], Step [10/19], Loss: 0.1193\n",
      "Epoch [99/100], Step [20/19], Loss: 0.0653\n",
      "Epoch [99/100], Step [30/19], Loss: 0.8887\n",
      "Epoch [99/100], Step [40/19], Loss: 0.1112\n",
      "Epoch [99/100], Step [50/19], Loss: 0.5586\n",
      "Epoch [99/100], Step [60/19], Loss: 0.0891\n",
      "Epoch [99/100], Step [70/19], Loss: 0.2387\n",
      "Epoch [99/100], Step [80/19], Loss: 0.1275\n",
      "Epoch [99/100], Step [90/19], Loss: 0.0594\n",
      "Epoch [99/100], Step [100/19], Loss: 0.2964\n",
      "Epoch [99/100], Step [110/19], Loss: 0.2135\n",
      "Epoch [99/100], Step [120/19], Loss: 0.3087\n",
      "Epoch [99/100], Step [130/19], Loss: 0.0771\n",
      "Epoch [99/100], Step [140/19], Loss: 0.2381\n",
      "Epoch [99/100], Step [150/19], Loss: 0.4745\n",
      "Epoch 98; loss = 0.2187\n",
      "Epoch [100/100], Step [10/19], Loss: 0.1379\n",
      "Epoch [100/100], Step [20/19], Loss: 0.0707\n",
      "Epoch [100/100], Step [30/19], Loss: 0.6691\n",
      "Epoch [100/100], Step [40/19], Loss: 0.1456\n",
      "Epoch [100/100], Step [50/19], Loss: 0.5698\n",
      "Epoch [100/100], Step [60/19], Loss: 0.0841\n",
      "Epoch [100/100], Step [70/19], Loss: 0.2426\n",
      "Epoch [100/100], Step [80/19], Loss: 0.1229\n",
      "Epoch [100/100], Step [90/19], Loss: 0.0850\n",
      "Epoch [100/100], Step [100/19], Loss: 0.3131\n",
      "Epoch [100/100], Step [110/19], Loss: 0.2320\n",
      "Epoch [100/100], Step [120/19], Loss: 0.3585\n",
      "Epoch [100/100], Step [130/19], Loss: 0.0737\n",
      "Epoch [100/100], Step [140/19], Loss: 0.2422\n",
      "Epoch [100/100], Step [150/19], Loss: 0.4469\n",
      "Epoch 99; loss = 0.2176\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "    iteration_count = 0.\n",
    "    for i, sample in enumerate(body_train):\n",
    "        iteration_count += 1.\n",
    "        labels, marks = sample['label'], sample['marks'] \n",
    "        marks = Variable(marks.view(-1, sequence_length, input_size))\n",
    "        labels = Variable(labels)\n",
    "        if cuda_enabled:\n",
    "            marks = marks.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = body_model(marks)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(body_train) // batch_size, loss.item()))\n",
    "    current_epoch_loss = loss_total / iteration_count\n",
    "    print('Epoch %d; loss = %0.4f' % (epoch, current_epoch_loss))\n",
    "    epoch_loss = current_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a91d93a-8bce-41ab-b01d-54099a49c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(body_model.state_dict(), 'ShouldIDrive_body_tracking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de13f12b-0345-4ca6-a7cd-1c06095d883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -----------------------------------------------\n",
      "tensor([[ 6.5810e-01,  2.5724e-19,  7.5718e-01,  ...,  8.8321e-33,\n",
      "         -7.6159e-01, -0.0000e+00],\n",
      "        [ 4.1381e-01,  3.4860e-01,  5.5811e-01,  ...,  3.3068e-02,\n",
      "          3.0821e-02,  3.0357e-02],\n",
      "        [ 4.0178e-01,  2.4045e-01,  5.7574e-01,  ...,  3.3068e-02,\n",
      "          3.0821e-02,  3.0357e-02],\n",
      "        ...,\n",
      "        [-9.2253e-01,  7.7886e-11,  7.3770e-01,  ...,  2.3976e-18,\n",
      "         -7.6159e-01, -1.6690e-27],\n",
      "        [ 7.7757e-01,  1.1184e-06,  7.6156e-01,  ...,  2.7621e-18,\n",
      "         -7.6154e-01, -1.7176e-22],\n",
      "        [ 1.5625e-01,  3.8614e-01,  5.9884e-01,  ...,  3.3068e-02,\n",
      "          3.0821e-02,  3.0357e-02]])\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([0.9709, 0.7354, 0.7484, 0.7592, 0.7351, 0.5413, 0.5981, 0.8865, 0.8366,\n",
      "        0.9700, 0.9761, 0.9739, 0.7852, 0.9507, 0.9967, 0.9978, 0.9818, 0.8284,\n",
      "        0.9997, 0.7183, 0.9652, 0.8532, 0.7657, 0.7153, 0.8473, 0.7055, 0.9351,\n",
      "        0.8504, 0.9771, 0.6693, 0.9664, 0.6720, 0.6827, 0.6767, 0.7168, 0.8424,\n",
      "        1.0000, 0.8807, 0.9941, 0.9995, 0.7000, 0.8158, 0.9363, 0.7567, 0.8857,\n",
      "        0.9754, 0.9716, 0.9805, 0.6673, 0.6827, 0.6827, 0.6827, 0.6827, 0.6827,\n",
      "        0.9984, 0.7196, 0.4605, 0.9301, 0.9987, 0.8724, 0.8357, 0.8942, 0.9181,\n",
      "        0.9530, 0.9701, 0.9676, 0.6295, 0.9717, 0.9477, 0.9980, 0.9996, 0.8855,\n",
      "        0.8693, 0.5816, 0.9998, 0.7883, 0.8035, 0.8512, 0.8900, 0.9386, 0.9655,\n",
      "        0.9769, 0.9758, 0.9707, 0.6633, 0.6827, 0.6827, 0.6827, 0.6827, 0.6827,\n",
      "        0.9983, 0.9898, 0.9983, 1.0000, 1.0000, 1.0000, 1.0000, 0.9334, 0.9283,\n",
      "        0.9512, 0.9672, 0.9824, 0.9848, 0.9803, 0.9982, 0.9709, 0.6452, 0.6827,\n",
      "        0.9204, 0.8958, 0.8530, 1.0000, 1.0000, 1.0000, 1.0000, 0.9996, 0.7887,\n",
      "        0.8400, 0.9085, 0.9688, 0.9850, 0.9655, 1.0000, 1.0000, 1.0000, 0.8737,\n",
      "        0.9860, 1.0000, 1.0000, 0.9999, 0.8371, 0.9999, 1.0000, 0.9241, 0.9510,\n",
      "        0.9796, 0.9809, 0.9849, 0.9795, 0.9735, 0.9943, 0.9862, 0.9852, 0.7179])\n",
      "tensor([42, 38, 55, 46,  2, 55, 44, 28,  6, 33, 33, 33, 33, 30, 19, 42, 47, 38,\n",
      "        12, 38, 19, 30, 33,  2, 46, 10, 33,  6,  6, 44, 30, 44, 44, 44, 38, 30,\n",
      "        42, 36, 34, 12, 38, 46, 36, 28, 28, 33, 33, 33, 44, 44, 44, 44, 44, 44,\n",
      "        42, 38, 55, 46, 60, 36, 36, 46, 33, 33, 33, 33, 61, 33, 42, 42, 42, 38,\n",
      "        30, 32, 12, 46, 46, 46, 46, 28, 28, 33, 33, 33, 44, 44, 44, 44, 44, 44,\n",
      "        47, 34, 34, 12, 36, 40, 34, 36,  5, 42,  6, 33, 33, 33, 42, 33, 44, 44,\n",
      "        30, 30, 38, 12, 60, 60, 12, 12, 46, 46,  6,  6, 33, 33, 12, 60, 12, 30,\n",
      "        47, 12, 12, 12,  5, 46, 46, 46, 42,  6, 33, 33, 33, 33, 19, 47, 47, 46])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/04/0h78flqs32ld0r0_bzdd_3dc0000gn/T/ipykernel_30093/2431780601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mbody_predicted_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbody_label_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# # Test the Model\n",
    "body_model.is_training = False\n",
    "timing = dict()\n",
    "timing['testing'] = datetime.datetime.now()\n",
    "print('Testing -----------------------------------------------')\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "body_predicted_list = []\n",
    "body_label_list = []\n",
    "for i, sample in enumerate(body_test_loader):#test_loader\n",
    "    labels, marks = sample['label'], sample['marks'] \n",
    "    marks = Variable(marks.view(-1, sequence_length, input_size))\n",
    "    if cuda_enabled:\n",
    "        marks = marks.cuda()\n",
    "\n",
    "    outputs = body_model(marks)\n",
    "    # print(outputs.data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(labels)\n",
    "    print(_)\n",
    "    \n",
    "    \n",
    "    print(predicted)\n",
    "    # print(marks)\n",
    "    # print(labels)\n",
    "    total += labels.size(0)\n",
    "    # correct += (predicted == labels).sum().item()\n",
    "    for p, l in zip(predicted, labels):\n",
    "        # print(p)\n",
    "        # print(l)\n",
    "        body_predicted_list.append(p)\n",
    "        body_label_list.append(l)\n",
    "        if p == l:\n",
    "            correct += 1.0\n",
    "\n",
    "timing['testing'] = datetime.datetime.now() - timing['testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b91a3-2ba8-4783-9f3e-19e31cfd5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print('================')\n",
    "# print(train.get_encoder().classes_)\n",
    "print(confusion_matrix(body_label_list, body_predicted_list))\n",
    "print('=============================================')\n",
    "print('Accuracy = %0.4f' % (accuracy_score(body_label_list, body_predicted_list)))\n",
    "print('=============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2dc42-66f1-46a8-9bfc-bcbf9d0cf0d5",
   "metadata": {},
   "source": [
    "#### Body Tracking LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3069b5c0-a30a-4f6e-8ca9-7db62decad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init NN\n",
    "input_size = 2\n",
    "sequence_length = 181\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_classes = 2  # TODO: Determine this from the data\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 300\n",
    "\n",
    "# The network\n",
    "model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "model.is_training = True\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_loss = 5000000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7891e5d5-b289-4c93-baee-ed4479aca66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1])\n",
      "tensor([[806., 313.,  -1.,  -1., 500., 626.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1., 723., 281.,  -1.,  -1., 640., 344.,  -1.,  -1.],\n",
      "        [806., 328.,  -1.,  -1., 500., 626.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1., 723., 281.,  -1.,  -1., 640., 344.,  -1.,  -1.],\n",
      "        [806., 313.,  -1.,  -1., 500., 626.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1., 723., 281., 834., 281., 640., 344.,  -1.,  -1.],\n",
      "        [806., 313.,  -1.,  -1., 500., 626.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1., 723., 281., 834., 281., 640., 344.,  -1.,  -1.],\n",
      "        [806., 313.,  -1.,  -1., 500., 626.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
      "          -1.,  -1.,  -1.,  -1., 723., 281., 834., 297., 640., 344.,  -1.,  -1.]])\n"
     ]
    }
   ],
   "source": [
    "for i, (sample) in enumerate(body_train_loader):\n",
    "    labels, marks = sample['label'], sample['marks']\n",
    "    print(labels[0])\n",
    "    print(marks[0][:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "35ff8616-4725-4578-afa6-864a6288c770",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 121, 2]' is invalid for input of size 8000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/04/0h78flqs32ld0r0_bzdd_3dc0000gn/T/ipykernel_27701/3967476375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0miteration_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'marks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 121, 2]' is invalid for input of size 8000"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "    iteration_count = 0.\n",
    "    for i, sample in enumerate(eye_train_loader):\n",
    "        iteration_count += 1.\n",
    "        labels, marks = sample['label'], sample['marks'] \n",
    "        marks = marks.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        if cuda_enabled:\n",
    "            marks = marks.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(marks)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss_total += loss.data[0]\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if (i+1) % 10 == 0:\n",
    "#             print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    % (epoch + 1, num_epochs, i + 1, len(train) // batch_size, loss.data[0]))\n",
    "#                    # .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "#     current_epoch_loss = loss_total / iteration_count\n",
    "#     print('Epoch %d; loss = %0.4f' % (epoch, current_epoch_loss))\n",
    "\n",
    "#     epoch_loss = current_epoch_loss\n",
    "\n",
    "# timing['training'] = datetime.datetime.now() - timing['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eaa5fd97-296b-453c-a290-116a19089c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(model.state_dict(), 'ShouldIDrive_body_tracking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8dfeccf8-7ae7-4543-89a1-95e50bae98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze NN\n",
    "print('Timing (feature extraction, training, timing)')\n",
    "print('=============================================')\n",
    "print(timing['features'])\n",
    "print(timing['training'])\n",
    "print(timing['testing'])\n",
    "print('')\n",
    "print('=============================================')\n",
    "print('')\n",
    "print('Confusion Matrix')\n",
    "print('================')\n",
    "# print(train.get_encoder().classes_)\n",
    "print(confusion_matrix(body_label_list, body_predicted_list))\n",
    "print('=============================================')\n",
    "print('Accuracy = %0.4f' % (accuracy_score(body_label_list, body_predicted_list)))\n",
    "print('=============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2855760-20e5-44d9-ba11-76ac1bda51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Train NN\n",
    "# for epoch in range(num_epochs):\n",
    "#     loss_total = 0.\n",
    "#     iteration_count = 0.\n",
    "#     for i, (mfcc, labels) in enumerate(train_loader):\n",
    "#         iteration_count += 1.\n",
    "#         mfcc = Variable(mfcc.view(-1, sequence_length, input_size))\n",
    "#         labels = Variable(labels)\n",
    "#         if cuda_enabled:\n",
    "#             mfcc = mfcc.cuda()\n",
    "#             labels = labels.cuda()\n",
    "\n",
    "#         # Forward + Backward + Optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(mfcc)\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss_total += loss.data[0]\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (i + 1) % 10 == 0:\n",
    "#             print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "#                   % (epoch + 1, num_epochs, i + 1, len(train) // batch_size, loss.data[0]))\n",
    "#     current_epoch_loss = loss_total / iteration_count\n",
    "#     print('Epoch %d; loss = %0.4f' % (epoch, current_epoch_loss))\n",
    "\n",
    "#     epoch_loss = current_epoch_loss\n",
    "\n",
    "#     timing['training'] = datetime.datetime.now() - timing['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef17084-4f99-4756-a8d3-3692c37270c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.39 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# with torch.no_grad():\n",
    "#     model.is_training = False\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc5a72-ecc5-4638-bff7-ea9a90a4efc5",
   "metadata": {},
   "source": [
    "Transforms\n",
    "----------\n",
    "\n",
    "One issue we can see from the above is that the samples are not of the\n",
    "same size. Most neural networks expect the images of a fixed size.\n",
    "Therefore, we will need to write some preprocessing code.\n",
    "Let's create three transforms:\n",
    "\n",
    "-  ``Rescale``: to scale the image\n",
    "-  ``RandomCrop``: to crop from image randomly. This is data\n",
    "   augmentation.\n",
    "-  ``ToTensor``: to convert the numpy images to torch images (we need to\n",
    "   swap axes).\n",
    "\n",
    "We will write them as callable classes instead of simple functions so\n",
    "that parameters of the transform need not be passed everytime it's\n",
    "called. For this, we just need to implement ``__call__`` method and\n",
    "if required, ``__init__`` method. We can then use a transform like this:\n",
    "\n",
    "::\n",
    "\n",
    "    tsfm = Transform(params)\n",
    "    transformed_sample = tsfm(sample)\n",
    "\n",
    "Observe below how these transforms had to be applied both on the image and\n",
    "landmarks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f73b00c-de4a-4cf5-91fb-bb0423bd6117",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        landmarks = landmarks - [left, top]\n",
    "\n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image).float(),\n",
    "                'landmarks': torch.from_numpy(landmarks).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "955e53d3-d41e-4bba-b2d6-7bf233bc81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display image and label.\n",
    "# sample = next(iter(train_loader))\n",
    "# images, labels = sample['image'], sample['landmarks']\n",
    "# # train_labels\n",
    "# print(f\"Images batch shape: {images.size()}\")\n",
    "# # print(images[0].shape)\n",
    "# print(f\"Labels batch shape: {labels.size()}\")\n",
    "# img = images[0].squeeze()\n",
    "# # print(images)\n",
    "# print(images.squeeze().shape)\n",
    "# label = labels[0]\n",
    "# plt.imshow(img[0], cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b7ae3ae8-935e-40a1-ad1a-db73f6ef8a73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to show a batch\n",
    "# def show_landmarks_batch(sample_batched):\n",
    "#     \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "#     images_batch, landmarks_batch = \\\n",
    "#             sample_batched['image'], sample_batched['landmarks']\n",
    "#     batch_size = len(images_batch)\n",
    "#     im_size = images_batch.size(2)\n",
    "#     grid_border_size = 2\n",
    "\n",
    "#     grid = utils.make_grid(images_batch)\n",
    "#     plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "#     for i in range(batch_size):\n",
    "#         plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,\n",
    "#                     landmarks_batch[i, :, 1].numpy() + grid_border_size,\n",
    "#                     s=10, marker='.', c='r')\n",
    "\n",
    "#         plt.title('Batch from dataloader')\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# for i_batch, sample_batched in enumerate(train_loader):\n",
    "#     print(i_batch, sample_batched['image'].size(),\n",
    "#           sample_batched['landmarks'].size())\n",
    "\n",
    "#     # observe 4th batch and stop.\n",
    "#     if i_batch == 3:\n",
    "#         plt.figure()\n",
    "#         show_landmarks_batch(sample_batched)\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

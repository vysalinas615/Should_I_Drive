{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ce5d3d-606a-48de-af9d-59088562877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "cuda_enabled = torch.cuda.is_available()\n",
    "#cuda_enabled = False\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceddd01-584a-4a2c-9ee5-af2da3cdbd3a",
   "metadata": {},
   "source": [
    "#### Body and Eye Tracking LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143e4ce6-78fe-4e61-a5e8-a63ce659cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sid_body_tracking_data(Dataset):\n",
    "\n",
    "    # okay, just a comment\n",
    "    def __init__(self, path, padlen, featlen=136, fold_list_number=0):\n",
    "        self.instance_list = []\n",
    "        self.instance_label = []\n",
    "        self.fold_list_number = fold_list_number\n",
    "        self.fold_list = []\n",
    "        self.pad_len = padlen\n",
    "        self.feat_len = featlen\n",
    "        self.__load_fold_list__(path, fold_list_number)\n",
    "        self.__read_data__('/data/lwang89/Documents/smile/spontaneous/results/')\n",
    "        self.__read_data__('/data/lwang89/Documents/smile/deliberate/results/')\n",
    "        \n",
    "    def __load_fold_list__(self, path, fold_list_number):\n",
    "        list_path = '/data/lwang89/Documents/smile/experimental_protocols/fold_all'\n",
    "        #         list_path = list_directory_name\n",
    "        allFiles = glob.glob(list_path + \"/*.txt\")\n",
    "        print(len(list_path))\n",
    "        whole_fold_list = []\n",
    "        train_fold_list = []\n",
    "        test_fold_list = []\n",
    "        for file_ in allFiles:\n",
    "            with open(file_) as f:\n",
    "                content = f.readlines()\n",
    "            # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "            content = [x.strip() for x in content]\n",
    "            whole_fold_list.append(content)\n",
    "\n",
    "        test_fold_list = whole_fold_list[fold_list_number]\n",
    "        if path == 'train':\n",
    "            for x in whole_fold_list:\n",
    "                if x != test_fold_list:\n",
    "                    train_fold_list.extend(x)\n",
    "            self.fold_list = train_fold_list\n",
    "        else:\n",
    "            self.fold_list = test_fold_list\n",
    "\n",
    "    def __read_data__(self, directory_name):\n",
    "        import glob\n",
    "\n",
    "        allFiles = glob.glob(directory_name + \"/*.csv\")\n",
    "        training_df = pd.DataFrame()\n",
    "        list_ = []\n",
    "\n",
    "        # loop files in folder, we need to check if file name is in the fold list\n",
    "        for file_ in allFiles:\n",
    "            if 'deliberate' in file_:\n",
    "                label = 0\n",
    "                file_name = file_[len(directory_name) :len(directory_name)+22]\n",
    "            else:\n",
    "                label = 1\n",
    "                file_name = file_[len(directory_name):len(directory_name)+23]\n",
    "\n",
    "            print(\"file name is %s, label is %d.\" %(file_name, label))\n",
    "            if file_name in self.fold_list:\n",
    "                self.instance_label.append(label)\n",
    "                df = pd.read_csv(file_)\n",
    "                df.iloc[:,0] = label\n",
    "                df_matrix = df.iloc[:,1:137].values\n",
    "                self.instance_list.append(df_matrix)\n",
    "\n",
    "        self.instance_list = self.__pad_data__(self.instance_list)\n",
    "                                                                                                                                                                                                                                     \n",
    "    def __pad_data__(self, series):\n",
    "        \n",
    "        padded = []\n",
    "        for i in range(len(series)):\n",
    "            row = np.zeros((self.feat_len, self.pad_len), dtype=np.float32)\n",
    "            for j in range(self.feat_len):\n",
    "                for k in range(len(series[i])):\n",
    "                    row[j][k] = series[i][k][j]\n",
    "            padded.append(row)\n",
    "            \n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.instance_list[index], self.instance_label[index]\n",
    "    def __len__(self):\n",
    "        return len(self.instance_list)\n",
    "    def __get_instance_label__(self):\n",
    "        return self.instance_label\n",
    "    def __get_instance_list__(self):\n",
    "        return np.array(self.instance_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5994c49f-8997-44c5-9cdf-d4671c24f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sid_eye_tracking_data(Dataset):\n",
    "\n",
    "    # okay, just a comment\n",
    "    def __init__(self, path, padlen, featlen=136, fold_list_number=0):\n",
    "        self.instance_list = []\n",
    "        self.instance_label = []\n",
    "        self.fold_list_number = fold_list_number\n",
    "        self.fold_list = []\n",
    "        self.pad_len = padlen\n",
    "        self.feat_len = featlen\n",
    "        self.__load_fold_list__(path, fold_list_number)\n",
    "        self.__read_data__('/data/lwang89/Documents/smile/spontaneous/results/')\n",
    "        self.__read_data__('/data/lwang89/Documents/smile/deliberate/results/')\n",
    "        \n",
    "    def __load_fold_list__(self, path, fold_list_number):\n",
    "        list_path = '/data/lwang89/Documents/smile/experimental_protocols/fold_all'\n",
    "        #         list_path = list_directory_name\n",
    "        allFiles = glob.glob(list_path + \"/*.txt\")\n",
    "        print(len(list_path))\n",
    "        whole_fold_list = []\n",
    "        train_fold_list = []\n",
    "        test_fold_list = []\n",
    "        for file_ in allFiles:\n",
    "            with open(file_) as f:\n",
    "                content = f.readlines()\n",
    "            # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "            content = [x.strip() for x in content]\n",
    "            whole_fold_list.append(content)\n",
    "\n",
    "        test_fold_list = whole_fold_list[fold_list_number]\n",
    "        if path == 'train':\n",
    "            for x in whole_fold_list:\n",
    "                if x != test_fold_list:\n",
    "                    train_fold_list.extend(x)\n",
    "            self.fold_list = train_fold_list\n",
    "        else:\n",
    "            self.fold_list = test_fold_list\n",
    "\n",
    "    def __read_data__(self, directory_name):\n",
    "        import glob\n",
    "\n",
    "        allFiles = glob.glob(directory_name + \"/*.csv\")\n",
    "        training_df = pd.DataFrame()\n",
    "        list_ = []\n",
    "\n",
    "        # loop files in folder, we need to check if file name is in the fold list\n",
    "        for file_ in allFiles:\n",
    "            if 'deliberate' in file_:\n",
    "                label = 0\n",
    "                file_name = file_[len(directory_name) :len(directory_name)+22]\n",
    "            else:\n",
    "                label = 1\n",
    "                file_name = file_[len(directory_name):len(directory_name)+23]\n",
    "\n",
    "            print(\"file name is %s, label is %d.\" %(file_name, label))\n",
    "            if file_name in self.fold_list:\n",
    "                self.instance_label.append(label)\n",
    "                df = pd.read_csv(file_)\n",
    "                df.iloc[:,0] = label\n",
    "                df_matrix = df.iloc[:,1:137].values\n",
    "                self.instance_list.append(df_matrix)\n",
    "\n",
    "        self.instance_list = self.__pad_data__(self.instance_list)\n",
    "                                                                                                                                                                                                                                     \n",
    "    def __pad_data__(self, series):\n",
    "        \n",
    "        padded = []\n",
    "        for i in range(len(series)):\n",
    "            row = np.zeros((self.feat_len, self.pad_len), dtype=np.float32)\n",
    "            for j in range(self.feat_len):\n",
    "                for k in range(len(series[i])):\n",
    "                    row[j][k] = series[i][k][j]\n",
    "            padded.append(row)\n",
    "            \n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.instance_list[index], self.instance_label[index]\n",
    "    def __len__(self):\n",
    "        return len(self.instance_list)\n",
    "    def __get_instance_label__(self):\n",
    "        return self.instance_label\n",
    "    def __get_instance_list__(self):\n",
    "        return np.array(self.instance_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d69548d-ef87-4852-928f-8680b6a2c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py\n",
    "class BiRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.is_training = False\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.linear = nn.Linear(self.hidden_size*2, self.num_classes)\n",
    "\n",
    "        #self.fc = nn.Dropout(p=0.75, inplace=False)\n",
    "        if cuda_enabled:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.fc = self.fc.cuda()\n",
    "            self.linear = self.linear.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)) # 2 for bidirection\n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size))\n",
    "        if cuda_enabled:\n",
    "            h0 = h0.cuda()  # 2 for bidirection\n",
    "            c0 = c0.cuda()\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        if self.is_training:\n",
    "            out = self.fc(out[:, -1, :])\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "\n",
    "        out = F.log_softmax(self.linear(out), dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfeccf8-7ae7-4543-89a1-95e50bae98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_body_tracking_biRNN():\n",
    "    import datetime\n",
    "    import sys\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "    # Keep timing -- for future reporting\n",
    "    timing = dict()\n",
    "\n",
    "    # Data description\n",
    "    input_size = 5000  # 2939, 20 # TODO: Determine this from the data\n",
    "    test_input_size = 5000\n",
    "    sequence_length = 136  # TODO: Determine this from the data\n",
    "    # Data\n",
    "    label = 'SvP'  \n",
    "\n",
    "    timing['start'] = datetime.datetime.now()\n",
    "    i = 1\n",
    "    test = sid_body_tracking_data('train', padlen=input_size, featlen=sequence_length, fold_list_number=i)\n",
    "\n",
    "    train = sid_body_tracking_data('test',padlen=test_input_size, featlen=sequence_length, fold_list_number=i)\n",
    "    print('We are testing the %d th fold.' %(i))\n",
    "    print('ShouldIDrive test data items: {}'.format(test.__len__()))\n",
    "    print('ShouldIDrive train data items: {}'.format(train.__len__()))\n",
    "\n",
    "    timing['features'] = datetime.datetime.now() - timing['start']\n",
    "    print('Extracted features from speech files.')\n",
    "\n",
    "    # Some hyperparams, etc. for the network\n",
    "    batch_size = 64\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    if not cuda_enabled:\n",
    "        kwargs['pin_memory'] = False\n",
    "        batch_size = 32\n",
    "\n",
    "    print('Starting loader -------------------------------------')\n",
    "    timing['training'] = datetime.datetime.now()\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    # sys.exit()\n",
    "\n",
    "    # The net... and training it\n",
    "    hidden_size = 128\n",
    "    num_layers = 2\n",
    "    num_classes = 2  # TODO: Determine this from the data\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 300\n",
    "\n",
    "    # The network\n",
    "    rnn = BiRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "    rnn.is_training = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_loss = 5000000000.\n",
    "    # Train it\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_total = 0.\n",
    "        iteration_count = 0.\n",
    "        for i, (mfcc, labels) in enumerate(train_loader):\n",
    "            iteration_count += 1.\n",
    "            mfcc = Variable(mfcc.view(-1, sequence_length, input_size))\n",
    "            labels = Variable(labels)\n",
    "            if cuda_enabled:\n",
    "                mfcc = mfcc.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = rnn(mfcc)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_total += loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                      % (epoch + 1, num_epochs, i + 1, len(train) // batch_size, loss.data[0]))\n",
    "        current_epoch_loss = loss_total / iteration_count\n",
    "        print('Epoch %d; loss = %0.4f' % (epoch, current_epoch_loss))\n",
    "\n",
    "        epoch_loss = current_epoch_loss\n",
    "\n",
    "    timing['training'] = datetime.datetime.now() - timing['training']\n",
    "\n",
    "    # Test the Model\n",
    "    rnn.is_training = False\n",
    "    timing['testing'] = datetime.datetime.now()\n",
    "    print('Testing -----------------------------------------------')\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    predicted_list = []\n",
    "    label_list = []\n",
    "    for mfcc, labels in test_loader:#test_loader\n",
    "        mfcc = Variable(mfcc.view(-1, sequence_length, input_size))\n",
    "        if cuda_enabled:\n",
    "            mfcc = mfcc.cuda()\n",
    "\n",
    "        outputs = rnn(mfcc)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        for p, l in zip(predicted, labels):\n",
    "            predicted_list.append(p)\n",
    "            label_list.append(l)\n",
    "            if p == l:\n",
    "                correct += 1.0\n",
    "    timing['testing'] = datetime.datetime.now() - timing['testing']\n",
    "    print('Timing (feature extraction, training, timing)')\n",
    "    print('=============================================')\n",
    "    print(timing['features'])\n",
    "    print(timing['training'])\n",
    "    print(timing['testing'])\n",
    "    print('')\n",
    "    print('=============================================')\n",
    "    print('')\n",
    "    print('Confusion Matrix')\n",
    "    print('================')\n",
    "    # print(train.get_encoder().classes_)\n",
    "    print(confusion_matrix(label_list, predicted_list))\n",
    "    print('=============================================')\n",
    "    print('Accuracy = %0.4f' % (accuracy_score(label_list, predicted_list)))\n",
    "    print('=============================================')\n",
    "\n",
    "    # Save the Model\n",
    "    torch.save(rnn.state_dict(), 'ShouldIDrive_body_tracking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43fe14e-2b44-45ac-aa34-aafb436637e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_eye_tracking_biRNN():\n",
    "    import datetime\n",
    "    import sys\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "    # Keep timing -- for future reporting\n",
    "    timing = dict()\n",
    "\n",
    "    # Data description\n",
    "    input_size = 5000  # 2939, 20 # TODO: Determine this from the data\n",
    "    test_input_size = 5000\n",
    "    sequence_length = 136  # TODO: Determine this from the data\n",
    "    # Data\n",
    "    label = 'SvP'  \n",
    "\n",
    "    timing['start'] = datetime.datetime.now()\n",
    "    i = 1\n",
    "    test = sid_eye_tracking_data('train', padlen=input_size, featlen=sequence_length, fold_list_number=i)\n",
    "\n",
    "    train = sid_eye_tracking_data('test',padlen=test_input_size, featlen=sequence_length, fold_list_number=i)\n",
    "    print('We are testing the %d th fold.' %(i))\n",
    "    print('ShouldIDrive test data items: {}'.format(test.__len__()))\n",
    "    print('ShouldIDrive train data items: {}'.format(train.__len__()))\n",
    "\n",
    "    timing['features'] = datetime.datetime.now() - timing['start']\n",
    "    print('Extracted features from speech files.')\n",
    "\n",
    "    # Some hyperparams, etc. for the network\n",
    "    batch_size = 64\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    if not cuda_enabled:\n",
    "        kwargs['pin_memory'] = False\n",
    "        batch_size = 32\n",
    "\n",
    "    print('Starting loader -------------------------------------')\n",
    "    timing['training'] = datetime.datetime.now()\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    # sys.exit()\n",
    "\n",
    "    # The net... and training it\n",
    "    hidden_size = 128\n",
    "    num_layers = 2\n",
    "    num_classes = 2  # TODO: Determine this from the data\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 300\n",
    "\n",
    "    # The network\n",
    "    rnn = BiRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "    rnn.is_training = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_loss = 5000000000.\n",
    "    # Train it\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_total = 0.\n",
    "        iteration_count = 0.\n",
    "        for i, (mfcc, labels) in enumerate(train_loader):\n",
    "            iteration_count += 1.\n",
    "            mfcc = Variable(mfcc.view(-1, sequence_length, input_size))\n",
    "            labels = Variable(labels)\n",
    "            if cuda_enabled:\n",
    "                mfcc = mfcc.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = rnn(mfcc)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_total += loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                      % (epoch + 1, num_epochs, i + 1, len(train) // batch_size, loss.data[0]))\n",
    "        current_epoch_loss = loss_total / iteration_count\n",
    "        print('Epoch %d; loss = %0.4f' % (epoch, current_epoch_loss))\n",
    "\n",
    "        epoch_loss = current_epoch_loss\n",
    "\n",
    "    timing['training'] = datetime.datetime.now() - timing['training']\n",
    "\n",
    "    # Test the Model\n",
    "    rnn.is_training = False\n",
    "    timing['testing'] = datetime.datetime.now()\n",
    "    print('Testing -----------------------------------------------')\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    predicted_list = []\n",
    "    label_list = []\n",
    "    for mfcc, labels in test_loader:#test_loader\n",
    "        mfcc = Variable(mfcc.view(-1, sequence_length, input_size))\n",
    "        if cuda_enabled:\n",
    "            mfcc = mfcc.cuda()\n",
    "\n",
    "        outputs = rnn(mfcc)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        for p, l in zip(predicted, labels):\n",
    "            predicted_list.append(p)\n",
    "            label_list.append(l)\n",
    "            if p == l:\n",
    "                correct += 1.0\n",
    "    timing['testing'] = datetime.datetime.now() - timing['testing']\n",
    "    print('Timing (feature extraction, training, timing)')\n",
    "    print('=============================================')\n",
    "    print(timing['features'])\n",
    "    print(timing['training'])\n",
    "    print(timing['testing'])\n",
    "    print('')\n",
    "    print('=============================================')\n",
    "    print('')\n",
    "    print('Confusion Matrix')\n",
    "    print('================')\n",
    "    # print(train.get_encoder().classes_)\n",
    "    print(confusion_matrix(label_list, predicted_list))\n",
    "    print('=============================================')\n",
    "    print('Accuracy = %0.4f' % (accuracy_score(label_list, predicted_list)))\n",
    "    print('=============================================')\n",
    "\n",
    "    # Save the Model\n",
    "    torch.save(rnn.state_dict(), 'ShouldIDrive_eye_tracking.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946f200-70d6-4a97-a7fc-21bc759a040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_body_tracking_biRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a282e-8476-49c3-b101-5cfab6a03a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_eye_tracking_biRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526bda08-bfb6-4f5d-b1bc-998571e1b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     main_biRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1b5b0-46bd-4e10-a774-c2083fcbde6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
